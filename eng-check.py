import streamlit as st
import spacy
import transformers

# í˜ì´ì§€ ì„¤ì • (ê°€ì¥ ë¨¼ì € í˜¸ì¶œí•´ì•¼ í•¨)
st.set_page_config(
    page_title="ì˜ì‘ë¬¸ ìë™ ì²¨ì‚­ ì‹œìŠ¤í…œ",
    page_icon="ğŸ“",
    layout="wide"
)

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
from collections import Counter
import re
import io
import random
from datetime import datetime
from textblob import TextBlob
import asyncio
import edge_tts
import tempfile
import os
import base64

# NLTK ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë°ì´í„° ì²˜ë¦¬
import nltk
from nltk.corpus import stopwords

# ë³€í™˜ê¸° ëª¨ë“ˆ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
has_transformers = 'transformers' in globals()

# ëŒ€ì²´ ë§ì¶¤ë²• ê²€ì‚¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì •
try:
    from spellchecker import SpellChecker
    spell = SpellChecker()
    has_spellchecker = True
except ImportError:
    has_spellchecker = False
    try:
        # ëŒ€ì²´ ë§ì¶¤ë²• ê²€ì‚¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ enchant ì‹œë„
        import enchant
        has_enchant = True
    except ImportError:
        has_enchant = False
    #st.info("ë§ì¶¤ë²• ê²€ì‚¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. TextBlobì„ ì‚¬ìš©í•œ ê¸°ë³¸ ë§ì¶¤ë²• ê²€ì‚¬ë§Œ ì œê³µë©ë‹ˆë‹¤.")

# NLTK í•„ìš” ë°ì´í„° ë‹¤ìš´ë¡œë“œ (Streamlit Cloudì—ì„œë„ ì‘ë™í•˜ë„ë¡ ssl ê²€ì¦ ë¬´ì‹œ)
try:
    import ssl
    try:
        _create_unverified_https_context = ssl._create_unverified_context
    except AttributeError:
        pass
    else:
        ssl._create_default_https_context = _create_unverified_https_context
    
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
except Exception as e:
    st.warning(f"NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

# ì •ê·œì‹ íŒ¨í„´ ì»´íŒŒì¼ ìºì‹±
@st.cache_resource
def get_compiled_patterns():
    return {
        'sentence_split': re.compile(r'(?<=[.!?])\s+'),
        'word_tokenize': re.compile(r'\b[\w\'-]+\b'),
        'punctuation': re.compile(r'[.,!?;:"]')
    }

# ìˆ˜ì •ëœ sent_tokenize í•¨ìˆ˜ (NLTK ì˜ì¡´ì„± ì œê±°)
def custom_sent_tokenize(text):
    if not text:
        return []
    
    # ì •ê·œì‹ ê¸°ë°˜ ë¬¸ì¥ ë¶„í• ê¸°
    # ì˜¨ì , ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ë’¤ì— ê³µë°±ì´ ì˜¤ëŠ” íŒ¨í„´ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
    patterns = get_compiled_patterns()
    sentences = patterns['sentence_split'].split(text)
    # ë¹ˆ ë¬¸ì¥ ì œê±°
    return [s.strip() for s in sentences if s.strip()]

# ìˆ˜ì •ëœ word_tokenize í•¨ìˆ˜ (NLTK ì˜ì¡´ì„± ì œê±°)
def custom_word_tokenize(text):
    if not text:
        return []
    
    # íš¨ê³¼ì ì¸ ì •ê·œì‹ íŒ¨í„´ìœ¼ë¡œ ë‹¨ì–´ í† í°í™”
    # ì¶•ì•½í˜•(I'm, don't ë“±), ì†Œìœ ê²©(John's), í•˜ì´í”ˆ ë‹¨ì–´(well-known) ë“±ì„ ì²˜ë¦¬
    patterns = get_compiled_patterns()
    words = patterns['word_tokenize'].findall(text)
    # êµ¬ë‘ì 
    punctuation = patterns['punctuation'].findall(text)
    
    tokens = []
    tokens.extend(words)
    tokens.extend(punctuation)
    
    return tokens

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'history' not in st.session_state:
    st.session_state.history = []

# í˜„ì¬ ì„ íƒëœ íƒ­ ì¶”ì  (student_pageì˜ ê²½ìš°)
if 'selected_tab' not in st.session_state:
    st.session_state.selected_tab = 0  # ê¸°ë³¸ íƒ­ì€ 0(ì˜ì‘ë¬¸ ê²€ì‚¬)

# ë§ì¶¤ë²• ì‚¬ì „ ì´ˆê¸°í™”
@st.cache_resource
def get_spell_checker():
    if has_spellchecker:
        return spell
    elif has_enchant:
        try:
            return enchant.Dict("en_US")
        except:
            return None
    return None

# ìì£¼ í‹€ë¦¬ëŠ” ë‹¨ì–´ì— ëŒ€í•œ ì‚¬ìš©ì ì •ì˜ ì œì•ˆ ì‚¬ì „
@st.cache_resource
def get_custom_suggestions():
    return {
        'tonite': ['tonight'],
        'tomorow': ['tomorrow'],
        'yestarday': ['yesterday'],
        'definately': ['definitely'],
        'recieve': ['receive'],
        'occurence': ['occurrence'],
        'calender': ['calendar'],
        'wierd': ['weird'],
        'alot': ['a lot'],
        'untill': ['until'],
        'thier': ['their'],
        'truely': ['truly'],
        'begining': ['beginning'],
        'beleive': ['believe'],
        'seperate': ['separate'],
        'goverment': ['government'],
        'neccessary': ['necessary'],
        'occasionaly': ['occasionally'],
        'independant': ['independent'],
        'basicly': ['basically'],
        'wich': ['which'],
        'wont': ["won't"],
        'cant': ["can't"],
        'dont': ["don't"],
        'isnt': ["isn't"],
        'didnt': ["didn't"],
        'couldnt': ["couldn't"],
        'shouldnt': ["shouldn't"],
        'wouldnt': ["wouldn't"],
        'doesnt': ["doesn't"],
        'wasnt': ["wasn't"],
        'werent': ["weren't"],
        'havent': ["haven't"],
        'hasnt': ["hasn't"],
        'hadnt': ["hadn't"],
        'arent': ["aren't"]
    }

# ë¬¸ë²• ê²€ì‚¬ í•¨ìˆ˜ (TextBlob ì‚¬ìš©)
def check_grammar(text):
    if not text.strip():
        return []
    
    errors = []
    
    try:
        blob = TextBlob(text)
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ì„
        sentences = custom_sent_tokenize(text)
        
        # ë§ì¶¤ë²• ê²€ì‚¬
        checker = get_spell_checker()
        words = custom_word_tokenize(text)
        
        # ì‚¬ìš©ì ì •ì˜ ì œì•ˆ ì‚¬ì „ ë¡œë“œ
        custom_suggestions = get_custom_suggestions()
        
        # ë¬¸ì¥ì˜ ì‹œì‘ ìœ„ì¹˜ë¥¼ ì¶”ì í•˜ê¸° ìœ„í•œ ë³€ìˆ˜
        offset = 0
        
        for sentence in sentences:
            # ê°„ë‹¨í•œ ë¬¸ë²• ê²€ì‚¬ (TextBlobì˜ sentiment ì‚¬ìš©)
            try:
                sentence_blob = TextBlob(sentence)
                
                # ë¬¸ì¥ì˜ ë‹¨ì–´ ë¶„ì„
                for word in custom_word_tokenize(sentence):
                    if word.isalpha():  # ì•ŒíŒŒë²³ ë‹¨ì–´ë§Œ í™•ì¸
                        # ë§ì¶¤ë²• í™•ì¸ (enchantê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ)
                        if checker and has_enchant and not checker.check(word):
                            # ì‚¬ìš©ì ì •ì˜ ì œì•ˆì´ ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸
                            word_lower = word.lower()
                            if word_lower in custom_suggestions:
                                suggestions = custom_suggestions[word_lower]
                            else:
                                # ê¸°ì¡´ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì œì•ˆ ì‚¬ìš©
                                suggestions = checker.suggest(word)[:3]  # ìµœëŒ€ 3ê°œ ì œì•ˆ
                            
                            word_offset = text.find(word, offset)
                            
                            if word_offset != -1:
                                errors.append({
                                    "offset": word_offset,
                                    "errorLength": len(word),
                                    "message": f"ë§ì¶¤ë²• ì˜¤ë¥˜: '{word}'",
                                    "replacements": suggestions
                                })
                        # PySpellChecker ì‚¬ìš© (enchant ëŒ€ì‹ )
                        elif checker and 'has_spellchecker' in globals() and has_spellchecker and word.lower() not in checker:
                            # ì‚¬ìš©ì ì •ì˜ ì œì•ˆì´ ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸
                            word_lower = word.lower()
                            if word_lower in custom_suggestions:
                                suggestions = custom_suggestions[word_lower]
                            else:
                                # PySpellChecker ì œì•ˆ ì‚¬ìš©
                                suggestions = [checker.correction(word)] + list(checker.candidates(word))[:2]
                            
                            word_offset = text.find(word, offset)
                            
                            if word_offset != -1:
                                errors.append({
                                    "offset": word_offset,
                                    "errorLength": len(word),
                                    "message": f"ë§ì¶¤ë²• ì˜¤ë¥˜: '{word}'",
                                    "replacements": suggestions
                                })
                        # TextBlobì„ ì‚¬ìš©í•œ ë§ì¶¤ë²• ê²€ì‚¬ ëŒ€ì•ˆ (ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ëŠ” ê²½ìš°)
                        elif not checker:
                            try:
                                # ì‚¬ìš©ì ì •ì˜ ì œì•ˆì´ ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸
                                word_lower = word.lower()
                                if word_lower in custom_suggestions:
                                    corrected = custom_suggestions[word_lower][0]  # ì²« ë²ˆì§¸ ì œì•ˆ ì‚¬ìš©
                                    word_offset = text.find(word, offset)
                                    if word_offset != -1:
                                        errors.append({
                                            "offset": word_offset,
                                            "errorLength": len(word),
                                            "message": f"ë§ì¶¤ë²• ì˜¤ë¥˜: '{word}'",
                                            "replacements": custom_suggestions[word_lower]
                                        })
                                else:
                                    # TextBlobì„ ì‚¬ìš©í•œ ê¸°ì¡´ ë¡œì§
                                    word_blob = TextBlob(word)
                                    corrected = str(word_blob.correct())
                                    if corrected != word and len(word) > 3:  # ì§§ì€ ë‹¨ì–´ëŠ” ë¬´ì‹œ
                                        word_offset = text.find(word, offset)
                                        if word_offset != -1:
                                            errors.append({
                                                "offset": word_offset,
                                                "errorLength": len(word),
                                                "message": f"ë§ì¶¤ë²• ì˜¤ë¥˜: '{word}'",
                                                "replacements": [corrected]
                                            })
                            except Exception as e:
                                # TextBlob ë§ì¶¤ë²• ê²€ì‚¬ ì˜¤ë¥˜ ë¬´ì‹œ
                                pass
            except Exception as e:
                # ë¬¸ì¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒì‹œ í•´ë‹¹ ë¬¸ì¥ ê±´ë„ˆë›°ê¸°
                pass
                
            # ë‹¤ìŒ ë¬¸ì¥ì˜ ê²€ìƒ‰ì„ ìœ„í•´ ì˜¤í”„ì…‹ ì—…ë°ì´íŠ¸
            offset += len(sentence)
    except Exception as e:
        st.error(f"í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    
    return errors

# í…ìŠ¤íŠ¸ í†µê³„ ë¶„ì„ í•¨ìˆ˜
def analyze_text(text):
    if not text.strip():
        return {
            'word_count': 0,
            'sentence_count': 0,
            'avg_word_length': 0,
            'avg_sentence_length': 0,
            'vocabulary_size': 0
        }
    
    sentences = custom_sent_tokenize(text)
    words = custom_word_tokenize(text)
    words = [word.lower() for word in words if re.match(r'\w+', word)]
    
    word_count = len(words)
    sentence_count = len(sentences)
    avg_word_length = sum(len(word) for word in words) / max(1, word_count)
    avg_sentence_length = word_count / max(1, sentence_count)
    vocabulary_size = len(set(words))
    
    return {
        'word_count': word_count,
        'sentence_count': sentence_count,
        'avg_word_length': round(avg_word_length, 2),
        'avg_sentence_length': round(avg_sentence_length, 2),
        'vocabulary_size': vocabulary_size
    }

# ì–´íœ˜ ë¶„ì„ í•¨ìˆ˜
def analyze_vocabulary(text):
    if not text.strip():
        return {
            'word_freq': {},
            'pos_dist': {}
        }
    
    words = custom_word_tokenize(text.lower())
    words = [word for word in words if re.match(r'\w+', word)]
    
    # ë¶ˆìš©ì–´ ì œê±°
    stop_words = set(stopwords.words('english'))
    filtered_words = [word for word in words if word not in stop_words]
    
    # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
    word_freq = Counter(filtered_words).most_common(20)
    
    return {
        'word_freq': dict(word_freq)
    }

# ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚°
def calculate_lexical_diversity(text):
    words = custom_word_tokenize(text.lower())
    words = [word for word in words if re.match(r'\w+', word)]
    
    if len(words) == 0:
        return 0
    
    return len(set(words)) / len(words)

# ë‹¨ì–´ ë¹ˆë„ ì‹œê°í™”
def plot_word_frequency(word_freq):
    if not word_freq:
        return None
    
    df = pd.DataFrame(list(word_freq.items()), columns=['ë‹¨ì–´', 'ë¹ˆë„'])
    df = df.sort_values('ë¹ˆë„', ascending=True)
    
    fig = px.bar(df.tail(10), x='ë¹ˆë„', y='ë‹¨ì–´', orientation='h', 
                title='ìƒìœ„ 10ê°œ ë‹¨ì–´ ë¹ˆë„')
    fig.update_layout(height=400)
    
    return fig

# ê¸°ë³¸ ë‹¨ì–´ ì…‹ ì •ì˜
@st.cache_resource
def default_vocabulary_sets():
    basic_words = {'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'i', 'it'}
    intermediate_words = {'achieve', 'consider', 'determine', 'establish', 'indicate'}
    advanced_words = {'arbitrary', 'cognitive', 'encompass', 'facilitate', 'implicit'}
    return {'basic': basic_words, 'intermediate': intermediate_words, 'advanced': advanced_words}

# í•™ìˆ  ë‹¨ì–´ ëª©ë¡
@st.cache_resource
def get_academic_word_list():
    # í•™ìˆ  ë‹¨ì–´ ëª©ë¡ (ì˜ˆì‹œ)
    return {'analyze', 'concept', 'data', 'environment', 'establish', 'evident', 
            'factor', 'interpret', 'method', 'principle', 'process', 'research', 
            'significant', 'theory', 'variable'}

# ë‹¨ì–´ ë¹ˆë„ ë°ì´í„° ë¡œë“œ
@st.cache_resource
def get_word_frequency_data():
    # ì˜ì–´ ë‹¨ì–´ ë¹ˆë„ ë°ì´í„° (ì˜ˆì‹œ)
    common_words = {'the': 0.05, 'be': 0.04, 'to': 0.03, 'of': 0.025, 'and': 0.02}
    return common_words

# ê³ ê¸‰ ë™ì˜ì–´ ì‚¬ì „
@st.cache_resource
def get_advanced_synonyms():
    return {
        # ê¸°ë³¸ í˜•ìš©ì‚¬
        'good': ['exemplary', 'exceptional', 'impeccable', 'outstanding', 'superb', 'commendable'],
        'bad': ['detrimental', 'deplorable', 'egregious', 'lamentable', 'abysmal', 'substandard'],
        'big': ['immense', 'formidable', 'monumental', 'colossal', 'substantial', 'extensive'],
        'small': ['minuscule', 'negligible', 'infinitesimal', 'diminutive', 'minute', 'marginal'],
        'happy': ['euphoric', 'exuberant', 'ecstatic', 'jubilant', 'delighted', 'elated'],
        'sad': ['despondent', 'crestfallen', 'dejected', 'disconsolate', 'melancholic', 'woeful'],
        'important': ['imperative', 'indispensable', 'paramount', 'pivotal', 'consequential', 'significant'],
        'difficult': ['formidable', 'insurmountable', 'Herculean', 'arduous', 'challenging', 'demanding'],
        'easy': ['effortless', 'rudimentary', 'facile', 'straightforward', 'uncomplicated', 'elementary'],
        'beautiful': ['resplendent', 'breathtaking', 'sublime', 'exquisite', 'magnificent', 'captivating'],
        
        # ì¶”ê°€ í˜•ìš©ì‚¬
        'interesting': ['intriguing', 'captivating', 'compelling', 'engrossing', 'fascinating', 'riveting'],
        'boring': ['tedious', 'monotonous', 'mundane', 'insipid', 'dull', 'unengaging'],
        'smart': ['brilliant', 'astute', 'sagacious', 'ingenious', 'erudite', 'perspicacious'],
        'stupid': ['obtuse', 'vacuous', 'inane', 'fatuous', 'imbecilic', 'absurd'],
        'fast': ['expeditious', 'prompt', 'accelerated', 'swift', 'rapid', 'nimble'],
        'slow': ['languorous', 'leisurely', 'sluggish', 'plodding', 'unhurried', 'dilatory'],
        
        # ìì£¼ ì‚¬ìš©ë˜ëŠ” ë™ì‚¬
        'say': ['articulate', 'pronounce', 'proclaim', 'assert', 'expound', 'enunciate'],
        'think': ['contemplate', 'ponder', 'deliberate', 'ruminate', 'cogitate', 'muse'],
        'see': ['observe', 'perceive', 'discern', 'witness', 'behold', 'scrutinize'],
        'use': ['utilize', 'employ', 'implement', 'leverage', 'harness', 'apply'],
        'make': ['construct', 'fabricate', 'forge', 'produce', 'generate', 'devise'],
        'get': ['acquire', 'obtain', 'procure', 'attain', 'secure', 'garner']
    }

# ê³ ê¸‰ í‘œí˜„ íŒ¨í„´
@st.cache_resource
def get_advanced_phrases():
    return {
        # ê¸°ë³¸ í‘œí˜„ ê³ ê¸‰í™”
        r'\bi think\b': ['I postulate that', 'I am of the conviction that', 'It is my considered opinion that', 'I firmly believe that', 'From my perspective', 'I have come to the conclusion that'],
        r'\bi like\b': ['I am particularly enamored with', 'I hold in high regard', 'I find great merit in', 'I am deeply appreciative of', 'I have a profound affinity for', 'I derive considerable pleasure from'],
        r'\bi want\b': ['I aspire to', 'I am inclined towards', 'My inclination is toward', 'I earnestly desire', 'I have a vested interest in', 'My objective is to'],
        r'\blots of\b': ['a plethora of', 'an abundance of', 'a multitude of', 'a substantial amount of', 'a considerable quantity of', 'a significant number of'],
        r'\bmany of\b': ['a preponderance of', 'a substantial proportion of', 'a significant contingent of', 'a notable segment of', 'a sizable fraction of', 'a considerable percentage of'],
        
        # ë¬¸ì¥ ì‹œì‘ ë¶€ë¶„ í–¥ìƒ
        r'^In my opinion\b': ['From my perspective', 'According to my assessment', 'Based on my evaluation', 'In my estimation', 'As I perceive it', 'In my considered judgment'],
        r'^I agree\b': ['I concur with the assessment that', 'I am in complete accord with', 'I share the sentiment that', 'I am aligned with the view that', 'I endorse the position that', 'I subscribe to the notion that'],
        r'^I disagree\b': ['I take exception to', 'I contest the assertion that', 'I must respectfully differ with', 'I cannot reconcile myself with', 'I find myself at variance with', 'I am compelled to challenge the idea that'],
        
        # í•œêµ­ì–´ í•™ìŠµìê°€ ìì£¼ ì‚¬ìš©í•˜ëŠ” ë¬¸êµ¬ ëŒ€ì²´
        r'\bit is important to\b': ['it is imperative to', 'it is essential to', 'it is crucial to', 'it is of paramount importance to', 'it is a fundamental necessity to', 'it is a critical requirement to'],
        r'\bin conclusion\b': ['in summation', 'to synthesize the aforementioned points', 'in culmination', 'as a final observation', 'to encapsulate the preceding discussion', 'as the logical denouement'],
        r'\bfor example\b': ['as an illustrative case', 'to cite a pertinent instance', 'as a demonstrative example', 'to exemplify this concept', 'as a representative case in point', 'to elucidate through a specific example']
    }

def rewrite_similar_level(text):
    """ë¹„ìŠ·í•œ ìˆ˜ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¬ì‘ì„± - ê°„ë‹¨í•œ ë™ì˜ì–´ êµì²´"""
    sentences = custom_sent_tokenize(text)
    rewritten = []
    
    # ê°„ë‹¨í•œ ë™ì˜ì–´ ì‚¬ì „
    synonyms = {
        'good': ['nice', 'fine', 'decent'],
        'bad': ['poor', 'unfortunate', 'unpleasant'],
        'big': ['large', 'sizable', 'substantial'],
        'small': ['little', 'tiny', 'slight'],
        'happy': ['glad', 'pleased', 'content'],
        'sad': ['unhappy', 'down', 'blue']
    }
    
    for sentence in sentences:
        words = custom_word_tokenize(sentence)
        new_words = []
    
        for word in words:
            word_lower = word.lower()
            # 20% í™•ë¥ ë¡œ ë™ì˜ì–´ êµì²´ ì‹œë„
            if word_lower in synonyms and random.random() < 0.2:
                replacement = random.choice(synonyms[word_lower])
                # ëŒ€ë¬¸ì ë³´ì¡´
                if word[0].isupper():
                    replacement = replacement.capitalize()
                new_words.append(replacement)
            else:
                new_words.append(word)
        
        rewritten.append(' '.join(new_words))
    
    return ' '.join(rewritten)

def rewrite_improved_level(text):
    """ì•½ê°„ í–¥ìƒëœ ìˆ˜ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¬ì‘ì„± - ì¤‘ê¸‰ ë™ì˜ì–´ë¡œ ëŒ€ì²´í•˜ê³  êµ¬ë¬¸ ê°œì„ """
    sentences = custom_sent_tokenize(text)
    rewritten = []
    
    # ì¤‘ê¸‰ ë™ì˜ì–´ ì‚¬ì „
    intermediate_synonyms = {
        'good': ['beneficial', 'favorable', 'quality'],
        'bad': ['negative', 'inferior', 'flawed'],
        'big': ['significant', 'considerable', 'extensive'],
        'small': ['minimal', 'limited', 'minor'],
        'say': ['state', 'mention', 'express'],
        'think': ['believe', 'consider', 'reflect'],
        'important': ['essential', 'significant', 'crucial']
    }
    
    # ê°œì„ í•  êµ¬ë¬¸ íŒ¨í„´
    phrase_patterns = {
        r'\bi think\b': ['In my opinion', 'I believe', 'From my perspective'],
        r'\ba lot\b': ['considerably', 'significantly', 'substantially'],
        r'\bvery\b': ['notably', 'particularly', 'especially']
    }
    
    for sentence in sentences:
        # ë™ì˜ì–´ êµì²´
        words = custom_word_tokenize(sentence)
        new_words = []
    
        for word in words:
            word_lower = word.lower()
            # 30% í™•ë¥ ë¡œ ì¤‘ê¸‰ ë™ì˜ì–´ êµì²´ ì‹œë„
            if word_lower in intermediate_synonyms and random.random() < 0.3:
                replacement = random.choice(intermediate_synonyms[word_lower])
                # ëŒ€ë¬¸ì ë³´ì¡´
                if word[0].isupper():
                    replacement = replacement.capitalize()
                new_words.append(replacement)
            else:
                new_words.append(word)
        
        improved = ' '.join(new_words)
        
        # êµ¬ë¬¸ íŒ¨í„´ ê°œì„ 
        for pattern, replacements in phrase_patterns.items():
            if re.search(pattern, improved, re.IGNORECASE) and random.random() < 0.4:
                improved = re.sub(pattern, random.choice(replacements), improved, flags=re.IGNORECASE)
        
        rewritten.append(improved)
    
    return ' '.join(rewritten)

def rewrite_advanced_level(text):
    """ê³ ê¸‰ ìˆ˜ì¤€ìœ¼ë¡œ í…ìŠ¤íŠ¸ ì¬ì‘ì„± - ê³ ê¸‰ ì–´íœ˜ì™€ í‘œí˜„ìœ¼ë¡œ ë³€í™˜"""
    # ë¬¸ì¥ í† í°í™”
    sentences = custom_sent_tokenize(text)
    advanced_sentences = []
    
    # ê³ ê¸‰ ë™ì˜ì–´ ë° í‘œí˜„ ê°€ì ¸ì˜¤ê¸°
    advanced_synonyms = get_advanced_synonyms()
    advanced_phrases = get_advanced_phrases()
    
    for sentence in sentences:
        # ë™ì˜ì–´ êµì²´
        for word, replacements in advanced_synonyms.items():
            pattern = r'\b' + re.escape(word) + r'\b'
            if re.search(pattern, sentence, re.IGNORECASE) and random.random() < 0.5:
                replacement = random.choice(replacements)
                # ì›ë˜ ë‹¨ì–´ê°€ ëŒ€ë¬¸ìë¡œ ì‹œì‘í•˜ë©´ ëŒ€ì²´ì–´ë„ ëŒ€ë¬¸ìë¡œ ì‹œì‘
                if re.search(pattern, sentence).group(0)[0].isupper():
                    replacement = replacement.capitalize()
                sentence = re.sub(pattern, replacement, sentence, flags=re.IGNORECASE)
        
        # ê³ ê¸‰ í‘œí˜„ìœ¼ë¡œ êµì²´
        for pattern, replacements in advanced_phrases.items():
            if re.search(pattern, sentence, re.IGNORECASE) and random.random() < 0.6:
                replacement = random.choice(replacements)
                sentence = re.sub(pattern, replacement, sentence, flags=re.IGNORECASE)
        
        advanced_sentences.append(sentence)
    
    # ì¬ì‘ì„±ëœ í…ìŠ¤íŠ¸ ë°˜í™˜
    return ' '.join(advanced_sentences)

# ê³ ê¸‰ ì¬ì‘ì„± í•¨ìˆ˜
def advanced_rewrite_text(text, level='advanced'):
    # ë³€í™˜ê¸° ëª¨ë¸ì´ ì—†ìœ¼ë©´ ê·œì¹™ ê¸°ë°˜ ë°©ì‹ìœ¼ë¡œ í´ë°±
    if not has_transformers:
        return rewrite_advanced_level(text)
    
    try:
        # ê°„ë‹¨í•œ ê·œì¹™ ê¸°ë°˜ ê³ ê¸‰ ì¬ì‘ì„± (transformers ëª¨ë¸ ì—†ì„ ë•Œ)
        return rewrite_advanced_level(text)
    except Exception as e:
        st.warning(f"ê³ ê¸‰ ì¬ì‘ì„± ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
        return rewrite_advanced_level(text)

def rewrite_text(text, level="similar"):
    """
    ì…ë ¥ í…ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ ë ˆë²¨ì— ë”°ë¼ ì¬ì‘ì„±í•©ë‹ˆë‹¤.
    
    Parameters:
    - text: ì¬ì‘ì„±í•  í…ìŠ¤íŠ¸
    - level: ì¬ì‘ì„± ë ˆë²¨ ("similar", "improved", "advanced" ì¤‘ í•˜ë‚˜)
    
    Returns:
    - ì¬ì‘ì„±ëœ í…ìŠ¤íŠ¸
    """
    if not text:
        return ""
    
    # ë ˆë²¨ì— ë”°ë¼ ì ì ˆí•œ ì¬ì‘ì„± í•¨ìˆ˜ í˜¸ì¶œ
    if level == "similar":
        return rewrite_similar_level(text)
    elif level == "improved":
        return rewrite_improved_level(text)
    elif level == "advanced":
        if has_transformers:
            return advanced_rewrite_text(text, level)
        else:
            return rewrite_advanced_level(text)
    else:
        return text  # ê¸°ë³¸ê°’ì€ ì›ë³¸ í…ìŠ¤íŠ¸ ë°˜í™˜

# í•™ìƒ í˜ì´ì§€
def evaluate_vocabulary_level(text):
    # ì˜¨ë¼ì¸ ë°ì´í„°ì…‹ì—ì„œ ì–´íœ˜ ë¡œë“œ
    vocabulary_sets = default_vocabulary_sets()
    
    # ì˜¨ë¼ì¸ ë°ì´í„°ì…‹ ë¡œë“œ ì‹œë„
    try:
        import requests
        
        # ì˜ì–´ ë‹¨ì–´ ë¹ˆë„ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
        word_freq_url = "https://raw.githubusercontent.com/hermitdave/FrequencyWords/master/content/2018/en/en_50k.txt"
        response = requests.get(word_freq_url)
        if response.status_code == 200:
            # ë‹¨ì–´ ë¹ˆë„ ë°ì´í„° íŒŒì‹± (í˜•ì‹: "ë‹¨ì–´ ë¹ˆë„")
            lines = response.text.splitlines()
            words = [line.split()[0] for line in lines if ' ' in line]
            
            # ë¹ˆë„ì— ë”°ë¼ ë‹¨ì–´ ë¶„ë¥˜
            total_words = len(words)
            basic_cutoff = int(total_words * 0.2)  # ìƒìœ„ 20%
            intermediate_cutoff = int(total_words * 0.5)  # ìƒìœ„ 20~50%
            
            basic_words = set(words[:basic_cutoff])
            intermediate_words = set(words[basic_cutoff:intermediate_cutoff])
            advanced_words = set(words[intermediate_cutoff:])
            
            vocabulary_sets = {'basic': basic_words, 'intermediate': intermediate_words, 'advanced': advanced_words}
    except Exception as e:
        pass
    
    words = custom_word_tokenize(text.lower())
    words = [word for word in words if re.match(r'\w+', word)]
    
    word_set = set(words)
    
    basic_count = len(word_set.intersection(vocabulary_sets['basic']))
    intermediate_count = len(word_set.intersection(vocabulary_sets['intermediate']))
    advanced_count = len(word_set.intersection(vocabulary_sets['advanced']))
    
    total = basic_count + intermediate_count + advanced_count
    if total == 0:
        return {'basic': 0, 'intermediate': 0, 'advanced': 0}
    
    return {
        'basic': basic_count / max(1, total),
        'intermediate': intermediate_count / max(1, total),
        'advanced': advanced_count / max(1, total)
    }

# í•™ìƒ í˜ì´ì§€
def show_student_page():
    st.title("ì˜ì‘ë¬¸ ìë™ ì²¨ì‚­ ì‹œìŠ¤í…œ - í•™ìƒ")
    
    # ë¡œê·¸ì•„ì›ƒ ë²„íŠ¼
    if st.button("ë¡œê·¸ì•„ì›ƒ", key="student_logout"):
        st.session_state.user_type = None
        st.rerun()
    
    # íƒ­ ì¸ë±ìŠ¤ë¥¼ ì„¸ì…˜ ìƒíƒœì—ì„œ ê°€ì ¸ì˜´
    tab_index = st.session_state.selected_tab
    
    # íƒ­ ìƒì„± - selected_tabì— ë”°ë¼ ì´ˆê¸° ì„ íƒ 
    tabs = st.tabs(["ì˜ì‘ë¬¸ ê²€ì‚¬", "ì˜ì‘ë¬¸ ì¬ì‘ì„±", "ë‚´ ì‘ë¬¸ ê¸°ë¡"])
    
    # í˜„ì¬ ì„ íƒëœ íƒ­ì„ ë³´ì—¬ì¤Œ
    with tabs[tab_index]:
        pass

    # ì˜ì‘ë¬¸ ê²€ì‚¬ íƒ­
    with tabs[0]:
        st.subheader("ì˜ì‘ë¬¸ ì…ë ¥")
        
        # ì˜ì‘ë¬¸ ì…ë ¥ê³¼ ë“£ê¸° ë²„íŠ¼ì„ ê°™ì€ í–‰ì— ë°°ì¹˜
        input_col, listen_col = st.columns([3, 1])
        
        with input_col:
            user_text = st.text_area("ì•„ë˜ì— ì˜ì–´ ì‘ë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", height=200, key="text_tab1")
        
        with listen_col:
            # ìŒì„± ìƒì„±/ì¬ìƒ ë²„íŠ¼
            st.markdown("<br><br>", unsafe_allow_html=True)  # ë²„íŠ¼ ìœ„ì¹˜ ì¡°ì •ì„ ìœ„í•œ ê³µë°±
            
            # ì‚¬ìš©ì ì…ë ¥ í…ìŠ¤íŠ¸ì˜ í•´ì‹œê°’ ê³„ì‚° (ë³€ê²½ ì‹œ ìë™ ê°±ì‹ ìš©)
            if user_text:
                text_hash = hash(user_text)
                audio_key = f"audio_tab1_{text_hash}"
                
                # ì„¸ì…˜ ìƒíƒœì— ìŒì„± íŒŒì¼ ê²½ë¡œê°€ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
                if audio_key not in st.session_state:
                    st.session_state[audio_key] = None
                
                # í† ê¸€ ìƒíƒœ ê´€ë¦¬
                if f"{audio_key}_playing" not in st.session_state:
                    st.session_state[f"{audio_key}_playing"] = False
                
                # í† ê¸€ ë²„íŠ¼ ìƒì„±
                if st.session_state[audio_key] is None:
                    if st.button("ğŸ“¢ ì˜ì‘ë¬¸ ë“£ê¸°", key=f"generate_audio_tab1", use_container_width=True):
                        if user_text.strip():  # í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ì‹¤í–‰
                            with st.spinner("ìŒì„± íŒŒì¼ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                                try:
                                    # ìŒì„± íŒŒì¼ ìƒì„±
                                    voice_model = "en-US-JennyNeural"  # ê¸°ë³¸ Jenny ìŒì„± ì‚¬ìš©
                                    
                                    # ì„ì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„±
                                    temp_dir = tempfile.gettempdir()
                                    audio_file_path = os.path.join(temp_dir, f"speech_tab1_{datetime.now().strftime('%Y%m%d_%H%M%S')}.wav")
                                    
                                    # ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰
                                    loop = asyncio.new_event_loop()
                                    asyncio.set_event_loop(loop)
                                    audio_path = loop.run_until_complete(text_to_speech(user_text, voice_model, audio_file_path))
                                    loop.close()
                                    
                                    # ì„¸ì…˜ ìƒíƒœì— ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ì €ì¥
                                    st.session_state[audio_key] = audio_path
                                    st.session_state[f"{audio_key}_playing"] = True
                                    st.experimental_rerun()
                                except Exception as e:
                                    st.error(f"ìŒì„± ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                        else:
                            st.warning("í…ìŠ¤íŠ¸ë¥¼ ë¨¼ì € ì…ë ¥í•´ì£¼ì„¸ìš”.")
                else:
                    # í† ê¸€ ë²„íŠ¼ ë¡œì§
                    button_label = "â¹ï¸ ìŒì„± ì •ì§€" if st.session_state[f"{audio_key}_playing"] else "â–¶ï¸ ìŒì„± ì¬ìƒ"
                    if st.button(button_label, key=f"toggle_audio_tab1", use_container_width=True):
                        # í† ê¸€ ìƒíƒœ ë³€ê²½
                        st.session_state[f"{audio_key}_playing"] = not st.session_state[f"{audio_key}_playing"]
                        st.experimental_rerun()
                    
                    # ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´ í‘œì‹œ (í˜„ì¬ í˜ì´ì§€ ìœ„ì¹˜ì— í‘œì‹œ)
                    if st.session_state[f"{audio_key}_playing"]:
                        audio_path = st.session_state[audio_key]
                        if os.path.exists(audio_path):
                            # ìŒì„± í”Œë ˆì´ì–´ í‘œì‹œ
                            audio_html = get_audio_player_html(audio_path, loop_count=5)
                            st.markdown(audio_html, unsafe_allow_html=True)
        
        # ë¶„ì„ ë²„íŠ¼ í–‰
        col1, col2 = st.columns([3, 1])
        
        # ëª¨ë“  ë¶„ì„ì„ í•œ ë²ˆì— ì‹¤í–‰í•˜ëŠ” ë²„íŠ¼
        with col1:
            if st.button("ì „ì²´ ë¶„ì„í•˜ê¸°", use_container_width=True, key="analyze_button"):
                if not user_text:
                    st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                else:
                    # í…ìŠ¤íŠ¸ í†µê³„ ë¶„ì„
                    stats = analyze_text(user_text)
                
                    # ë¬¸ë²• ì˜¤ë¥˜ ê²€ì‚¬
                    try:
                        grammar_errors = check_grammar(user_text)
                    except Exception as e:
                        st.error(f"ë¬¸ë²• ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                        grammar_errors = []
                    
                    # ì–´íœ˜ ë¶„ì„
                    vocab_analysis = analyze_vocabulary(user_text)
                    
                    # ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜
                    diversity_score = calculate_lexical_diversity(user_text)
                    
                    # ì–´íœ˜ ìˆ˜ì¤€ í‰ê°€
                    vocab_level = evaluate_vocabulary_level(user_text)
                    
                    # ì„¸ì…˜ ìƒíƒœì— ê²°ê³¼ ì €ì¥
                    if 'analysis_results' not in st.session_state:
                        st.session_state.analysis_results = {}
                    
                    st.session_state.analysis_results = {
                        'stats': stats,
                        'grammar_errors': grammar_errors,
                        'vocab_analysis': vocab_analysis,
                        'diversity_score': diversity_score,
                        'vocab_level': vocab_level,
                        'original_text': user_text  # ì›ë³¸ í…ìŠ¤íŠ¸ë„ ì €ì¥
                    }
                    
                    # ê¸°ë¡ì— ì €ì¥
                    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    st.session_state.history.append({
                        'timestamp': timestamp,
                        'text': user_text,
                        'error_count': len(grammar_errors) if grammar_errors else 0
                    })
                    
                    st.success("ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì•„ë˜ íƒ­ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                    
                    # ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŒì„ í‘œì‹œí•˜ëŠ” í”Œë˜ê·¸
                    st.session_state.analysis_completed = True
                    st.rerun()  # ì¬ì‹¤í–‰í•˜ì—¬ ë²„íŠ¼ í‘œì‹œ ì—…ë°ì´íŠ¸
        
        # ì¬ì‘ì„± ì¶”ì²œ ë²„íŠ¼ ì¶”ê°€
        with col2:
            # ë¶„ì„ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë²„íŠ¼ í‘œì‹œ
            if 'analysis_results' in st.session_state and 'original_text' in st.session_state.analysis_results:
                if st.button("âœ¨ ì˜ì‘ë¬¸ ì¬ì‘ì„± ì¶”ì²œ âœ¨", 
                          key="rewrite_recommendation",
                          use_container_width=True,
                          help="ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ì‘ë¬¸ì„ ë” ì¢‹ì€ í‘œí˜„ìœ¼ë¡œ ì¬ì‘ì„±í•´ë³´ì„¸ìš”!",
                          type="primary"):
                    # í…ìŠ¤íŠ¸ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                    st.session_state.copy_to_rewrite = st.session_state.analysis_results['original_text']
                    
                    # ì¬ì‘ì„± íƒ­ìœ¼ë¡œ ì „í™˜í•˜ê¸° ìœ„í•´ selected_tab ì—…ë°ì´íŠ¸
                    st.session_state.selected_tab = 1  # 1ì€ ì˜ì‘ë¬¸ ì¬ì‘ì„± íƒ­
                    
                    # ì‚¬ìš©ìì—ê²Œ ì•ˆë‚´ ë©”ì‹œì§€ í‘œì‹œ
                    st.success("í…ìŠ¤íŠ¸ê°€ ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì¬ì‘ì„± íƒ­ìœ¼ë¡œ ì´ë™í•©ë‹ˆë‹¤!")
                    st.balloons()  # ì‹œê°ì  íš¨ê³¼ ì¶”ê°€
                    
                    # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ (íƒ­ ì „í™˜ì„ ìœ„í•´)
                    st.rerun()
        
        # ê²°ê³¼ í‘œì‹œë¥¼ ìœ„í•œ íƒ­
        result_tab1, result_tab2, result_tab3 = st.tabs(["ë¬¸ë²• ê²€ì‚¬", "ì–´íœ˜ ë¶„ì„", "í…ìŠ¤íŠ¸ í†µê³„"])
        
        with result_tab1:
            if 'analysis_results' in st.session_state and 'grammar_errors' in st.session_state.analysis_results:
                grammar_errors = st.session_state.analysis_results['grammar_errors']
                
                if grammar_errors:
                    st.subheader("ë¬¸ë²• ì˜¤ë¥˜ ëª©ë¡")
                    
                    error_data = []
                    for error in grammar_errors:
                        error_data.append({
                            "ì˜¤ë¥˜": user_text[error['offset']:error['offset'] + error['errorLength']],
                            "ì˜¤ë¥˜ ë‚´ìš©": error['message'],
                            "ìˆ˜ì • ì œì•ˆ": error['replacements']
                        })
                    
                    st.dataframe(pd.DataFrame(error_data))
                else:
                    st.success("ë¬¸ë²• ì˜¤ë¥˜ê°€ ì—†ìŠµë‹ˆë‹¤!")
                
                # ìŒì„± ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ í‘œì‹œ (ê¸°ì¡´ ë²„íŠ¼ ìœ„ì¹˜ì—ëŠ” ë‹¤ìš´ë¡œë“œë§Œ ìœ ì§€)
                audio_key = f"audio_tab1_{hash(st.session_state.analysis_results['original_text'])}" if 'original_text' in st.session_state.analysis_results else None
                if audio_key and audio_key in st.session_state and st.session_state[audio_key]:
                    audio_path = st.session_state[audio_key]
                    if os.path.exists(audio_path):
                        with st.expander("ìŒì„± íŒŒì¼ ë‹¤ìš´ë¡œë“œ"):
                            with open(audio_path, "rb") as f:
                                audio_bytes = f.read()
                            
                            st.download_button(
                                label="ìŒì„± ë‹¤ìš´ë¡œë“œ",
                                data=audio_bytes,
                                file_name=f"audio_essay_{datetime.now().strftime('%Y%m%d_%H%M%S')}.wav",
                                mime="audio/wav"
                            )
        
        with result_tab2:
            if 'analysis_results' in st.session_state and 'vocab_analysis' in st.session_state.analysis_results:
                vocab_analysis = st.session_state.analysis_results['vocab_analysis']
                diversity_score = st.session_state.analysis_results['diversity_score']
                vocab_level = st.session_state.analysis_results['vocab_level']
                        
                # ë‹¨ì–´ ë¹ˆë„ ì‹œê°í™” - ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•œ ì˜ˆì™¸ ì²˜ë¦¬ ì¶”ê°€
                if vocab_analysis and 'word_freq' in vocab_analysis and vocab_analysis['word_freq']:
                    fig = plot_word_frequency(vocab_analysis['word_freq'])
                    if fig:
                        st.plotly_chart(fig, use_container_width=True)
                else:
                    st.info("ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                
                # ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜
                st.metric("ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜", f"{diversity_score:.2f}")
                
                # ì–´íœ˜ ìˆ˜ì¤€ í‰ê°€
                if vocab_level:
                    level_df = pd.DataFrame({
                        'ìˆ˜ì¤€': ['ê¸°ì´ˆ', 'ì¤‘ê¸‰', 'ê³ ê¸‰'],
                        'ë¹„ìœ¨': [vocab_level['basic'], vocab_level['intermediate'], vocab_level['advanced']]
                    })
                    
                    fig = px.pie(level_df, values='ë¹„ìœ¨', names='ìˆ˜ì¤€', 
                                title='ì–´íœ˜ ìˆ˜ì¤€ ë¶„í¬')
                    st.plotly_chart(fig, use_container_width=True)
                else:
                    st.info("ì–´íœ˜ ìˆ˜ì¤€ í‰ê°€ë¥¼ ìœ„í•œ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        with result_tab3:
            if 'analysis_results' in st.session_state and 'stats' in st.session_state.analysis_results:
                stats = st.session_state.analysis_results['stats']
                    
                st.subheader("í…ìŠ¤íŠ¸ í†µê³„")
                    
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("ë‹¨ì–´ ìˆ˜", stats['word_count'])
                    st.metric("ë¬¸ì¥ ìˆ˜", stats['sentence_count'])
                with col2:
                    st.metric("í‰ê·  ë‹¨ì–´ ê¸¸ì´", stats['avg_word_length'])
                    st.metric("í‰ê·  ë¬¸ì¥ ê¸¸ì´ (ë‹¨ì–´)", stats['avg_sentence_length'])
                    
                    st.metric("ì–´íœ˜ í¬ê¸° (ê³ ìœ  ë‹¨ì–´ ìˆ˜)", stats['vocabulary_size'])
                    
                    # ê²Œì´ì§€ ì°¨íŠ¸ë¡œ í‘œí˜„í•˜ê¸°
                    progress_col1, progress_col2 = st.columns(2)
                with progress_col1:
                        # í‰ê·  ë¬¸ì¥ ê¸¸ì´ ê²Œì´ì§€ (ì ì • ì˜ì–´ ë¬¸ì¥ ê¸¸ì´: 15-20 ë‹¨ì–´)
                    sentence_gauge = min(1.0, stats['avg_sentence_length'] / 20)
                    st.progress(sentence_gauge)
                    st.caption(f"ë¬¸ì¥ ê¸¸ì´ ì ì •ì„±: {int(sentence_gauge * 100)}%")
                    
                with progress_col2:
                        # ì–´íœ˜ ë‹¤ì–‘ì„± ê²Œì´ì§€
                    vocab_ratio = stats['vocabulary_size'] / max(1, stats['word_count'])
                    st.progress(min(1.0, vocab_ratio * 2))  # 0.5 ì´ìƒì´ë©´ 100%
                    st.caption(f"ì–´íœ˜ ë‹¤ì–‘ì„±: {int(min(1.0, vocab_ratio * 2) * 100)}%")
    
    # ì˜ì‘ë¬¸ ì¬ì‘ì„± íƒ­
    with tabs[1]:
        st.subheader("ì˜ì‘ë¬¸ ì¬ì‘ì„±")
        
        # ì™¼ìª½ ì—´: ì…ë ¥ ë° ì˜µì…˜
        left_col, right_col = st.columns(2)
        
        with left_col:
            # ë¶„ì„ íƒ­ì—ì„œ ë„˜ì–´ì˜¨ ê²½ìš° í•´ë‹¹ í…ìŠ¤íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ë¡œë“œ
            default_text = ""
            if 'copy_to_rewrite' in st.session_state:
                default_text = st.session_state.copy_to_rewrite
                st.success("ë¶„ì„ ê²°ê³¼ í…ìŠ¤íŠ¸ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
                # í•œ ë²ˆ ì‚¬ìš© í›„ ì„ì‹œ ë³€ìˆ˜ë¡œ ì˜®ê²¨ ì €ì¥
                st.session_state.copy_to_rewrite_temp = default_text
                del st.session_state.copy_to_rewrite
            elif 'copy_to_rewrite_temp' in st.session_state:
                default_text = st.session_state.copy_to_rewrite_temp
            
            rewrite_text_input = st.text_area("ì•„ë˜ì— ì˜ì–´ ì‘ë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", 
                                            value=default_text,
                                            height=200, 
                                            key="text_tab2")
            
            level_option = st.radio(
                "ì‘ë¬¸ ìˆ˜ì¤€ ì„ íƒ",
                options=["ë¹„ìŠ·í•œ ìˆ˜ì¤€", "ì•½ê°„ ë†’ì€ ìˆ˜ì¤€", "ê³ ê¸‰ ìˆ˜ì¤€"],
                horizontal=True
            )
            
            level_map = {
                "ë¹„ìŠ·í•œ ìˆ˜ì¤€": "similar",
                "ì•½ê°„ ë†’ì€ ìˆ˜ì¤€": "improved",
                "ê³ ê¸‰ ìˆ˜ì¤€": "advanced"
            }
            
            if st.button("ì¬ì‘ì„±í•˜ê¸°"):
                if not rewrite_text_input:
                    st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                else:
                    level = level_map.get(level_option, "similar")
                    
                    # ì¬ì‘ì„± ì²˜ë¦¬
                    with st.spinner("í…ìŠ¤íŠ¸ë¥¼ ì¬ì‘ì„± ì¤‘ì…ë‹ˆë‹¤..."):
                        rewritten_text = rewrite_text(rewrite_text_input, level)
                        
                        # ì¬ì‘ì„±ëœ í…ìŠ¤íŠ¸ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                        if 'rewritten_text' not in st.session_state:
                            st.session_state.rewritten_text = {}
                        
                        st.session_state.rewritten_text[level] = rewritten_text
                        
                        # ê¸°ë¡ì— ì¶”ê°€
                        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        st.session_state.history.append({
                            'timestamp': timestamp,
                            'text': rewrite_text_input,
                            'action': f"ì¬ì‘ì„± ({level_option})"
                        })
        
        with right_col:
            st.subheader("ì¬ì‘ì„± ê²°ê³¼")
            
            if 'rewritten_text' in st.session_state and st.session_state.rewritten_text:
                level = level_map.get(level_option, "similar")
                
                if level in st.session_state.rewritten_text:
                    rewritten = st.session_state.rewritten_text[level]
                    st.text_area("ì¬ì‘ì„±ëœ í…ìŠ¤íŠ¸", value=rewritten, height=250, key="rewritten_result")
                    
                    # ìŒì„± ì˜µì…˜ ì¶”ê°€
                    st.subheader("ë³¸ë¬¸ ì½ê¸° ì˜µì…˜")
                    voice_options = {
                        "Jenny (ì—¬ì„±, ë¯¸êµ­)": "en-US-JennyNeural",
                        "Guy (ë‚¨ì„±, ë¯¸êµ­)": "en-US-GuyNeural",
                        "Aria (ì—¬ì„±, ì˜êµ­)": "en-GB-SoniaNeural"
                    }
                    selected_voice = st.selectbox(
                        "ìŒì„± ì„ íƒ",
                        options=list(voice_options.keys()),
                        key="voice_selection"
                    )
                    
                    # ì¬ì‘ì„± í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ ë° ìŒì„± ë³€í™˜ ë²„íŠ¼
                    col1, col2 = st.columns(2)
                    
                    with col1:
                        # í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                        if rewritten:
                            text_output = io.BytesIO()
                            text_output.write(rewritten.encode('utf-8'))
                            text_output.seek(0)
                            
                            st.download_button(
                                label="í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ",
                                data=text_output,
                                file_name=f"rewritten_text_{level}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                                mime="text/plain"
                            )
                    
                    with col2:
                        # ìŒì„± íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                        if rewritten:
                            if st.button("ìŒì„± íŒŒì¼ ìƒì„±", key="generate_speech"):
                                with st.spinner("ìŒì„± íŒŒì¼ì„ ìƒì„± ì¤‘ì…ë‹ˆë‹¤..."):
                                    # ì„ íƒëœ ìŒì„± ëª¨ë¸ ê°€ì ¸ì˜¤ê¸°
                                    voice_model = voice_options[selected_voice]
                                    
                                    # ì„ì‹œ íŒŒì¼ ê²½ë¡œ ìƒì„±
                                    temp_dir = tempfile.gettempdir()
                                    audio_file_path = os.path.join(temp_dir, f"speech_{datetime.now().strftime('%Y%m%d_%H%M%S')}.wav")
                                    
                                    # ë¹„ë™ê¸° í•¨ìˆ˜ ì‹¤í–‰ì„ ìœ„í•œ ëŸ°íƒ€ì„ ì„¤ì •
                                    loop = asyncio.new_event_loop()
                                    asyncio.set_event_loop(loop)
                                    audio_path = loop.run_until_complete(text_to_speech(rewritten, voice_model, audio_file_path))
                                    
                                    # ì„¸ì…˜ ìƒíƒœì— ì˜¤ë””ì˜¤ íŒŒì¼ ê²½ë¡œ ì €ì¥
                                    st.session_state.audio_path = audio_path
                                    st.success("ìŒì„± íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤!")
                                    st.experimental_rerun()  # ì¬ì‹¤í–‰í•˜ì—¬ ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´ í‘œì‹œ
            
                    # ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´ í‘œì‹œ
                    if 'audio_path' in st.session_state and os.path.exists(st.session_state.audio_path):
                        st.subheader("ë³¸ë¬¸ ë“£ê¸°")
                        
                        # ì˜¤ë””ì˜¤ ì¬ìƒ ìƒíƒœ ê´€ë¦¬
                        if 'audio_playing' not in st.session_state:
                            st.session_state.audio_playing = True
                        
                        # ë³¸ë¬¸ ë“£ê¸° í† ê¸€ ë²„íŠ¼
                        play_col, download_col = st.columns([3, 1])
                        
                        with play_col:
                            # í† ê¸€ ë²„íŠ¼ ë¡œì§
                            button_label = "â¹ï¸ ìŒì„± ì •ì§€" if st.session_state.audio_playing else "â–¶ï¸ ìŒì„± ì¬ìƒ"
                            if st.button(button_label, key="toggle_audio"):
                                # í† ê¸€ ìƒíƒœ ë³€ê²½
                                st.session_state.audio_playing = not st.session_state.audio_playing
                                st.experimental_rerun()
                            
                            # í˜„ì¬ ìƒíƒœì— ë”°ë¼ ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´ í‘œì‹œ
                            if st.session_state.audio_playing:
                                audio_html = get_audio_player_html(st.session_state.audio_path, loop_count=5)
                                st.markdown(audio_html, unsafe_allow_html=True)
                        
                        with download_col:
                            with open(st.session_state.audio_path, "rb") as f:
                                audio_bytes = f.read()
                            
                            # ìŒì„± íŒŒì¼ ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                            st.download_button(
                                label="ìŒì„± ë‹¤ìš´ë¡œë“œ",
                                data=audio_bytes,
                                file_name=f"audio_{level}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.wav",
                                mime="audio/wav"
                            )
            
                    # ì›ë³¸ê³¼ ì¬ì‘ì„± í…ìŠ¤íŠ¸ ë¹„êµ
                    if rewrite_text_input and rewritten:
                        st.subheader("ì›ë³¸ vs ì¬ì‘ì„± ë¹„êµ")
                        
                        comparison_data = []
                        original_sentences = custom_sent_tokenize(rewrite_text_input)
                        rewritten_sentences = custom_sent_tokenize(rewritten)
                        
                        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¹„êµ (ë” ì§§ì€ ë¦¬ìŠ¤íŠ¸ ê¸°ì¤€)
                        for i in range(min(len(original_sentences), len(rewritten_sentences))):
                            comparison_data.append({
                                "ì›ë³¸": original_sentences[i],
                                "ì¬ì‘ì„±": rewritten_sentences[i]
                            })
                        
                        if comparison_data:
                            st.dataframe(pd.DataFrame(comparison_data), use_container_width=True)
            else:
                st.info("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê³  ì¬ì‘ì„± ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
    
    # ë‚´ ì‘ë¬¸ ê¸°ë¡ íƒ­
    with tabs[2]:
        st.subheader("ë‚´ ì‘ë¬¸ ê¸°ë¡")
        if not st.session_state.history:
            st.info("ì•„ì§ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            history_df = pd.DataFrame(st.session_state.history)
            st.dataframe(history_df)
            
            # ì˜¤ë¥˜ ìˆ˜ ì¶”ì´ ì°¨íŠ¸
            if len(history_df) > 1 and 'error_count' in history_df.columns:
                fig = px.line(history_df, x='timestamp', y='error_count', 
                            title='ë¬¸ë²• ì˜¤ë¥˜ ìˆ˜ ì¶”ì´',
                            labels={'timestamp': 'ë‚ ì§œ', 'error_count': 'ì˜¤ë¥˜ ìˆ˜'})
                st.plotly_chart(fig, use_container_width=True)

# êµì‚¬ í˜ì´ì§€
def show_teacher_page():
    st.title("ì˜ì‘ë¬¸ ìë™ ì²¨ì‚­ ì‹œìŠ¤í…œ - êµì‚¬")
    
    # ë¡œê·¸ì•„ì›ƒ ë²„íŠ¼
    if st.button("ë¡œê·¸ì•„ì›ƒ", key="teacher_logout"):
        st.session_state.user_type = None
        st.rerun()
    
    # íƒ­ ìƒì„±
    tabs = st.tabs(["ì˜ì‘ë¬¸ ì²¨ì‚­", "í•™ìŠµ ëŒ€ì‹œë³´ë“œ"])
    
    # ì˜ì‘ë¬¸ ì²¨ì‚­ íƒ­
    with tabs[0]:
        st.subheader("ì˜ì‘ë¬¸ ì…ë ¥ ë° ì²¨ì‚­")
        
        user_text = st.text_area("í•™ìƒì˜ ì˜ì–´ ì‘ë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", height=200, key="teacher_text")
        
        col1, col2 = st.columns([3, 1])
        
        # ëª¨ë“  ë¶„ì„ì„ í•œ ë²ˆì— ì‹¤í–‰í•˜ëŠ” ë²„íŠ¼
        with col1:
            if st.button("ì „ì²´ ë¶„ì„í•˜ê¸°", key="teacher_analyze_all", use_container_width=True):
                if not user_text:
                    st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                else:
                    # ë¬¸ë²• ì˜¤ë¥˜ ê²€ì‚¬
                    try:
                        grammar_errors = check_grammar(user_text)
                    except Exception as e:
                        st.error(f"ë¬¸ë²• ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                        grammar_errors = []
                    
                    # ì–´íœ˜ ë¶„ì„
                    vocab_analysis = analyze_vocabulary(user_text)
                    
                    # ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜
                    diversity_score = calculate_lexical_diversity(user_text)
                    
                    # ì–´íœ˜ ìˆ˜ì¤€ í‰ê°€
                    vocab_level = evaluate_vocabulary_level(user_text)
                    
                    # í…ìŠ¤íŠ¸ í†µê³„ ë¶„ì„
                    stats = analyze_text(user_text)
                    
                    # ì„¸ì…˜ ìƒíƒœì— ê²°ê³¼ ì €ì¥
                    if 'teacher_analysis_results' not in st.session_state:
                        st.session_state.teacher_analysis_results = {}
                    
                    st.session_state.teacher_analysis_results = {
                        'stats': stats,
                        'grammar_errors': grammar_errors,
                        'vocab_analysis': vocab_analysis,
                        'diversity_score': diversity_score,
                        'vocab_level': vocab_level,
                        'original_text': user_text  # ì›ë³¸ í…ìŠ¤íŠ¸ë„ ì €ì¥
                    }
                    
                    st.success("ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì•„ë˜ íƒ­ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        
        # ì¬ì‘ì„± ì¶”ì²œ ë²„íŠ¼ ì¶”ê°€
        with col2:
            # ë¶„ì„ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë²„íŠ¼ í‘œì‹œ
            if 'teacher_analysis_results' in st.session_state and 'original_text' in st.session_state.teacher_analysis_results:
                if st.button("âœ¨ ì˜ì‘ë¬¸ ì¬ì‘ì„± ì¶”ì²œ âœ¨", 
                          key="teacher_rewrite_recommendation",
                          use_container_width=True,
                          help="ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìƒì˜ ì˜ì‘ë¬¸ì„ ë” ì¢‹ì€ í‘œí˜„ìœ¼ë¡œ ì¬ì‘ì„±í•´ë³´ì„¸ìš”!",
                          type="primary"):
                    # êµì‚¬ ì²¨ì‚­ ì˜ì—­ì— ëª¨ë²” ë‹µì•ˆ ì‘ì„±ìš©ìœ¼ë¡œ ì¶”ê°€
                    # ì¬ì‘ì„± í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¦‰ì‹œ ê³ ê¸‰ ìˆ˜ì¤€ìœ¼ë¡œ ì¬ì‘ì„±
                    original_text = st.session_state.teacher_analysis_results['original_text']
                    rewritten_text = rewrite_text(original_text, "advanced")
                    
                    # ì²¨ì‚­ ë…¸íŠ¸ì— ì¬ì‘ì„±ëœ í…ìŠ¤íŠ¸ ì¶”ê°€
                    if 'feedback_template' not in st.session_state:
                        st.session_state.feedback_template = "ë‹¤ìŒì€ í•™ìƒ ì‘ë¬¸ì„ ê³ ê¸‰ ìˆ˜ì¤€ìœ¼ë¡œ ì¬ì‘ì„±í•œ ì˜ˆì‹œì…ë‹ˆë‹¤:\n\n" + rewritten_text
                    else:
                        st.session_state.feedback_template += "\n\nì¶”ì²œ ëª¨ë²” ì˜ˆì‹œ:\n" + rewritten_text
                    
                    st.success("ê³ ê¸‰ ìˆ˜ì¤€ìœ¼ë¡œ ì¬ì‘ì„±ëœ í…ìŠ¤íŠ¸ê°€ ì²¨ì‚­ ë…¸íŠ¸ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    st.balloons()  # ì‹œê°ì  íš¨ê³¼ ì¶”ê°€
        
        # ê²°ê³¼ í‘œì‹œë¥¼ ìœ„í•œ íƒ­
        result_tab1, result_tab2, result_tab3 = st.tabs(["ë¬¸ë²• ê²€ì‚¬", "ì–´íœ˜ ë¶„ì„", "í…ìŠ¤íŠ¸ í†µê³„"])
        
        with result_tab1:
            if 'teacher_analysis_results' in st.session_state and 'grammar_errors' in st.session_state.teacher_analysis_results:
                grammar_errors = st.session_state.teacher_analysis_results['grammar_errors']
                        
                if grammar_errors:
                            st.subheader("ë¬¸ë²• ì˜¤ë¥˜ ëª©ë¡")
                            
                            error_data = []
                            for error in grammar_errors:
                                error_data.append({
                            "ì˜¤ë¥˜": user_text[error['offset']:error['offset'] + error['errorLength']],
                            "ì˜¤ë¥˜ ë‚´ìš©": error['message'],
                            "ìˆ˜ì • ì œì•ˆ": error['replacements']
                                })
                            
                            st.dataframe(pd.DataFrame(error_data))
                else:
                            st.success("ë¬¸ë²• ì˜¤ë¥˜ê°€ ì—†ìŠµë‹ˆë‹¤!")
            
        with result_tab2:
            if 'teacher_analysis_results' in st.session_state and 'vocab_analysis' in st.session_state.teacher_analysis_results:
                vocab_analysis = st.session_state.teacher_analysis_results['vocab_analysis']
                diversity_score = st.session_state.teacher_analysis_results['diversity_score']
                vocab_level = st.session_state.teacher_analysis_results['vocab_level']
                        
                # ë‹¨ì–´ ë¹ˆë„ ì‹œê°í™” - ì—ëŸ¬ ë°©ì§€ë¥¼ ìœ„í•œ ì˜ˆì™¸ ì²˜ë¦¬ ì¶”ê°€
                if vocab_analysis and 'word_freq' in vocab_analysis and vocab_analysis['word_freq']:
                    fig = plot_word_frequency(vocab_analysis['word_freq'])
                    if fig:
                        st.plotly_chart(fig, use_container_width=True)
                else:
                    st.info("ë‹¨ì–´ ë¹ˆë„ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
                
                # ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜
                st.metric("ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜", f"{diversity_score:.2f}")
                
                # ì–´íœ˜ ìˆ˜ì¤€ í‰ê°€
                if vocab_level:
                    level_df = pd.DataFrame({
                        'ìˆ˜ì¤€': ['ê¸°ì´ˆ', 'ì¤‘ê¸‰', 'ê³ ê¸‰'],
                        'ë¹„ìœ¨': [vocab_level['basic'], vocab_level['intermediate'], vocab_level['advanced']]
                    })
                    
                    fig = px.pie(level_df, values='ë¹„ìœ¨', names='ìˆ˜ì¤€', 
                                title='ì–´íœ˜ ìˆ˜ì¤€ ë¶„í¬')
                    st.plotly_chart(fig, use_container_width=True)
                else:
                    st.info("ì–´íœ˜ ìˆ˜ì¤€ í‰ê°€ë¥¼ ìœ„í•œ ë°ì´í„°ê°€ ì¶©ë¶„í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        
        with result_tab3:
            if 'teacher_analysis_results' in st.session_state and 'stats' in st.session_state.teacher_analysis_results:
                stats = st.session_state.teacher_analysis_results['stats']
                    
                st.subheader("í…ìŠ¤íŠ¸ í†µê³„")
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("ë‹¨ì–´ ìˆ˜", stats['word_count'])
                    st.metric("ë¬¸ì¥ ìˆ˜", stats['sentence_count'])
                with col2:
                    st.metric("í‰ê·  ë‹¨ì–´ ê¸¸ì´", stats['avg_word_length'])
                    st.metric("í‰ê·  ë¬¸ì¥ ê¸¸ì´ (ë‹¨ì–´)", stats['avg_sentence_length'])
                    
                st.metric("ì–´íœ˜ í¬ê¸° (ê³ ìœ  ë‹¨ì–´ ìˆ˜)", stats['vocabulary_size'])
        
        # êµì‚¬ ì „ìš© ê¸°ëŠ¥
        st.subheader("ì²¨ì‚­ ë° í”¼ë“œë°±")
        
        # ì²¨ì‚­ ë…¸íŠ¸ í…œí”Œë¦¿
        feedback_default = "ë‹¤ìŒ ì‚¬í•­ì„ ì¤‘ì ì ìœ¼ë¡œ ê°œì„ í•´ë³´ì„¸ìš”:\n1. \n2. \n3. "
        if 'feedback_template' in st.session_state:
            feedback_default = st.session_state.feedback_template
        
        feedback = st.text_area("ì²¨ì‚­ ë…¸íŠ¸", 
                             value=feedback_default, 
                             height=200,  # ë” ë†’ê²Œ ì¡°ì •
                 key="feedback_template")
        
        # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        st.session_state.feedback_template = feedback
        
        # ì ìˆ˜ ì…ë ¥
        score_col1, score_col2, score_col3 = st.columns(3)
        with score_col1:
            grammar_score = st.slider("ë¬¸ë²• ì ìˆ˜", 0, 10, 5)
        with score_col2:
            vocab_score = st.slider("ì–´íœ˜ ì ìˆ˜", 0, 10, 5)
        with score_col3:
            content_score = st.slider("ë‚´ìš© ì ìˆ˜", 0, 10, 5)
        
        total_score = (grammar_score + vocab_score + content_score) / 3
        st.metric("ì¢…í•© ì ìˆ˜", f"{total_score:.1f} / 10")
        
        # ì €ì¥ ì˜µì…˜
        if st.button("ì²¨ì‚­ ê²°ê³¼ ì €ì¥ (Excel)"):
            if not user_text:
                st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                # ë¬¸ë²• ì˜¤ë¥˜ ê²€ì‚¬
                grammar_errors = check_grammar(user_text)
                
                # ì €ì¥í•  ë°ì´í„° ìƒì„±
                error_data = []
                for error in grammar_errors:
                    error_data.append({
                        "ì˜¤ë¥˜": user_text[error['offset']:error['offset'] + error['errorLength']],
                        "ì˜¤ë¥˜ ë‚´ìš©": error['message'],
                        "ìˆ˜ì • ì œì•ˆ": str(error['replacements']),
                        "ìœ„ì¹˜": f"{error['offset']}:{error['offset'] + error['errorLength']}"
                    })
                
                # ì–´íœ˜ ë¶„ì„
                vocab_analysis = analyze_vocabulary(user_text)
                
                # í†µê³„ ë¶„ì„
                stats = analyze_text(user_text)
                
                # ì—¬ëŸ¬ ì‹œíŠ¸ê°€ ìˆëŠ” Excel íŒŒì¼ ìƒì„±
                excel_buffer = io.BytesIO()
                with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                    # ì›ë³¸ í…ìŠ¤íŠ¸ ì‹œíŠ¸
                    pd.DataFrame({"ì›ë³¸ í…ìŠ¤íŠ¸": [user_text]}).to_excel(writer, sheet_name="ì›ë³¸ í…ìŠ¤íŠ¸", index=False)
                    
                    # ì˜¤ë¥˜ ë°ì´í„° ì‹œíŠ¸
                    pd.DataFrame(error_data).to_excel(writer, sheet_name="ë¬¸ë²• ì˜¤ë¥˜", index=False)
                    
                    # í†µê³„ ì‹œíŠ¸
                    stats_df = pd.DataFrame({
                        "í•­ëª©": ["ë‹¨ì–´ ìˆ˜", "ë¬¸ì¥ ìˆ˜", "í‰ê·  ë‹¨ì–´ ê¸¸ì´", "í‰ê·  ë¬¸ì¥ ê¸¸ì´", "ì–´íœ˜ í¬ê¸°"],
                        "ê°’": [stats['word_count'], stats['sentence_count'], stats['avg_word_length'], 
                             stats['avg_sentence_length'], stats['vocabulary_size']]
                    })
                    stats_df.to_excel(writer, sheet_name="í†µê³„", index=False)
                    
                    # í‰ê°€ ì ìˆ˜ ì‹œíŠ¸
                    score_df = pd.DataFrame({
                        "í‰ê°€ í•­ëª©": ["ë¬¸ë²•", "ì–´íœ˜", "ë‚´ìš©", "ì¢…í•© ì ìˆ˜"],
                        "ì ìˆ˜": [grammar_score, vocab_score, content_score, total_score]
                    })
                    score_df.to_excel(writer, sheet_name="í‰ê°€ ì ìˆ˜", index=False)
                    
                    # í”¼ë“œë°± ì‹œíŠ¸
                    pd.DataFrame({"ì²¨ì‚­ í”¼ë“œë°±": [feedback]}).to_excel(writer, sheet_name="í”¼ë“œë°±", index=False)
                
                # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                excel_buffer.seek(0)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                st.download_button(
                    label="Excel ë‹¤ìš´ë¡œë“œ",
                    data=excel_buffer,
                    file_name=f"ì²¨ì‚­ê²°ê³¼_{timestamp}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                )
    
    # í•™ìŠµ ëŒ€ì‹œë³´ë“œ íƒ­
    with tabs[1]:
        st.subheader("í•™ìŠµ ëŒ€ì‹œë³´ë“œ")
        
        # ìƒ˜í”Œ í•™ìƒ ë°ì´í„° (ì‹¤ì œë¡œëŠ” DBì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨)
        sample_data = {
            "ë‚ ì§œ": ["2023-01-01", "2023-01-08", "2023-01-15", "2023-01-22", "2023-01-29"],
            "í•™ìƒ_A": [7, 6, 8, 7, 9],
            "í•™ìƒ_B": [5, 6, 6, 7, 6],
            "í•™ìƒ_C": [8, 8, 7, 9, 9],
            "í•™ìƒ_D": [4, 5, 6, 6, 7]
        }
        
        scores_df = pd.DataFrame(sample_data)
        
        # í•™ìƒë³„ ì ìˆ˜ ì¶”ì´
        fig = px.line(scores_df, x="ë‚ ì§œ", y=["í•™ìƒ_A", "í•™ìƒ_B", "í•™ìƒ_C", "í•™ìƒ_D"],
                     title="í•™ìƒë³„ ì˜ì‘ë¬¸ ì ìˆ˜ ì¶”ì´",
                     labels={"value": "ì ìˆ˜", "variable": "í•™ìƒ"})
        st.plotly_chart(fig, use_container_width=True)
        
        # ìµœê·¼ ì ìˆ˜ ë¶„í¬
        latest_scores = scores_df.iloc[-1, 1:].values
        students = scores_df.columns[1:]
        
        fig = px.bar(x=students, y=latest_scores, 
                    title="ìµœê·¼ ì˜ì‘ë¬¸ ì ìˆ˜ ë¶„í¬",
                    labels={"x": "í•™ìƒ", "y": "ì ìˆ˜"})
        st.plotly_chart(fig, use_container_width=True)
        
        # í‰ê·  ì˜¤ë¥˜ ìœ í˜• (ìƒ˜í”Œ ë°ì´í„°)
        error_types = {
            "ì˜¤ë¥˜ ìœ í˜•": ["ë¬¸ì¥ êµ¬ì¡°", "ì‹œì œ", "ê´€ì‚¬", "ì „ì¹˜ì‚¬", "ëŒ€ëª…ì‚¬", "ì² ì", "êµ¬ë‘ì "],
            "ë¹ˆë„": [45, 30, 25, 20, 15, 10, 5]
        }
        
        error_df = pd.DataFrame(error_types)
        
        fig = px.pie(error_df, values="ë¹ˆë„", names="ì˜¤ë¥˜ ìœ í˜•",
                    title="í‰ê·  ì˜¤ë¥˜ ìœ í˜• ë¶„í¬")
        st.plotly_chart(fig, use_container_width=True)

# ë©”ì¸ í•¨ìˆ˜
def main():
    # ì œëª© ë° ì†Œê°œ
    # st.title("ì˜ì‘ë¬¸ ìë™ ì²¨ì‚­ ì‹œìŠ¤í…œ")
    st.markdown("""
    ì´ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ í•™ìƒë“¤ì˜ ì˜ì‘ë¬¸ì„ ìë™ìœ¼ë¡œ ì²¨ì‚­í•˜ê³  í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.
    """)
    
    # ì§ì ‘ í•™ìƒ í˜ì´ì§€ë¡œ ì´ë™
    show_student_page()

if __name__ == "__main__":
    main()

@st.cache_resource
def load_vocabulary_datasets():
    # ì˜¨ë¼ì¸ ì†ŒìŠ¤ì—ì„œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ (ì‹¤ì œ ì‘ë™í•˜ëŠ” URLë¡œ ìˆ˜ì •)
    word_freq_url = "https://raw.githubusercontent.com/hermitdave/FrequencyWords/master/content/2018/en/en_50k.txt"
    
    try:
        import requests
        
        # ì˜ì–´ ë‹¨ì–´ ë¹ˆë„ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
        response = requests.get(word_freq_url)
        if response.status_code == 200:
            # ë‹¨ì–´ ë¹ˆë„ ë°ì´í„° íŒŒì‹± (í˜•ì‹: "ë‹¨ì–´ ë¹ˆë„")
            lines = response.text.splitlines()
            words = [line.split()[0] for line in lines if ' ' in line]
            
            # ë¹ˆë„ì— ë”°ë¼ ë‹¨ì–´ ë¶„ë¥˜
            total_words = len(words)
            basic_cutoff = int(total_words * 0.2)  # ìƒìœ„ 20%
            intermediate_cutoff = int(total_words * 0.5)  # ìƒìœ„ 20~50%
            
            basic_words = set(words[:basic_cutoff])
            intermediate_words = set(words[basic_cutoff:intermediate_cutoff])
            advanced_words = set(words[intermediate_cutoff:])
            
            return {'basic': basic_words, 'intermediate': intermediate_words, 'advanced': advanced_words}
    except Exception as e:
        st.warning(f"ì˜¨ë¼ì¸ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    
    # ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ì‹œ ë‚´ì¥ ë°ì´í„°ì…‹ ì‚¬ìš©
    return default_vocabulary_sets()

def evaluate_advanced_vocabulary(text):
    words = custom_word_tokenize(text.lower())
    
    # ë‹¨ì–´ ë¹ˆë„ ê¸°ë°˜ í‰ê°€
    word_frequencies = get_word_frequency_data()  # ë‹¨ì–´ë³„ ë¹ˆë„ ë°ì´í„° ë¡œë“œ
    
    rare_words = [w for w in words if w in word_frequencies and word_frequencies[w] < 0.001]
    
    # í•™ìˆ  ë‹¨ì–´ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    academic_word_list = get_academic_word_list()
    academic_words = [w for w in words if w in academic_word_list]
    
    vocab_score = (len(rare_words) * 2 + len(academic_words)) / max(len(words), 1)
    return vocab_score

# í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ ì¶”ê°€ (ë¼ì¸ 360 ì´í›„ì— ì¶”ê°€)
async def text_to_speech(text, voice="en-US-JennyNeural", output_file=None):
    """
    í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ê³  íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Parameters:
    - text: ìŒì„±ìœ¼ë¡œ ë³€í™˜í•  í…ìŠ¤íŠ¸
    - voice: ìŒì„± ëª¨ë¸ (ê¸°ë³¸ê°’: 'en-US-JennyNeural')
    - output_file: ì¶œë ¥ íŒŒì¼ ê²½ë¡œ (Noneì¸ ê²½ìš° ì„ì‹œ íŒŒì¼ ìƒì„±)
    
    Returns:
    - ìŒì„± íŒŒì¼ ê²½ë¡œ
    """
    if not text:
        return None
    
    # ì¶œë ¥ íŒŒì¼ì´ ì§€ì •ë˜ì§€ ì•Šì€ ê²½ìš° ì„ì‹œ íŒŒì¼ ìƒì„±
    if output_file is None:
        temp_dir = tempfile.gettempdir()
        output_file = os.path.join(temp_dir, f"speech_{random.randint(1000, 9999)}.wav")
    
    # í…ìŠ¤íŠ¸ë¥¼ ìŒì„±ìœ¼ë¡œ ë³€í™˜í•˜ê³  íŒŒì¼ë¡œ ì €ì¥
    communicate = edge_tts.Communicate(text, voice)
    await communicate.save(output_file)
    
    return output_file

# ìŒì„± íŒŒì¼ì„ HTML ì˜¤ë””ì˜¤ ìš”ì†Œë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜
def get_audio_player_html(audio_path, loop_count=5, autoplay=True):
    """
    ìŒì„± íŒŒì¼ì„ ì¬ìƒí•  ìˆ˜ ìˆëŠ” HTML ì˜¤ë””ì˜¤ í”Œë ˆì´ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Parameters:
    - audio_path: ìŒì„± íŒŒì¼ ê²½ë¡œ
    - loop_count: ë°˜ë³µ ì¬ìƒ íšŸìˆ˜ (ê¸°ë³¸ê°’: 5)
    - autoplay: ìë™ ì¬ìƒ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
    
    Returns:
    - HTML ì½”ë“œ ë¬¸ìì—´
    """
    if not audio_path or not os.path.exists(audio_path):
        return ""
    
    # íŒŒì¼ì„ base64ë¡œ ì¸ì½”ë”©
    with open(audio_path, "rb") as f:
        audio_bytes = f.read()
    
    audio_b64 = base64.b64encode(audio_bytes).decode()
    audio_html = f"""
        <audio id="audio-player" controls {' loop' if loop_count > 1 else ''} {' autoplay' if autoplay else ''}>
            <source src="data:audio/wav;base64,{audio_b64}" type="audio/wav">
            Your browser does not support the audio element.
        </audio>
        <script>
            var audioPlayer = document.getElementById('audio-player');
            var playCount = 0;
            var maxPlays = {loop_count};
            
            audioPlayer.addEventListener('ended', function() {{
                playCount++;
                if (playCount < maxPlays) {{
                    audioPlayer.play();
                }}
            }});
        </script>
    """
    return audio_html
