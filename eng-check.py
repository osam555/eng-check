import streamlit as st
import spacy
import transformers

# í˜ì´ì§€ ì„¤ì • (ê°€ì¥ ë¨¼ì € í˜¸ì¶œí•´ì•¼ í•¨)
st.set_page_config(
    page_title="ì˜ì‘ë¬¸ ìë™ ì²¨ì‚­ ì‹œìŠ¤í…œ",
    page_icon="ğŸ“",
    layout="wide"
)

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
from collections import Counter
import re
import io
import random
from datetime import datetime
from textblob import TextBlob

# NLTK ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë°ì´í„° ì²˜ë¦¬
import nltk
from nltk.corpus import stopwords

# ë³€í™˜ê¸° ëª¨ë“ˆ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
has_transformers = 'transformers' in globals()

# ëŒ€ì²´ ë§ì¶¤ë²• ê²€ì‚¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì •
try:
    import enchant
    has_enchant = True
except ImportError:
    has_enchant = False
    try:
        # ëŒ€ì²´ ë§ì¶¤ë²• ê²€ì‚¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‹œë„
        from spellchecker import SpellChecker
        spell = SpellChecker()
        has_spellchecker = True
    except ImportError:
        has_spellchecker = False
    #st.info("ë§ì¶¤ë²• ê²€ì‚¬ ë¼ì´ë¸ŒëŸ¬ë¦¬(enchant)ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. TextBlobì„ ì‚¬ìš©í•œ ê¸°ë³¸ ë§ì¶¤ë²• ê²€ì‚¬ë§Œ ì œê³µë©ë‹ˆë‹¤.")

# NLTK í•„ìš” ë°ì´í„° ë‹¤ìš´ë¡œë“œ (Streamlit Cloudì—ì„œë„ ì‘ë™í•˜ë„ë¡ ssl ê²€ì¦ ë¬´ì‹œ)
try:
    import ssl
    try:
        _create_unverified_https_context = ssl._create_unverified_context
    except AttributeError:
        pass
    else:
        ssl._create_default_https_context = _create_unverified_https_context
    
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
except Exception as e:
    st.warning(f"NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

# ì •ê·œì‹ íŒ¨í„´ ì»´íŒŒì¼ ìºì‹±
@st.cache_resource
def get_compiled_patterns():
    return {
        'sentence_split': re.compile(r'(?<=[.!?])\s+'),
        'word_tokenize': re.compile(r'\b[\w\'-]+\b'),
        'punctuation': re.compile(r'[.,!?;:"]')
    }

# ìˆ˜ì •ëœ sent_tokenize í•¨ìˆ˜ (NLTK ì˜ì¡´ì„± ì œê±°)
def custom_sent_tokenize(text):
    if not text:
        return []
    
    # ì •ê·œì‹ ê¸°ë°˜ ë¬¸ì¥ ë¶„í• ê¸°
    # ì˜¨ì , ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ë’¤ì— ê³µë°±ì´ ì˜¤ëŠ” íŒ¨í„´ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
    patterns = get_compiled_patterns()
    sentences = patterns['sentence_split'].split(text)
    # ë¹ˆ ë¬¸ì¥ ì œê±°
    return [s.strip() for s in sentences if s.strip()]

# ìˆ˜ì •ëœ word_tokenize í•¨ìˆ˜ (NLTK ì˜ì¡´ì„± ì œê±°)
def custom_word_tokenize(text):
    if not text:
        return []
    
    # íš¨ê³¼ì ì¸ ì •ê·œì‹ íŒ¨í„´ìœ¼ë¡œ ë‹¨ì–´ í† í°í™”
    # ì¶•ì•½í˜•(I'm, don't ë“±), ì†Œìœ ê²©(John's), í•˜ì´í”ˆ ë‹¨ì–´(well-known) ë“±ì„ ì²˜ë¦¬
    patterns = get_compiled_patterns()
    words = patterns['word_tokenize'].findall(text)
    # êµ¬ë‘ì 
    punctuation = patterns['punctuation'].findall(text)
    
    tokens = []
    tokens.extend(words)
    tokens.extend(punctuation)
    
    return tokens

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'history' not in st.session_state:
    st.session_state.history = []

# ë§ì¶¤ë²• ì‚¬ì „ ì´ˆê¸°í™”
@st.cache_resource
def get_spell_checker():
    if has_enchant:
        try:
            return enchant.Dict("en_US")
        except:
            return None
    elif 'has_spellchecker' in globals() and has_spellchecker:
        return spell
    return None

# ìì£¼ í‹€ë¦¬ëŠ” ë‹¨ì–´ì— ëŒ€í•œ ì‚¬ìš©ì ì •ì˜ ì œì•ˆ ì‚¬ì „
@st.cache_resource
def get_custom_suggestions():
    return {
        'tonite': ['tonight'],
        'tomorow': ['tomorrow'],
        'yestarday': ['yesterday'],
        'definately': ['definitely'],
        'recieve': ['receive'],
        'occurence': ['occurrence'],
        'calender': ['calendar'],
        'wierd': ['weird'],
        'alot': ['a lot'],
        'untill': ['until'],
        'thier': ['their'],
        'truely': ['truly'],
        'begining': ['beginning'],
        'beleive': ['believe'],
        'seperate': ['separate'],
        'goverment': ['government'],
        'neccessary': ['necessary'],
        'occasionaly': ['occasionally'],
        'independant': ['independent'],
        'basicly': ['basically'],
        'wich': ['which'],
        'wont': ["won't"],
        'cant': ["can't"],
        'dont': ["don't"],
        'isnt': ["isn't"],
        'didnt': ["didn't"],
        'couldnt': ["couldn't"],
        'shouldnt': ["shouldn't"],
        'wouldnt': ["wouldn't"],
        'doesnt': ["doesn't"],
        'wasnt': ["wasn't"],
        'werent': ["weren't"],
        'havent': ["haven't"],
        'hasnt': ["hasn't"],
        'hadnt': ["hadn't"],
        'arent': ["aren't"]
    }

# ë¬¸ë²• ê²€ì‚¬ í•¨ìˆ˜ (TextBlob ì‚¬ìš©)
def check_grammar(text):
    if not text.strip():
        return []
    
    errors = []
    
    try:
        blob = TextBlob(text)
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ì„
        sentences = custom_sent_tokenize(text)
        
        # ë§ì¶¤ë²• ê²€ì‚¬
        checker = get_spell_checker()
        words = custom_word_tokenize(text)
        
        # ì‚¬ìš©ì ì •ì˜ ì œì•ˆ ì‚¬ì „ ë¡œë“œ
        custom_suggestions = get_custom_suggestions()
        
        # ë¬¸ì¥ì˜ ì‹œì‘ ìœ„ì¹˜ë¥¼ ì¶”ì í•˜ê¸° ìœ„í•œ ë³€ìˆ˜
        offset = 0
        
        for sentence in sentences:
            # ê°„ë‹¨í•œ ë¬¸ë²• ê²€ì‚¬ (TextBlobì˜ sentiment ì‚¬ìš©)
            try:
                sentence_blob = TextBlob(sentence)
                
                # ë¬¸ì¥ì˜ ë‹¨ì–´ ë¶„ì„
                for word in custom_word_tokenize(sentence):
                    if word.isalpha():  # ì•ŒíŒŒë²³ ë‹¨ì–´ë§Œ í™•ì¸
                        # ë§ì¶¤ë²• í™•ì¸ (enchantê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ)
                        if checker and has_enchant and not checker.check(word):
                            # ì‚¬ìš©ì ì •ì˜ ì œì•ˆì´ ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸
                            word_lower = word.lower()
                            if word_lower in custom_suggestions:
                                suggestions = custom_suggestions[word_lower]
                            else:
                                # ê¸°ì¡´ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì œì•ˆ ì‚¬ìš©
                                suggestions = checker.suggest(word)[:3]  # ìµœëŒ€ 3ê°œ ì œì•ˆ
                            
                            word_offset = text.find(word, offset)
                            
                            if word_offset != -1:
                                errors.append({
                                    "offset": word_offset,
                                    "errorLength": len(word),
                                    "message": f"ë§ì¶¤ë²• ì˜¤ë¥˜: '{word}'",
                                    "replacements": suggestions
                                })
                        # PySpellChecker ì‚¬ìš© (enchant ëŒ€ì‹ )
                        elif checker and 'has_spellchecker' in globals() and has_spellchecker and word.lower() not in checker:
                            # ì‚¬ìš©ì ì •ì˜ ì œì•ˆì´ ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸
                            word_lower = word.lower()
                            if word_lower in custom_suggestions:
                                suggestions = custom_suggestions[word_lower]
                            else:
                                # PySpellChecker ì œì•ˆ ì‚¬ìš©
                                suggestions = [checker.correction(word)] + list(checker.candidates(word))[:2]
                            
                            word_offset = text.find(word, offset)
                            
                            if word_offset != -1:
                                errors.append({
                                    "offset": word_offset,
                                    "errorLength": len(word),
                                    "message": f"ë§ì¶¤ë²• ì˜¤ë¥˜: '{word}'",
                                    "replacements": suggestions
                                })
                        # TextBlobì„ ì‚¬ìš©í•œ ë§ì¶¤ë²• ê²€ì‚¬ ëŒ€ì•ˆ (ë‹¤ë¥¸ ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ëŠ” ê²½ìš°)
                        elif not checker:
                            try:
                                # ì‚¬ìš©ì ì •ì˜ ì œì•ˆì´ ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸
                                word_lower = word.lower()
                                if word_lower in custom_suggestions:
                                    corrected = custom_suggestions[word_lower][0]  # ì²« ë²ˆì§¸ ì œì•ˆ ì‚¬ìš©
                                    word_offset = text.find(word, offset)
                                    if word_offset != -1:
                                        errors.append({
                                            "offset": word_offset,
                                            "errorLength": len(word),
                                            "message": f"ë§ì¶¤ë²• ì˜¤ë¥˜: '{word}'",
                                            "replacements": custom_suggestions[word_lower]
                                        })
                                else:
                                    # TextBlobì„ ì‚¬ìš©í•œ ê¸°ì¡´ ë¡œì§
                                    word_blob = TextBlob(word)
                                    corrected = str(word_blob.correct())
                                    if corrected != word and len(word) > 3:  # ì§§ì€ ë‹¨ì–´ëŠ” ë¬´ì‹œ
                                        word_offset = text.find(word, offset)
                                        if word_offset != -1:
                                            errors.append({
                                                "offset": word_offset,
                                                "errorLength": len(word),
                                                "message": f"ë§ì¶¤ë²• ì˜¤ë¥˜: '{word}'",
                                                "replacements": [corrected]
                                            })
                            except Exception as e:
                                # TextBlob ë§ì¶¤ë²• ê²€ì‚¬ ì˜¤ë¥˜ ë¬´ì‹œ
                                pass
            except Exception as e:
                # ë¬¸ì¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒì‹œ í•´ë‹¹ ë¬¸ì¥ ê±´ë„ˆë›°ê¸°
                pass
                
            # ë‹¤ìŒ ë¬¸ì¥ì˜ ê²€ìƒ‰ì„ ìœ„í•´ ì˜¤í”„ì…‹ ì—…ë°ì´íŠ¸
            offset += len(sentence)
    except Exception as e:
        st.error(f"í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    
    return errors

# í…ìŠ¤íŠ¸ í†µê³„ ë¶„ì„ í•¨ìˆ˜
def analyze_text(text):
    if not text.strip():
        return {
            'word_count': 0,
            'sentence_count': 0,
            'avg_word_length': 0,
            'avg_sentence_length': 0,
            'vocabulary_size': 0
        }
    
    sentences = custom_sent_tokenize(text)
    words = custom_word_tokenize(text)
    words = [word.lower() for word in words if re.match(r'\w+', word)]
    
    word_count = len(words)
    sentence_count = len(sentences)
    avg_word_length = sum(len(word) for word in words) / max(1, word_count)
    avg_sentence_length = word_count / max(1, sentence_count)
    vocabulary_size = len(set(words))
    
    return {
        'word_count': word_count,
        'sentence_count': sentence_count,
        'avg_word_length': round(avg_word_length, 2),
        'avg_sentence_length': round(avg_sentence_length, 2),
        'vocabulary_size': vocabulary_size
    }

# ì–´íœ˜ ë¶„ì„ í•¨ìˆ˜
def analyze_vocabulary(text):
    if not text.strip():
        return {
            'word_freq': {},
            'pos_dist': {}
        }
    
    words = custom_word_tokenize(text.lower())
    words = [word for word in words if re.match(r'\w+', word)]
    
    # ë¶ˆìš©ì–´ ì œê±°
    stop_words = set(stopwords.words('english'))
    filtered_words = [word for word in words if word not in stop_words]
    
    # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
    word_freq = Counter(filtered_words).most_common(20)
    
    return {
        'word_freq': dict(word_freq)
    }

# ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚°
def calculate_lexical_diversity(text):
    words = custom_word_tokenize(text.lower())
    words = [word for word in words if re.match(r'\w+', word)]
    
    if len(words) == 0:
        return 0
    
    return len(set(words)) / len(words)

# ë‹¨ì–´ ë¹ˆë„ ì‹œê°í™”
def plot_word_frequency(word_freq):
    if not word_freq:
        return None
    
    df = pd.DataFrame(list(word_freq.items()), columns=['ë‹¨ì–´', 'ë¹ˆë„'])
    df = df.sort_values('ë¹ˆë„', ascending=True)
    
    fig = px.bar(df.tail(10), x='ë¹ˆë„', y='ë‹¨ì–´', orientation='h', 
                title='ìƒìœ„ 10ê°œ ë‹¨ì–´ ë¹ˆë„')
    fig.update_layout(height=400)
    
    return fig

# í•™ìƒ í˜ì´ì§€
def evaluate_vocabulary_level(text):
    # ì˜¨ë¼ì¸ ë°ì´í„°ì…‹ì—ì„œ ì–´íœ˜ ë¡œë“œ
    vocabulary_sets = load_vocabulary_datasets()
    
    words = custom_word_tokenize(text.lower())
    words = [word for word in words if re.match(r'\w+', word)]
    
    word_set = set(words)
    
    basic_count = len(word_set.intersection(vocabulary_sets['basic']))
    intermediate_count = len(word_set.intersection(vocabulary_sets['intermediate']))
    advanced_count = len(word_set.intersection(vocabulary_sets['advanced']))
    
    total = basic_count + intermediate_count + advanced_count
    if total == 0:
        return {'basic': 0, 'intermediate': 0, 'advanced': 0}
    
    return {
        'basic': basic_count / max(1, total),
        'intermediate': intermediate_count / max(1, total),
        'advanced': advanced_count / max(1, total)
    }

# í•™ìƒ í˜ì´ì§€
def show_student_page():
    st.title("ì˜ì‘ë¬¸ ìë™ ì²¨ì‚­ ì‹œìŠ¤í…œ - í•™ìƒ")
    
    # ë¡œê·¸ì•„ì›ƒ ë²„íŠ¼
    if st.button("ë¡œê·¸ì•„ì›ƒ", key="student_logout"):
        st.session_state.user_type = None
        st.rerun()
    
    tabs = st.tabs(["ì˜ì‘ë¬¸ ê²€ì‚¬", "ì˜ì‘ë¬¸ ì¬ì‘ì„±", "ë‚´ ì‘ë¬¸ ê¸°ë¡"])
    
    # ì˜ì‘ë¬¸ ê²€ì‚¬ íƒ­
    with tabs[0]:
        st.subheader("ì˜ì‘ë¬¸ ì…ë ¥")
        user_text = st.text_area("ì•„ë˜ì— ì˜ì–´ ì‘ë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", height=200, key="text_tab1")
        
        col1, col2 = st.columns([3, 1])
        
        # ëª¨ë“  ë¶„ì„ì„ í•œ ë²ˆì— ì‹¤í–‰í•˜ëŠ” ë²„íŠ¼
        with col1:
            if st.button("ì „ì²´ ë¶„ì„í•˜ê¸°", use_container_width=True, key="analyze_button"):
                    if not user_text:
                        st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    else:
                    # í…ìŠ¤íŠ¸ í†µê³„ ë¶„ì„
                        stats = analyze_text(user_text)
                    
                        try:
                            # ë¬¸ë²• ì˜¤ë¥˜ ê²€ì‚¬
                            grammar_errors = check_grammar(user_text)
                        except Exception as e:
                            st.error(f"ë¬¸ë²• ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                            grammar_errors = []
                        
                    # ì–´íœ˜ ë¶„ì„
                    vocab_analysis = analyze_vocabulary(user_text)
                    
                    # ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜
                    diversity_score = calculate_lexical_diversity(user_text)
                    
                    # ì–´íœ˜ ìˆ˜ì¤€ í‰ê°€
                    vocab_level = evaluate_vocabulary_level(user_text)
                    
                    # ì„¸ì…˜ ìƒíƒœì— ê²°ê³¼ ì €ì¥
                    if 'analysis_results' not in st.session_state:
                        st.session_state.analysis_results = {}
                    
                    st.session_state.analysis_results = {
                        'stats': stats,
                        'grammar_errors': grammar_errors,
                        'vocab_analysis': vocab_analysis,
                        'diversity_score': diversity_score,
                        'vocab_level': vocab_level,
                        'original_text': user_text  # ì›ë³¸ í…ìŠ¤íŠ¸ë„ ì €ì¥
                    }
                            
                            # ê¸°ë¡ì— ì €ì¥
                    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    st.session_state.history.append({
                                'timestamp': timestamp,
                                'text': user_text,
                        'error_count': len(grammar_errors) if grammar_errors else 0
                    })
                    
                    st.success("ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì•„ë˜ íƒ­ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                    
                    # ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŒì„ í‘œì‹œí•˜ëŠ” í”Œë˜ê·¸
                    st.session_state.analysis_completed = True
                    st.rerun()  # ì¬ì‹¤í–‰í•˜ì—¬ ë²„íŠ¼ í‘œì‹œ ì—…ë°ì´íŠ¸
        
        # ì¬ì‘ì„± ì¶”ì²œ ë²„íŠ¼ ì¶”ê°€
        with col2:
            # ë¶„ì„ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë²„íŠ¼ í‘œì‹œ
            if 'analysis_results' in st.session_state and 'original_text' in st.session_state.analysis_results:
                if st.button("âœ¨ ì˜ì‘ë¬¸ ì¬ì‘ì„± ì¶”ì²œ âœ¨", 
                          key="rewrite_recommendation",
                          use_container_width=True,
                          help="ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ì‘ë¬¸ì„ ë” ì¢‹ì€ í‘œí˜„ìœ¼ë¡œ ì¬ì‘ì„±í•´ë³´ì„¸ìš”!",
                          type="primary"):
                    # í…ìŠ¤íŠ¸ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                    st.session_state.copy_to_rewrite = st.session_state.analysis_results['original_text']
                    
                    # ì‚¬ìš©ìì—ê²Œ ì•ˆë‚´ ë©”ì‹œì§€ í‘œì‹œ
                    st.success("í…ìŠ¤íŠ¸ê°€ ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒë‹¨ì˜ 'ì˜ì‘ë¬¸ ì¬ì‘ì„±' íƒ­ì„ í´ë¦­í•˜ì„¸ìš”!")
                    st.balloons()  # ì‹œê°ì  íš¨ê³¼ ì¶”ê°€
        
        # ê²°ê³¼ í‘œì‹œë¥¼ ìœ„í•œ íƒ­
        result_tab1, result_tab2, result_tab3 = st.tabs(["ë¬¸ë²• ê²€ì‚¬", "ì–´íœ˜ ë¶„ì„", "í…ìŠ¤íŠ¸ í†µê³„"])
        
        with result_tab1:
            if 'analysis_results' in st.session_state and 'grammar_errors' in st.session_state.analysis_results:
                grammar_errors = st.session_state.analysis_results['grammar_errors']
                
                if grammar_errors:
                    st.subheader("ë¬¸ë²• ì˜¤ë¥˜ ëª©ë¡")
                    
                    error_data = []
                    for error in grammar_errors:
                        error_data.append({
                            "ì˜¤ë¥˜": user_text[error['offset']:error['offset'] + error['errorLength']],
                            "ì˜¤ë¥˜ ë‚´ìš©": error['message'],
                            "ìˆ˜ì • ì œì•ˆ": error['replacements']
                        })
                    
                    st.dataframe(pd.DataFrame(error_data))
                else:
                    st.success("ë¬¸ë²• ì˜¤ë¥˜ê°€ ì—†ìŠµë‹ˆë‹¤!")
        
        with result_tab2:
            if 'analysis_results' in st.session_state and 'vocab_analysis' in st.session_state.analysis_results:
                vocab_analysis = st.session_state.analysis_results['vocab_analysis']
                diversity_score = st.session_state.analysis_results['diversity_score']
                vocab_level = st.session_state.analysis_results['vocab_level']
                        
                # ë‹¨ì–´ ë¹ˆë„ ì‹œê°í™”
                fig = plot_word_frequency(vocab_analysis['word_freq'])
                if fig:
                    st.plotly_chart(fig, use_container_width=True)
                    
                # ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜
                st.metric("ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜", f"{diversity_score:.2f}", 
                         delta="ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•œ ì–´íœ˜ ì‚¬ìš©")
                
                # ì–´íœ˜ ìˆ˜ì¤€ í‰ê°€
                level_df = pd.DataFrame({
                    'ìˆ˜ì¤€': ['ê¸°ì´ˆ', 'ì¤‘ê¸‰', 'ê³ ê¸‰'],
                    'ë¹„ìœ¨': [vocab_level['basic'], vocab_level['intermediate'], vocab_level['advanced']]
                })
                
                fig = px.pie(level_df, values='ë¹„ìœ¨', names='ìˆ˜ì¤€', 
                            title='ì–´íœ˜ ìˆ˜ì¤€ ë¶„í¬',
                            color_discrete_sequence=px.colors.sequential.Viridis)
                st.plotly_chart(fig, use_container_width=True)
        
        with result_tab3:
            if 'analysis_results' in st.session_state and 'stats' in st.session_state.analysis_results:
                stats = st.session_state.analysis_results['stats']
                    
                st.subheader("í…ìŠ¤íŠ¸ í†µê³„")
                    
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("ë‹¨ì–´ ìˆ˜", stats['word_count'])
                    st.metric("ë¬¸ì¥ ìˆ˜", stats['sentence_count'])
                with col2:
                    st.metric("í‰ê·  ë‹¨ì–´ ê¸¸ì´", stats['avg_word_length'])
                    st.metric("í‰ê·  ë¬¸ì¥ ê¸¸ì´ (ë‹¨ì–´)", stats['avg_sentence_length'])
                    
                    st.metric("ì–´íœ˜ í¬ê¸° (ê³ ìœ  ë‹¨ì–´ ìˆ˜)", stats['vocabulary_size'])
                    
                    # ê²Œì´ì§€ ì°¨íŠ¸ë¡œ í‘œí˜„í•˜ê¸°
                    progress_col1, progress_col2 = st.columns(2)
                with progress_col1:
                        # í‰ê·  ë¬¸ì¥ ê¸¸ì´ ê²Œì´ì§€ (ì ì • ì˜ì–´ ë¬¸ì¥ ê¸¸ì´: 15-20 ë‹¨ì–´)
                    sentence_gauge = min(1.0, stats['avg_sentence_length'] / 20)
                    st.progress(sentence_gauge)
                    st.caption(f"ë¬¸ì¥ ê¸¸ì´ ì ì •ì„±: {int(sentence_gauge * 100)}%")
                    
                with progress_col2:
                        # ì–´íœ˜ ë‹¤ì–‘ì„± ê²Œì´ì§€
                    vocab_ratio = stats['vocabulary_size'] / max(1, stats['word_count'])
                    st.progress(min(1.0, vocab_ratio * 2))  # 0.5 ì´ìƒì´ë©´ 100%
                    st.caption(f"ì–´íœ˜ ë‹¤ì–‘ì„±: {int(min(1.0, vocab_ratio * 2) * 100)}%")
    
    # ì˜ì‘ë¬¸ ì¬ì‘ì„± íƒ­
    with tabs[1]:
        st.subheader("ì˜ì‘ë¬¸ ì¬ì‘ì„±")
        
        # ì™¼ìª½ ì—´: ì…ë ¥ ë° ì˜µì…˜
        left_col, right_col = st.columns(2)
        
        with left_col:
            # ë¶„ì„ íƒ­ì—ì„œ ë„˜ì–´ì˜¨ ê²½ìš° í•´ë‹¹ í…ìŠ¤íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ë¡œë“œ
            default_text = ""
            if 'copy_to_rewrite' in st.session_state:
                default_text = st.session_state.copy_to_rewrite
                st.success("ë¶„ì„ ê²°ê³¼ í…ìŠ¤íŠ¸ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
                # í•œ ë²ˆ ì‚¬ìš© í›„ ì„ì‹œ ë³€ìˆ˜ë¡œ ì˜®ê²¨ ì €ì¥
                st.session_state.copy_to_rewrite_temp = default_text
                del st.session_state.copy_to_rewrite
            elif 'copy_to_rewrite_temp' in st.session_state:
                default_text = st.session_state.copy_to_rewrite_temp
            
            rewrite_text_input = st.text_area("ì•„ë˜ì— ì˜ì–´ ì‘ë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", 
                                            value=default_text,
                                            height=200, 
                                            key="text_tab2")
            
            level_option = st.radio(
                "ì‘ë¬¸ ìˆ˜ì¤€ ì„ íƒ",
                options=["ë¹„ìŠ·í•œ ìˆ˜ì¤€", "ì•½ê°„ ë†’ì€ ìˆ˜ì¤€", "ê³ ê¸‰ ìˆ˜ì¤€"],
                horizontal=True
            )
            
            level_map = {
                "ë¹„ìŠ·í•œ ìˆ˜ì¤€": "similar",
                "ì•½ê°„ ë†’ì€ ìˆ˜ì¤€": "improved",
                "ê³ ê¸‰ ìˆ˜ì¤€": "advanced"
            }
            
            if st.button("ì¬ì‘ì„±í•˜ê¸°"):
                if not rewrite_text_input:
                    st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                else:
                    level = level_map.get(level_option, "similar")
                    
                    # ì¬ì‘ì„± ì²˜ë¦¬
                    with st.spinner("í…ìŠ¤íŠ¸ë¥¼ ì¬ì‘ì„± ì¤‘ì…ë‹ˆë‹¤..."):
                        rewritten_text = rewrite_text(rewrite_text_input, level)
                        
                        # ì¬ì‘ì„±ëœ í…ìŠ¤íŠ¸ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                        if 'rewritten_text' not in st.session_state:
                            st.session_state.rewritten_text = {}
                        
                        st.session_state.rewritten_text[level] = rewritten_text
                        
                        # ê¸°ë¡ì— ì¶”ê°€
                        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        st.session_state.history.append({
                            'timestamp': timestamp,
                            'text': rewrite_text_input,
                            'action': f"ì¬ì‘ì„± ({level_option})"
                        })
        
        with right_col:
            st.subheader("ì¬ì‘ì„± ê²°ê³¼")
            
            if 'rewritten_text' in st.session_state and st.session_state.rewritten_text:
                level = level_map.get(level_option, "similar")
                
                if level in st.session_state.rewritten_text:
                    rewritten = st.session_state.rewritten_text[level]
                    st.text_area("ì¬ì‘ì„±ëœ í…ìŠ¤íŠ¸", value=rewritten, height=250, key="rewritten_result")
                    
                    # ì¬ì‘ì„± í…ìŠ¤íŠ¸ ë³µì‚¬ ê¸°ëŠ¥ ëŒ€ì‹  ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ ì œê³µ
                    if rewritten:
                        output = io.BytesIO()
                        output.write(rewritten.encode('utf-8'))
                        output.seek(0)
                        
                        st.download_button(
                            label="ì¬ì‘ì„± í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ",
                            data=output,
                            file_name=f"rewritten_text_{level}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                            mime="text/plain"
                        )
                    
                    # ì›ë³¸ê³¼ ì¬ì‘ì„± í…ìŠ¤íŠ¸ ë¹„êµ
                    if rewrite_text_input and rewritten:
                        st.subheader("ì›ë³¸ vs ì¬ì‘ì„± ë¹„êµ")
                        
                        comparison_data = []
                        original_sentences = custom_sent_tokenize(rewrite_text_input)
                        rewritten_sentences = custom_sent_tokenize(rewritten)
                        
                        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¹„êµ (ë” ì§§ì€ ë¦¬ìŠ¤íŠ¸ ê¸°ì¤€)
                        for i in range(min(len(original_sentences), len(rewritten_sentences))):
                            comparison_data.append({
                                "ì›ë³¸": original_sentences[i],
                                "ì¬ì‘ì„±": rewritten_sentences[i]
                            })
                        
                        if comparison_data:
                            st.dataframe(pd.DataFrame(comparison_data), use_container_width=True)
            else:
                st.info("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê³  ì¬ì‘ì„± ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
    
    # ë‚´ ì‘ë¬¸ ê¸°ë¡ íƒ­
    with tabs[2]:
        st.subheader("ë‚´ ì‘ë¬¸ ê¸°ë¡")
        if not st.session_state.history:
            st.info("ì•„ì§ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            history_df = pd.DataFrame(st.session_state.history)
            st.dataframe(history_df)
            
            # ì˜¤ë¥˜ ìˆ˜ ì¶”ì´ ì°¨íŠ¸
            if len(history_df) > 1 and 'error_count' in history_df.columns:
                fig = px.line(history_df, x='timestamp', y='error_count', 
                            title='ë¬¸ë²• ì˜¤ë¥˜ ìˆ˜ ì¶”ì´',
                            labels={'timestamp': 'ë‚ ì§œ', 'error_count': 'ì˜¤ë¥˜ ìˆ˜'})
                st.plotly_chart(fig, use_container_width=True)

# êµì‚¬ í˜ì´ì§€
def show_teacher_page():
    st.title("ì˜ì‘ë¬¸ ìë™ ì²¨ì‚­ ì‹œìŠ¤í…œ - êµì‚¬")
    
    # ë¡œê·¸ì•„ì›ƒ ë²„íŠ¼
    if st.button("ë¡œê·¸ì•„ì›ƒ", key="teacher_logout"):
        st.session_state.user_type = None
        st.rerun()
    
    # íƒ­ ìƒì„±
    tabs = st.tabs(["ì˜ì‘ë¬¸ ì²¨ì‚­", "í•™ìŠµ ëŒ€ì‹œë³´ë“œ"])
    
    # ì˜ì‘ë¬¸ ì²¨ì‚­ íƒ­
    with tabs[0]:
        st.subheader("ì˜ì‘ë¬¸ ì…ë ¥ ë° ì²¨ì‚­")
        
        user_text = st.text_area("í•™ìƒì˜ ì˜ì–´ ì‘ë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", height=200, key="teacher_text")
        
        col1, col2 = st.columns([3, 1])
        
        # ëª¨ë“  ë¶„ì„ì„ í•œ ë²ˆì— ì‹¤í–‰í•˜ëŠ” ë²„íŠ¼
        with col1:
            if st.button("ì „ì²´ ë¶„ì„í•˜ê¸°", key="teacher_analyze_all", use_container_width=True):
                    if not user_text:
                        st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    else:
                        try:
                            # ë¬¸ë²• ì˜¤ë¥˜ ê²€ì‚¬
                            grammar_errors = check_grammar(user_text)
                        except Exception as e:
                            st.error(f"ë¬¸ë²• ê²€ì‚¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                            grammar_errors = []
                        
                    # ì–´íœ˜ ë¶„ì„
                    vocab_analysis = analyze_vocabulary(user_text)
                    
                    # ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜
                    diversity_score = calculate_lexical_diversity(user_text)
                    
                    # ì–´íœ˜ ìˆ˜ì¤€ í‰ê°€
                    vocab_level = evaluate_vocabulary_level(user_text)
                    
                    # í…ìŠ¤íŠ¸ í†µê³„ ë¶„ì„
                    stats = analyze_text(user_text)
                    
                    # ì„¸ì…˜ ìƒíƒœì— ê²°ê³¼ ì €ì¥
                    if 'teacher_analysis_results' not in st.session_state:
                        st.session_state.teacher_analysis_results = {}
                    
                    st.session_state.teacher_analysis_results = {
                        'stats': stats,
                        'grammar_errors': grammar_errors,
                        'vocab_analysis': vocab_analysis,
                        'diversity_score': diversity_score,
                        'vocab_level': vocab_level,
                        'original_text': user_text  # ì›ë³¸ í…ìŠ¤íŠ¸ë„ ì €ì¥
                    }
                    
                    st.success("ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì•„ë˜ íƒ­ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        
        # ì¬ì‘ì„± ì¶”ì²œ ë²„íŠ¼ ì¶”ê°€
        with col2:
            # ë¶„ì„ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë²„íŠ¼ í‘œì‹œ
            if 'teacher_analysis_results' in st.session_state and 'original_text' in st.session_state.teacher_analysis_results:
                if st.button("âœ¨ ì˜ì‘ë¬¸ ì¬ì‘ì„± ì¶”ì²œ âœ¨", 
                          key="teacher_rewrite_recommendation",
                          use_container_width=True,
                          help="ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìƒì˜ ì˜ì‘ë¬¸ì„ ë” ì¢‹ì€ í‘œí˜„ìœ¼ë¡œ ì¬ì‘ì„±í•´ë³´ì„¸ìš”!",
                          type="primary"):
                    # êµì‚¬ ì²¨ì‚­ ì˜ì—­ì— ëª¨ë²” ë‹µì•ˆ ì‘ì„±ìš©ìœ¼ë¡œ ì¶”ê°€
                    # ì¬ì‘ì„± í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¦‰ì‹œ ê³ ê¸‰ ìˆ˜ì¤€ìœ¼ë¡œ ì¬ì‘ì„±
                    original_text = st.session_state.teacher_analysis_results['original_text']
                    rewritten_text = rewrite_text(original_text, "advanced")
                    
                    # ì²¨ì‚­ ë…¸íŠ¸ì— ì¬ì‘ì„±ëœ í…ìŠ¤íŠ¸ ì¶”ê°€
                    if 'feedback_template' not in st.session_state:
                        st.session_state.feedback_template = "ë‹¤ìŒì€ í•™ìƒ ì‘ë¬¸ì„ ê³ ê¸‰ ìˆ˜ì¤€ìœ¼ë¡œ ì¬ì‘ì„±í•œ ì˜ˆì‹œì…ë‹ˆë‹¤:\n\n" + rewritten_text
                    else:
                        st.session_state.feedback_template += "\n\nì¶”ì²œ ëª¨ë²” ì˜ˆì‹œ:\n" + rewritten_text
                    
                    st.success("ê³ ê¸‰ ìˆ˜ì¤€ìœ¼ë¡œ ì¬ì‘ì„±ëœ í…ìŠ¤íŠ¸ê°€ ì²¨ì‚­ ë…¸íŠ¸ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    st.balloons()  # ì‹œê°ì  íš¨ê³¼ ì¶”ê°€
        
        # ê²°ê³¼ í‘œì‹œë¥¼ ìœ„í•œ íƒ­
        result_tab1, result_tab2, result_tab3 = st.tabs(["ë¬¸ë²• ê²€ì‚¬", "ì–´íœ˜ ë¶„ì„", "í…ìŠ¤íŠ¸ í†µê³„"])
        
        with result_tab1:
            if 'teacher_analysis_results' in st.session_state and 'grammar_errors' in st.session_state.teacher_analysis_results:
                grammar_errors = st.session_state.teacher_analysis_results['grammar_errors']
                        
                if grammar_errors:
                            st.subheader("ë¬¸ë²• ì˜¤ë¥˜ ëª©ë¡")
                            
                            error_data = []
                            for error in grammar_errors:
                                error_data.append({
                            "ì˜¤ë¥˜": user_text[error['offset']:error['offset'] + error['errorLength']],
                            "ì˜¤ë¥˜ ë‚´ìš©": error['message'],
                            "ìˆ˜ì • ì œì•ˆ": error['replacements']
                                })
                            
                            st.dataframe(pd.DataFrame(error_data))
                else:
                            st.success("ë¬¸ë²• ì˜¤ë¥˜ê°€ ì—†ìŠµë‹ˆë‹¤!")
            
        with result_tab2:
            if 'teacher_analysis_results' in st.session_state and 'vocab_analysis' in st.session_state.teacher_analysis_results:
                vocab_analysis = st.session_state.teacher_analysis_results['vocab_analysis']
                diversity_score = st.session_state.teacher_analysis_results['diversity_score']
                vocab_level = st.session_state.teacher_analysis_results['vocab_level']
                        
                # ë‹¨ì–´ ë¹ˆë„ ì‹œê°í™”
                fig = plot_word_frequency(vocab_analysis['word_freq'])
                if fig:
                    st.plotly_chart(fig, use_container_width=True)
                    
                # ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜
                st.metric("ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜", f"{diversity_score:.2f}")
                
                # ì–´íœ˜ ìˆ˜ì¤€ í‰ê°€
                level_df = pd.DataFrame({
                    'ìˆ˜ì¤€': ['ê¸°ì´ˆ', 'ì¤‘ê¸‰', 'ê³ ê¸‰'],
                    'ë¹„ìœ¨': [vocab_level['basic'], vocab_level['intermediate'], vocab_level['advanced']]
                })
                
                fig = px.pie(level_df, values='ë¹„ìœ¨', names='ìˆ˜ì¤€', 
                            title='ì–´íœ˜ ìˆ˜ì¤€ ë¶„í¬')
                st.plotly_chart(fig, use_container_width=True)
        
        with result_tab3:
            if 'teacher_analysis_results' in st.session_state and 'stats' in st.session_state.teacher_analysis_results:
                stats = st.session_state.teacher_analysis_results['stats']
                    
                st.subheader("í…ìŠ¤íŠ¸ í†µê³„")
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("ë‹¨ì–´ ìˆ˜", stats['word_count'])
                    st.metric("ë¬¸ì¥ ìˆ˜", stats['sentence_count'])
                with col2:
                    st.metric("í‰ê·  ë‹¨ì–´ ê¸¸ì´", stats['avg_word_length'])
                    st.metric("í‰ê·  ë¬¸ì¥ ê¸¸ì´ (ë‹¨ì–´)", stats['avg_sentence_length'])
                    
                st.metric("ì–´íœ˜ í¬ê¸° (ê³ ìœ  ë‹¨ì–´ ìˆ˜)", stats['vocabulary_size'])
        
        # êµì‚¬ ì „ìš© ê¸°ëŠ¥
        st.subheader("ì²¨ì‚­ ë° í”¼ë“œë°±")
        
        # ì²¨ì‚­ ë…¸íŠ¸ í…œí”Œë¦¿
        feedback_default = "ë‹¤ìŒ ì‚¬í•­ì„ ì¤‘ì ì ìœ¼ë¡œ ê°œì„ í•´ë³´ì„¸ìš”:\n1. \n2. \n3. "
        if 'feedback_template' in st.session_state:
            feedback_default = st.session_state.feedback_template
        
        feedback = st.text_area("ì²¨ì‚­ ë…¸íŠ¸", 
                             value=feedback_default, 
                             height=200,  # ë” ë†’ê²Œ ì¡°ì •
                 key="feedback_template")
        
        # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        st.session_state.feedback_template = feedback
        
        # ì ìˆ˜ ì…ë ¥
        score_col1, score_col2, score_col3 = st.columns(3)
        with score_col1:
            grammar_score = st.slider("ë¬¸ë²• ì ìˆ˜", 0, 10, 5)
        with score_col2:
            vocab_score = st.slider("ì–´íœ˜ ì ìˆ˜", 0, 10, 5)
        with score_col3:
            content_score = st.slider("ë‚´ìš© ì ìˆ˜", 0, 10, 5)
        
        total_score = (grammar_score + vocab_score + content_score) / 3
        st.metric("ì¢…í•© ì ìˆ˜", f"{total_score:.1f} / 10")
        
        # ì €ì¥ ì˜µì…˜
        if st.button("ì²¨ì‚­ ê²°ê³¼ ì €ì¥ (Excel)"):
            if not user_text:
                st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                # ë¬¸ë²• ì˜¤ë¥˜ ê²€ì‚¬
                grammar_errors = check_grammar(user_text)
                
                # ì €ì¥í•  ë°ì´í„° ìƒì„±
                error_data = []
                for error in grammar_errors:
                    error_data.append({
                        "ì˜¤ë¥˜": user_text[error['offset']:error['offset'] + error['errorLength']],
                        "ì˜¤ë¥˜ ë‚´ìš©": error['message'],
                        "ìˆ˜ì • ì œì•ˆ": str(error['replacements']),
                        "ìœ„ì¹˜": f"{error['offset']}:{error['offset'] + error['errorLength']}"
                    })
                
                # ì–´íœ˜ ë¶„ì„
                vocab_analysis = analyze_vocabulary(user_text)
                
                # í†µê³„ ë¶„ì„
                stats = analyze_text(user_text)
                
                # ì—¬ëŸ¬ ì‹œíŠ¸ê°€ ìˆëŠ” Excel íŒŒì¼ ìƒì„±
                excel_buffer = io.BytesIO()
                with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                    # ì›ë³¸ í…ìŠ¤íŠ¸ ì‹œíŠ¸
                    pd.DataFrame({"ì›ë³¸ í…ìŠ¤íŠ¸": [user_text]}).to_excel(writer, sheet_name="ì›ë³¸ í…ìŠ¤íŠ¸", index=False)
                    
                    # ì˜¤ë¥˜ ë°ì´í„° ì‹œíŠ¸
                    pd.DataFrame(error_data).to_excel(writer, sheet_name="ë¬¸ë²• ì˜¤ë¥˜", index=False)
                    
                    # í†µê³„ ì‹œíŠ¸
                    stats_df = pd.DataFrame({
                        "í•­ëª©": ["ë‹¨ì–´ ìˆ˜", "ë¬¸ì¥ ìˆ˜", "í‰ê·  ë‹¨ì–´ ê¸¸ì´", "í‰ê·  ë¬¸ì¥ ê¸¸ì´", "ì–´íœ˜ í¬ê¸°"],
                        "ê°’": [stats['word_count'], stats['sentence_count'], stats['avg_word_length'], 
                             stats['avg_sentence_length'], stats['vocabulary_size']]
                    })
                    stats_df.to_excel(writer, sheet_name="í†µê³„", index=False)
                    
                    # í‰ê°€ ì ìˆ˜ ì‹œíŠ¸
                    score_df = pd.DataFrame({
                        "í‰ê°€ í•­ëª©": ["ë¬¸ë²•", "ì–´íœ˜", "ë‚´ìš©", "ì¢…í•© ì ìˆ˜"],
                        "ì ìˆ˜": [grammar_score, vocab_score, content_score, total_score]
                    })
                    score_df.to_excel(writer, sheet_name="í‰ê°€ ì ìˆ˜", index=False)
                    
                    # í”¼ë“œë°± ì‹œíŠ¸
                    pd.DataFrame({"ì²¨ì‚­ í”¼ë“œë°±": [feedback]}).to_excel(writer, sheet_name="í”¼ë“œë°±", index=False)
                
                # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                excel_buffer.seek(0)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                st.download_button(
                    label="Excel ë‹¤ìš´ë¡œë“œ",
                    data=excel_buffer,
                    file_name=f"ì²¨ì‚­ê²°ê³¼_{timestamp}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                )
    
    # í•™ìŠµ ëŒ€ì‹œë³´ë“œ íƒ­
    with tabs[1]:
        st.subheader("í•™ìŠµ ëŒ€ì‹œë³´ë“œ")
        
        # ìƒ˜í”Œ í•™ìƒ ë°ì´í„° (ì‹¤ì œë¡œëŠ” DBì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨)
        sample_data = {
            "ë‚ ì§œ": ["2023-01-01", "2023-01-08", "2023-01-15", "2023-01-22", "2023-01-29"],
            "í•™ìƒ_A": [7, 6, 8, 7, 9],
            "í•™ìƒ_B": [5, 6, 6, 7, 6],
            "í•™ìƒ_C": [8, 8, 7, 9, 9],
            "í•™ìƒ_D": [4, 5, 6, 6, 7]
        }
        
        scores_df = pd.DataFrame(sample_data)
        
        # í•™ìƒë³„ ì ìˆ˜ ì¶”ì´
        fig = px.line(scores_df, x="ë‚ ì§œ", y=["í•™ìƒ_A", "í•™ìƒ_B", "í•™ìƒ_C", "í•™ìƒ_D"],
                     title="í•™ìƒë³„ ì˜ì‘ë¬¸ ì ìˆ˜ ì¶”ì´",
                     labels={"value": "ì ìˆ˜", "variable": "í•™ìƒ"})
        st.plotly_chart(fig, use_container_width=True)
        
        # ìµœê·¼ ì ìˆ˜ ë¶„í¬
        latest_scores = scores_df.iloc[-1, 1:].values
        students = scores_df.columns[1:]
        
        fig = px.bar(x=students, y=latest_scores, 
                    title="ìµœê·¼ ì˜ì‘ë¬¸ ì ìˆ˜ ë¶„í¬",
                    labels={"x": "í•™ìƒ", "y": "ì ìˆ˜"})
        st.plotly_chart(fig, use_container_width=True)
        
        # í‰ê·  ì˜¤ë¥˜ ìœ í˜• (ìƒ˜í”Œ ë°ì´í„°)
        error_types = {
            "ì˜¤ë¥˜ ìœ í˜•": ["ë¬¸ì¥ êµ¬ì¡°", "ì‹œì œ", "ê´€ì‚¬", "ì „ì¹˜ì‚¬", "ëŒ€ëª…ì‚¬", "ì² ì", "êµ¬ë‘ì "],
            "ë¹ˆë„": [45, 30, 25, 20, 15, 10, 5]
        }
        
        error_df = pd.DataFrame(error_types)
        
        fig = px.pie(error_df, values="ë¹ˆë„", names="ì˜¤ë¥˜ ìœ í˜•",
                    title="í‰ê·  ì˜¤ë¥˜ ìœ í˜• ë¶„í¬")
        st.plotly_chart(fig, use_container_width=True)

# ë©”ì¸ í•¨ìˆ˜
def main():
    # ì œëª© ë° ì†Œê°œ
    st.title("ì˜ì‘ë¬¸ ìë™ ì²¨ì‚­ ì‹œìŠ¤í…œ")
    st.markdown("""
    ì´ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ í•™ìƒë“¤ì˜ ì˜ì‘ë¬¸ì„ ìë™ìœ¼ë¡œ ì²¨ì‚­í•˜ê³  í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.
    í•™ìƒì€ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê³  ìë™ ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """)
    
    # ì§ì ‘ í•™ìƒ í˜ì´ì§€ë¡œ ì´ë™
    show_student_page()

if __name__ == "__main__":
    main()

@st.cache_resource
def load_vocabulary_datasets():
    # ì˜¨ë¼ì¸ ì†ŒìŠ¤ì—ì„œ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ (ì‹¤ì œ ì‘ë™í•˜ëŠ” URLë¡œ ìˆ˜ì •)
    word_freq_url = "https://raw.githubusercontent.com/hermitdave/FrequencyWords/master/content/2018/en/en_50k.txt"
    
    try:
        import requests
        
        # ì˜ì–´ ë‹¨ì–´ ë¹ˆë„ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
        response = requests.get(word_freq_url)
        if response.status_code == 200:
            # ë‹¨ì–´ ë¹ˆë„ ë°ì´í„° íŒŒì‹± (í˜•ì‹: "ë‹¨ì–´ ë¹ˆë„")
            lines = response.text.splitlines()
            words = [line.split()[0] for line in lines if ' ' in line]
            
            # ë¹ˆë„ì— ë”°ë¼ ë‹¨ì–´ ë¶„ë¥˜
            total_words = len(words)
            basic_cutoff = int(total_words * 0.2)  # ìƒìœ„ 20%
            intermediate_cutoff = int(total_words * 0.5)  # ìƒìœ„ 20~50%
            
            basic_words = set(words[:basic_cutoff])
            intermediate_words = set(words[basic_cutoff:intermediate_cutoff])
            advanced_words = set(words[intermediate_cutoff:])
            
            return {'basic': basic_words, 'intermediate': intermediate_words, 'advanced': advanced_words}
    except Exception as e:
        st.warning(f"ì˜¨ë¼ì¸ ë°ì´í„°ì…‹ ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    
    # ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ì‹œ ë‚´ì¥ ë°ì´í„°ì…‹ ì‚¬ìš©
    return default_vocabulary_sets()

def evaluate_advanced_vocabulary(text):
    words = custom_word_tokenize(text.lower())
    
    # ë‹¨ì–´ ë¹ˆë„ ê¸°ë°˜ í‰ê°€
    word_frequencies = get_word_frequency_data()  # ë‹¨ì–´ë³„ ë¹ˆë„ ë°ì´í„° ë¡œë“œ
    
    rare_words = [w for w in words if w in word_frequencies and word_frequencies[w] < 0.001]
    
    # í•™ìˆ  ë‹¨ì–´ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    academic_word_list = get_academic_word_list()
    academic_words = [w for w in words if w in academic_word_list]
    
    vocab_score = (len(rare_words) * 2 + len(academic_words)) / max(len(words), 1)
    return vocab_score

@st.cache_resource
def load_rewrite_model():
    if not has_transformers:
        return None, None
        
    try:
        # T5 ë˜ëŠ” GPT ê¸°ë°˜ ëª¨ë¸ ë¡œë“œ
        model_name = "t5-base"  # ë˜ëŠ” "gpt2-medium"
        tokenizer = transformers.AutoTokenizer.from_pretrained(model_name)
        model = transformers.AutoModelForSeq2SeqLM.from_pretrained(model_name)
        return tokenizer, model
    except Exception as e:
        st.warning(f"ëª¨ë¸ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None, None

def advanced_rewrite_text(text, level='advanced'):
    tokenizer, model = load_rewrite_model()
    if not tokenizer or not model:
        # í´ë°±: ê¸°ì¡´ ê·œì¹™ ê¸°ë°˜ ë°©ì‹ ì‚¬ìš©
        return rewrite_advanced_level(text)
        
    try:
        import torch
        prefix = f"paraphrase to {level} level: "
        inputs = tokenizer(prefix + text, return_tensors="pt", max_length=512, truncation=True)
        
        outputs = model.generate(
            inputs.input_ids, 
            max_length=512,
            temperature=0.8,  # ì°½ì˜ì„± ì¡°ì ˆ
            num_return_sequences=1
        )
        
        return tokenizer.decode(outputs[0], skip_special_tokens=True)
    except Exception as e:
        st.warning(f"AI ëª¨ë¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}. ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        # í´ë°±: ê¸°ì¡´ ê·œì¹™ ê¸°ë°˜ ë°©ì‹ ì‚¬ìš©
        return rewrite_advanced_level(text)

def context_aware_rewrite(text, subject_area="general"):
    # ì£¼ì œë³„ ì–´íœ˜ ë°ì´í„°ì…‹ ë¡œë“œ
    domain_vocabulary = load_domain_vocabulary(subject_area)
    
    sentences = custom_sent_tokenize(text)
    rewritten = []
    
    # ë¬¸ë§¥ ìœ ì§€í•˜ë©° ì£¼ì œ ê´€ë ¨ ì–´íœ˜ë¡œ ê°•í™”
    for i, sentence in enumerate(sentences):
        # ì´ì „/ë‹¤ìŒ ë¬¸ì¥ ì°¸ì¡°í•´ì„œ ë¬¸ë§¥ ìœ ì§€
        prev_context = sentences[i-1] if i > 0 else ""
        next_context = sentences[i+1] if i < len(sentences)-1 else ""
        
        enhanced = enhance_sentence_with_domain(
            sentence, 
            domain_vocabulary, 
            prev_context, 
            next_context
        )
        rewritten.append(enhanced)
    
    return " ".join(rewritten)

def transform_grammar_structure(sentence, level):
    # ë¬¸ë²•ì  ë³µì¡ì„± ë ˆë²¨ ë§¤í•‘
    complexity_patterns = {
        "intermediate": [
            # ë‹¨ìˆœ ì‹œì œ â†’ ì™„ë£Œ ì‹œì œ
            (r'\b(do|does|did)\b', 'have done'),
            # ëŠ¥ë™íƒœ â†’ ìˆ˜ë™íƒœ ë³€í™˜
            (r'(\w+)\s+(\w+ed|s)\s+(\w+)', r'\3 was \2ed by \1')
        ],
        "advanced": [
            # ë‹¨ìˆœ ì¡°ê±´ë¬¸ â†’ ê°€ì •ë²• ë³€í™˜
            (r'If\s+(\w+)\s+(\w+),\s+(\w+)\s+(\w+)', r'Were \1 to \2, \3 would \4'),
            # ë¶„ì‚¬êµ¬ë¬¸ ë„ì…
            (r'(\w+)\s+(\w+ed|s)\s+and\s+(\w+)', r'\1 \2ed, \3ing')
        ]
    }
    
    # ë¬¸ë²• êµ¬ì¡° ë³€í™˜ ì ìš©
    for pattern, replacement in complexity_patterns.get(level, []):
        sentence = re.sub(pattern, replacement, sentence)
    
    return sentence

def maintain_topic_coherence(sentences):
    # ì£¼ì œì–´ ì¶”ì¶œ
    topic_words = extract_topic_words(sentences)
    
    # ì£¼ì œ ì¼ê´€ì„± ê°•í™”
    coherent_sentences = []
    for sentence in sentences:
        # ë¬¸ì¥ì´ ì£¼ì œì™€ ì—°ê´€ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ê³  ë³´ê°•
        if not has_topic_reference(sentence, topic_words):
            # ì£¼ì œ ì—°ê²°ì–´ ì¶”ê°€
            sentence = add_topic_reference(sentence, topic_words)
        coherent_sentences.append(sentence)
    
    # ì „ì²´ íë¦„ ê°œì„ 
    add_transition_phrases(coherent_sentences)
    
    return coherent_sentences

def genre_specific_rewrite(text, genre):
    """
    íŠ¹ì • ì¥ë¥´(ì—ì„¸ì´, ë¹„ì¦ˆë‹ˆìŠ¤ ì´ë©”ì¼, í•™ìˆ  ë…¼ë¬¸ ë“±)ì— ë§ëŠ” ìŠ¤íƒ€ì¼ë¡œ ì¬ì‘ì„±
    """
    genre_styles = {
        "academic": {
            "phrases": ["It can be argued that", "The evidence suggests that", 
                       "This study examines", "The findings indicate"],
            "tone": "formal",
            "sentence_length": "long"
        },
        "business": {
            "phrases": ["I am writing to inquire about", "I would like to request", 
                      "Please find attached", "I look forward to hearing from you"],
            "tone": "professional",
            "sentence_length": "medium"
        },
        "creative": {
            "phrases": ["Imagine a world where", "The air was thick with", 
                      "In that moment", "Everything changed when"],
            "tone": "descriptive", 
            "sentence_length": "varied"
        }
    }
    
    style = genre_styles.get(genre, genre_styles["academic"])
    return apply_genre_style(text, style)

def optimize_complexity(text, target_level):
    # í˜„ì¬ í…ìŠ¤íŠ¸ ë³µì¡ë„ ë¶„ì„
    current_complexity = analyze_text_complexity(text)
    
    # íƒ€ê²Ÿ ë ˆë²¨ê³¼ í˜„ì¬ ë³µì¡ë„ ë¹„êµ
    if current_complexity < target_level:
        # ì–´íœ˜ ê³ ê¸‰í™”
        text = enhance_vocabulary(text)
        # ë¬¸ì¥ êµ¬ì¡° ë³µì¡í™”
        text = add_complexity(text)
    elif current_complexity > target_level:
        # ê°„ê²°í™”
        text = simplify_text(text)
    
    return text

def culturally_appropriate_rewrite(text, target_culture="american"):
    # ë¬¸í™”ì  íŠ¹ì„±ì— ë§ëŠ” í‘œí˜„ ë°ì´í„°ì…‹
    cultural_expressions = {
        "american": {
            "idioms": ["hit the nail on the head", "ballpark figure"],
            "references": ["Super Bowl", "Thanksgiving"],
            "measurements": "imperial"  # feet, pounds
        },
        "british": {
            "idioms": ["bob's your uncle", "chin up"],
            "references": ["BBC", "Bank Holiday"],
            "measurements": "metric"  # meters, kilograms
        }
    }
    
    culture_data = cultural_expressions.get(target_culture, cultural_expressions["american"])
    return adapt_to_culture(text, culture_data)

# í•™ìˆ  ë‹¨ì–´ ëª©ë¡
@st.cache_resource
def get_academic_word_list():
    # í•™ìˆ  ë‹¨ì–´ ëª©ë¡ (ì˜ˆì‹œ)
    return {'analyze', 'concept', 'data', 'environment', 'establish', 'evident', 
            'factor', 'interpret', 'method', 'principle', 'process', 'research', 
            'significant', 'theory', 'variable'}

# ë‹¨ì–´ ë¹ˆë„ ë°ì´í„° ë¡œë“œ
@st.cache_resource
def get_word_frequency_data():
    # ì˜ì–´ ë‹¨ì–´ ë¹ˆë„ ë°ì´í„° (ì˜ˆì‹œ)
    common_words = {'the': 0.05, 'be': 0.04, 'to': 0.03, 'of': 0.025, 'and': 0.02}
    return common_words

# ê¸°ë³¸ ë‹¨ì–´ ì…‹ ì •ì˜
@st.cache_resource
def default_vocabulary_sets():
    basic_words = {'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'i', 'it'}
    intermediate_words = {'achieve', 'consider', 'determine', 'establish', 'indicate'}
    advanced_words = {'arbitrary', 'cognitive', 'encompass', 'facilitate', 'implicit'}
    return {'basic': basic_words, 'intermediate': intermediate_words, 'advanced': advanced_words}

# ì£¼ì œë³„ ì–´íœ˜ ë°ì´í„°ì…‹ ë¡œë“œ
@st.cache_resource
def load_domain_vocabulary(subject_area="general"):
    domain_vocabs = {
        "general": ["discuss", "explain", "describe", "analyze"],
        "business": ["market", "strategy", "investment", "revenue"],
        "science": ["hypothesis", "experiment", "theory", "analysis"],
        "technology": ["innovation", "interface", "algorithm", "platform"]
    }
    return domain_vocabs.get(subject_area, domain_vocabs["general"])

# ì£¼ì œì–´ ì¶”ì¶œ
def extract_topic_words(sentences, top_n=3):
    all_words = []
    for sentence in sentences:
        words = custom_word_tokenize(sentence.lower())
        words = [w for w in words if re.match(r'\w+', w) and w not in stopwords.words('english')]
        all_words.extend(words)
    
    word_counts = Counter(all_words)
    return [word for word, _ in word_counts.most_common(top_n)]

# ë¬¸ì¥ì´ ì£¼ì œì™€ ì—°ê´€ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
def has_topic_reference(sentence, topic_words):
    words = custom_word_tokenize(sentence.lower())
    return any(topic in words for topic in topic_words)

# ì£¼ì œ ì—°ê²°ì–´ ì¶”ê°€
def add_topic_reference(sentence, topic_words):
    if not topic_words:
        return sentence
    
    topic = random.choice(topic_words)
    reference_phrases = [
        f"Regarding {topic}, ",
        f"In terms of {topic}, ",
        f"Concerning {topic}, "
    ]
    
    return random.choice(reference_phrases) + sentence

# ì „í™˜ ë¬¸êµ¬ ì¶”ê°€
def add_transition_phrases(sentences):
    if len(sentences) <= 1:
        return sentences
    
    transitions = [
        "Furthermore, ", "Moreover, ", "In addition, ",
        "Consequently, ", "Therefore, ", "Thus, ",
        "On the other hand, ", "However, ", "Nevertheless, "
    ]
    
    for i in range(1, len(sentences)):
        if random.random() < 0.4:  # 40% í™•ë¥ ë¡œ ì „í™˜ ë¬¸êµ¬ ì¶”ê°€
            sentences[i] = random.choice(transitions) + sentences[i]
    
    return sentences

# ë¬¸ì¥ ë³µì¡ë„ ë¶„ì„
def analyze_text_complexity(text):
    sentences = custom_sent_tokenize(text)
    words = custom_word_tokenize(text)
    
    # í‰ê·  ë¬¸ì¥ ê¸¸ì´
    avg_sentence_length = len(words) / max(len(sentences), 1)
    
    # ê¸´ ë‹¨ì–´(6ì ì´ìƒ) ë¹„ìœ¨
    long_words = [w for w in words if len(w) >= 6]
    long_word_ratio = len(long_words) / max(len(words), 1)
    
    # ë³µí•© ë¬¸ì¥ ë¹„ìœ¨ (and, but, because ë“± í¬í•¨)
    complex_markers = ['and', 'but', 'because', 'however', 'therefore', 'although', 'since']
    complex_sentences = sum(1 for s in sentences if any(marker in custom_word_tokenize(s.lower()) for marker in complex_markers))
    complex_ratio = complex_sentences / max(len(sentences), 1)
    
    # ë³µì¡ë„ ì ìˆ˜ (0~1)
    complexity = (avg_sentence_length / 25 + long_word_ratio + complex_ratio) / 3
    return min(max(complexity, 0), 1)  # 0ê³¼ 1 ì‚¬ì´ë¡œ ì œí•œ

# ì–´íœ˜ ê³ ê¸‰í™”
def enhance_vocabulary(text):
    words = custom_word_tokenize(text)
    enhanced = []
    
    # ê¸°ë³¸ ë‹¨ì–´ì— ëŒ€í•œ ê³ ê¸‰ ëŒ€ì²´ì–´
    enhancements = {
        'good': 'excellent',
        'bad': 'detrimental',
        'big': 'substantial',
        'small': 'minimal',
        'happy': 'euphoric',
        'sad': 'depressed',
        'important': 'crucial',
        'difficult': 'formidable',
        'easy': 'facile',
        'beautiful': 'resplendent'
    }
    
    for word in words:
        word_lower = word.lower()
        if word_lower in enhancements and random.random() < 0.7:
            replacement = enhancements[word_lower]
            if word[0].isupper():
                replacement = replacement.capitalize()
            enhanced.append(replacement)
        else:
            enhanced.append(word)
    
    return ' '.join(enhanced)

# ë¬¸ì¥ êµ¬ì¡° ë³µì¡í™”
def add_complexity(text):
    sentences = custom_sent_tokenize(text)
    complex_sentences = []
    
    for sentence in sentences:
        # ë¬¸ì¥ ê¸¸ì´ì— ë”°ë¼ ë‹¤ë¥¸ ì „ëµ ì ìš©
        if len(custom_word_tokenize(sentence)) < 10:
            # ì§§ì€ ë¬¸ì¥ì€ ìˆ˜ì‹ì–´êµ¬ ì¶”ê°€
            modifiers = [
                "Interestingly, ", "Specifically, ", "Notably, ",
                "In this context, ", "From this perspective, "
            ]
            sentence = random.choice(modifiers) + sentence
        else:
            # ê¸´ ë¬¸ì¥ì€ êµ¬ì¡° ë³€ê²½
            if sentence.startswith("I "):
                sentence = sentence.replace("I ", "The author ")
            elif random.random() < 0.3:
                # 30% í™•ë¥ ë¡œ ìˆ˜ë™íƒœë¡œ ë³€ê²½ ì‹œë„
                if " is " in sentence:
                    parts = sentence.split(" is ")
                    if len(parts) >= 2:
                        sentence = parts[1] + " is " + parts[0]
        
        complex_sentences.append(sentence)
    
    return ' '.join(complex_sentences)

# ë¬¸ì¥ ê°„ì†Œí™”
def simplify_text(text):
    sentences = custom_sent_tokenize(text)
    simplified_sentences = []
    
    for sentence in sentences:
        words = custom_word_tokenize(sentence)
        if len(words) > 20:  # ê¸´ ë¬¸ì¥ ë¶„í• 
            middle = len(words) // 2
            first_half = ' '.join(words[:middle])
            second_half = ' '.join(words[middle:])
            simplified_sentences.append(first_half + '.')
            simplified_sentences.append(second_half)
        else:
            simplified_sentences.append(sentence)
    
    return ' '.join(simplified_sentences)

# ì¥ë¥´ë³„ ìŠ¤íƒ€ì¼ ì ìš©
def apply_genre_style(text, style):
    sentences = custom_sent_tokenize(text)
    styled_sentences = []
    
    # ì²« ë¬¸ì¥ì— ìŠ¤íƒ€ì¼ êµ¬ë¬¸ ì¶”ê°€
    if sentences and random.random() < 0.7:
        phrases = style.get("phrases", [])
        if phrases:
            sentences[0] = random.choice(phrases) + " " + sentences[0].lower()
    
    # ë‚˜ë¨¸ì§€ ë¬¸ì¥ì— ëŒ€í•œ ìŠ¤íƒ€ì¼ ì ìš©
    for i, sentence in enumerate(sentences):
        if i == 0:
            styled_sentences.append(sentence)
            continue
        
        # ìŠ¤íƒ€ì¼ì— ë”°ë¥¸ ë¬¸ì¥ ê¸¸ì´ ì¡°ì •
        if style.get("sentence_length") == "long" and len(custom_word_tokenize(sentence)) < 10:
            # ì§§ì€ ë¬¸ì¥ì„ ë” ê¸¸ê²Œ
            modifiers = ["furthermore", "additionally", "consequently", "in this context"]
            sentence = random.choice(modifiers) + ", " + sentence.lower()
        elif style.get("sentence_length") == "short" and len(custom_word_tokenize(sentence)) > 15:
            # ê¸´ ë¬¸ì¥ì„ ë¶„í• 
            words = custom_word_tokenize(sentence)
            middle = len(words) // 2
            first_half = ' '.join(words[:middle])
            second_half = ' '.join(words[middle:])
            styled_sentences.append(first_half + '.')
            sentence = second_half
        
        styled_sentences.append(sentence)
    
    return ' '.join(styled_sentences)

# ë¬¸í™”ì  ë§¥ë½ì— ë§ê²Œ ì ì‘
def adapt_to_culture(text, culture_data):
    sentences = custom_sent_tokenize(text)
    culturally_adapted = []
    
    for sentence in sentences:
        # 10% í™•ë¥ ë¡œ ë¬¸í™”ì  ê´€ìš©êµ¬ ì¶”ê°€
        if random.random() < 0.1 and culture_data.get("idioms"):
            idiom = random.choice(culture_data.get("idioms"))
            sentence = sentence + " " + idiom + "."
        
        # ì¸¡ì • ë‹¨ìœ„ ë³€í™˜
        if culture_data.get("measurements") == "imperial":
            sentence = re.sub(r'(\d+)\s*km', lambda m: f"{float(m.group(1)) * 0.621:.1f} miles", sentence)
            sentence = re.sub(r'(\d+)\s*kg', lambda m: f"{float(m.group(1)) * 2.205:.1f} pounds", sentence)
        elif culture_data.get("measurements") == "metric":
            sentence = re.sub(r'(\d+)\s*miles', lambda m: f"{float(m.group(1)) * 1.609:.1f} km", sentence)
            sentence = re.sub(r'(\d+)\s*pounds', lambda m: f"{float(m.group(1)) * 0.454:.1f} kg", sentence)
        
        culturally_adapted.append(sentence)
    
    return ' '.join(culturally_adapted)

# ë¬¸ì¥ ë‹¨ìœ„ ë„ë©”ì¸ ê°•í™” 
def enhance_sentence_with_domain(sentence, domain_vocabulary, prev_context="", next_context=""):
    words = custom_word_tokenize(sentence)
    domain_enhanced = []
    
    for word in words:
        word_lower = word.lower()
        # ì¼ë°˜ì ì¸ ë‹¨ì–´ë¥¼ ë„ë©”ì¸ ê´€ë ¨ ë‹¨ì–´ë¡œ êµì²´ (20% í™•ë¥ )
        if word_lower in ['good', 'great', 'important', 'interesting'] and random.random() < 0.2:
            domain_word = random.choice(domain_vocabulary)
            domain_enhanced.append(domain_word)
        else:
            domain_enhanced.append(word)
    
    enhanced_sentence = ' '.join(domain_enhanced)
    
    # ë¬¸ë§¥ì„ ê³ ë ¤í•œ ì—°ê²° êµ¬ë¬¸ ì¶”ê°€
    if prev_context and not any(sentence.lower().startswith(x) for x in ['however', 'moreover', 'furthermore']):
        connectors = ['Furthermore', 'Moreover', 'In addition', 'Subsequently']
        enhanced_sentence = f"{random.choice(connectors)}, {enhanced_sentence.lower()}"
    
    return enhanced_sentence

# ê³ ê¸‰ ì¬ì‘ì„±ì„ ìœ„í•œ ë™ì˜ì–´ ë° ë¬¸êµ¬ ì‚¬ì „
@st.cache_resource
def get_advanced_synonyms():
    return {
        # ê¸°ì¡´ ë‹¨ì–´
        'good': ['exemplary', 'exceptional', 'impeccable', 'superb', 'outstanding', 'excellent'],
        'bad': ['detrimental', 'deplorable', 'egregious', 'substandard', 'inadequate', 'unfavorable'],
        'big': ['immense', 'formidable', 'monumental', 'substantial', 'enormous', 'extensive'],
        'small': ['minuscule', 'negligible', 'infinitesimal', 'minute', 'diminutive', 'marginal'],
        'happy': ['euphoric', 'exuberant', 'ecstatic', 'elated', 'jubilant', 'overjoyed'],
        'sad': ['despondent', 'crestfallen', 'dejected', 'melancholic', 'disheartened', 'disconsolate'],
        'important': ['imperative', 'indispensable', 'paramount', 'pivotal', 'crucial', 'vital'],
        'difficult': ['formidable', 'insurmountable', 'herculean', 'arduous', 'demanding', 'laborious'],
        'easy': ['effortless', 'rudimentary', 'facile', 'straightforward', 'uncomplicated', 'simple'],
        'beautiful': ['resplendent', 'breathtaking', 'sublime', 'exquisite', 'magnificent', 'stunning'],
        
        # ì¶”ê°€ ë‹¨ì–´
        'interesting': ['fascinating', 'compelling', 'intriguing', 'captivating', 'engaging', 'absorbing'],
        'boring': ['tedious', 'monotonous', 'mundane', 'insipid', 'dull', 'unengaging'],
        'smart': ['astute', 'perspicacious', 'erudite', 'brilliant', 'sagacious', 'ingenious'],
        'stupid': ['obtuse', 'imperceptive', 'injudicious', 'imprudent', 'misguided', 'inept'],
        'nice': ['amiable', 'congenial', 'benevolent', 'affable', 'cordial', 'genial'],
        'angry': ['indignant', 'irate', 'incensed', 'furious', 'exasperated', 'vexed'],
        'scared': ['apprehensive', 'trepidatious', 'daunted', 'alarmed', 'disquieted', 'perturbed'],
        'funny': ['humorous', 'comical', 'amusing', 'hilarious', 'entertaining', 'witty'],
        'useless': ['ineffectual', 'fruitless', 'unproductive', 'futile', 'unavailing', 'inefficacious'],
        'useful': ['advantageous', 'beneficial', 'serviceable', 'practical', 'valuable', 'constructive'],
        
        # ë™ì‚¬
        'say': ['articulate', 'proclaim', 'assert', 'express', 'pronounce', 'convey'],
        'look': ['observe', 'scrutinize', 'examine', 'survey', 'inspect', 'perceive'],
        'make': ['construct', 'fabricate', 'produce', 'generate', 'establish', 'formulate'],
        'get': ['acquire', 'obtain', 'procure', 'attain', 'secure', 'gain'],
        'take': ['appropriate', 'seize', 'acquire', 'grasp', 'adopt', 'employ'],
        'give': ['bestow', 'confer', 'impart', 'provide', 'allocate', 'distribute'],
        'find': ['discover', 'locate', 'detect', 'uncover', 'identify', 'ascertain'],
        'tell': ['relate', 'divulge', 'disclose', 'reveal', 'narrate', 'recount'],
        'ask': ['inquire', 'interrogate', 'query', 'question', 'solicit', 'request'],
        'feel': ['experience', 'perceive', 'sense', 'discern', 'intuit', 'apprehend'],
        
        # ìì£¼ ì“°ëŠ” ë¶€ì‚¬
        'very': ['exceedingly', 'immensely', 'tremendously', 'extraordinarily', 'remarkably', 'profoundly'],
        'really': ['genuinely', 'authentically', 'truly', 'undeniably', 'veritably', 'legitimately'],
        'just': ['precisely', 'specifically', 'exactly', 'particularly', 'solely', 'exclusively'],
        'so': ['consequently', 'therefore', 'thus', 'accordingly', 'hence', 'subsequently'],
        'quite': ['considerably', 'substantially', 'significantly', 'markedly', 'notably', 'decidedly'],
        'too': ['excessively', 'overly', 'unduly', 'inordinately', 'immoderately', 'disproportionately']
    }

@st.cache_resource
def get_advanced_phrases():
    return {
        # ê¸°ë³¸ í‘œí˜„
        r'\bi think\b': ['I postulate that', 'I am of the conviction that', 'It is my considered opinion that', 
                         'I hold the perspective that', 'My assessment suggests that', 'I would posit that'],
        r'\bi like\b': ['I am particularly enamored with', 'I hold in high regard', 'I find great merit in', 
                       'I am especially fond of', 'I have a distinct preference for', 'I am drawn to'],
        r'\bi want\b': ['I aspire to', 'I am inclined towards', 'My inclination is toward', 
                       'I seek to', 'It is my desire to', 'I am motivated to'],
        r'\blots of\b': ['a plethora of', 'an abundance of', 'a multitude of', 
                        'a substantial quantity of', 'a considerable amount of', 'numerous instances of'],
        r'\bmany of\b': ['a preponderance of', 'a substantial proportion of', 'a significant contingent of', 
                        'a notable fraction of', 'a considerable number of', 'a substantial segment of'],
                        
        # ì¶”ê°€ ê¸°ë³¸ í‘œí˜„
        r'\bI need\b': ['I require', 'I necessitate', 'It is imperative for me to', 
                       'I find it essential to', 'I am in need of', 'I deem it necessary to'],
        r'\bI believe\b': ['I am convinced that', 'I maintain that', 'I subscribe to the notion that', 
                         'I adhere to the principle that', 'My conviction is that', 'I firmly hold that'],
        r'\bI know\b': ['I am cognizant of', 'I am well-versed in', 'I am thoroughly familiar with', 
                       'I possess comprehensive knowledge of', 'I am well-acquainted with', 'I am fully aware of'],
        r'\bI agree\b': ['I concur with', 'I am in accordance with', 'I align myself with', 
                        'I share the sentiment that', 'I endorse the view that', 'I am of one mind with'],
        r'\bI disagree\b': ['I take issue with', 'I contest the notion that', 'I diverge from the view that', 
                          'I am at variance with', 'I oppose the perspective that', 'I cannot reconcile myself with'],
        
        # ì‹œì‘ ë¬¸êµ¬ ê°œì„ 
        r'^In conclusion': ['To synthesize the aforementioned points', 'Drawing all aspects into consideration', 
                           'Upon final analysis', 'As a culmination of these insights', 'In summation', 'To consolidate the arguments presented'],
        r'^Moreover': ['Furthermore, it warrants emphasis that', 'In addition to the foregoing', 
                      'Beyond what has been established', 'As an extension of this reasoning', 'To augment these considerations', 'Building upon this foundation'],
        r'^However': ['Nonetheless, it must be acknowledged that', 'Conversely, one must consider that', 
                     'Despite this assertion', 'In contrast to this perspective', 'Yet, it remains evident that', 'Notwithstanding these factors'],
        r'^First': ['As an initial consideration', 'Foremost among these factors', 
                   'The primary aspect to consider is', 'To begin this analysis', 'The foundational element is', 'At the forefront of this discussion'],
        r'^For example': ['To illustrate this concept', 'As exemplified by', 
                         'A pertinent instance of this phenomenon is', 'Consider, for instance', 'A demonstrative case is', 'By way of illustration'],
        
        # í•œêµ­ í•™ìŠµì íŠ¹í™” í‘œí˜„ ê°œì„ 
        r'\bI will do\b': ['I shall undertake', 'I intend to proceed with', 'I commit myself to', 
                          'I will diligently execute', 'I will dedicate myself to accomplishing', 'I will systematically address'],
        r'\bvery much\b': ['substantially', 'considerably', 'significantly', 
                          'to a marked degree', 'to a notable extent', 'remarkably'],
        r'\bI studied\b': ['I engaged in scholarly pursuit of', 'I devoted time to examining', 
                          'I conducted an academic exploration of', 'I undertook a systematic study of', 'I applied myself to learning', 'I dedicated myself to mastering'],
        r'\bIt is important\b': ['It is imperative', 'It is of paramount significance', 
                                'It holds critical importance', 'It is fundamentally essential', 'It remains a crucial consideration', 'It constitutes a vital element']
    }

@st.cache_resource
def get_domain_specific_phrases(domain='academic'):
    """
    íŠ¹ì • ì˜ì—­(í•™ìˆ , ë¹„ì¦ˆë‹ˆìŠ¤, ì¼ìƒ ë“±)ì— ë§ëŠ” ì–´íœ˜ ë° ë¬¸êµ¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
    """
    phrases = {
        'academic': {
            r'\bshow\b': ['demonstrate', 'indicate', 'establish', 'reveal', 'manifest', 'elucidate'],
            r'\btopic\b': ['subject matter', 'field of inquiry', 'area of study', 'focus of research', 'domain of investigation'],
            r'\bresearch\b': ['scholarly investigation', 'academic inquiry', 'empirical study', 'systematic exploration', 'scientific examination'],
            r'\bdata\b': ['empirical evidence', 'quantitative measurements', 'statistical information', 'research findings', 'collected observations'],
            r'\bmethod\b': ['methodological approach', 'analytical framework', 'procedural technique', 'investigative protocol', 'systematic procedure'],
            r'^The paper discusses': ['This scholarly work examines', 'The present study analyzes', 'This research investigates', 'This academic treatise addresses', 'The current investigation explores'],
            r'^This study aims to': ['The objective of this research is to', 'This investigation seeks to', 'The purpose of this scholarly inquiry is to', 'This academic examination endeavors to', 'This analysis attempts to'],
            r'^Results show': ['The empirical findings indicate', 'The data demonstrates', 'The research outcomes reveal', 'The analytical results suggest', 'The experimental evidence confirms']
        },
        'business': {
            r'\bmoney\b': ['financial resources', 'capital', 'monetary assets', 'funds', 'financial means'],
            r'\bcostumer\b': ['client', 'patron', 'consumer', 'end-user', 'purchaser'],
            r'\bboss\b': ['executive director', 'chief executive officer', 'senior manager', 'supervisor', 'administrative head'],
            r'\bjob\b': ['professional role', 'career position', 'occupational function', 'employment capacity', 'professional responsibility'],
            r'\bmeeting\b': ['strategic gathering', 'corporate assembly', 'professional conference', 'business consultation', 'executive session'],
            r'^I want to apply': ['I wish to submit my candidacy', 'I am interested in pursuing a position', 'I would like to express my interest in', 'I am seeking to secure a role', 'I aspire to join your organization as'],
            r'^Please find attached': ['I have enclosed for your consideration', 'Attached herewith is', 'I submit for your review', 'Kindly find accompanying this correspondence', 'For your perusal, I have included'],
            r'^We need to discuss': ['It would be beneficial to address', 'We should consider deliberating on', 'I propose we convene to examine', 'It is imperative we confer regarding', 'A consultation is warranted concerning']
        },
        'conversation': {
            r'\bnice to meet you\b': ['pleased to make your acquaintance', 'delighted to encounter you', 'it is a pleasure to be introduced', 'charmed to meet you', 'gratified by our meeting'],
            r'\bthanks\b': ['I appreciate your assistance', 'I am grateful for', 'my sincere gratitude for', 'I extend my thanks for', 'I would like to express my appreciation for'],
            r'\bsorry\b': ['I apologize for', 'I regret that', 'please accept my apologies for', 'I must express my regret regarding', 'I am remorseful about'],
            r'\bbye\b': ['farewell', 'until we meet again', 'I bid you adieu', 'I look forward to our next encounter', 'I must now take my leave'],
            r'\bfriend\b': ['companion', 'confidant', 'associate', 'acquaintance', 'ally'],
            r'^How are you': ['I trust you are faring well', 'How do you find yourself today', 'I hope this day finds you in good spirits', 'May I inquire after your wellbeing', 'I trust all is well with you'],
            r'^I hope you': ['It is my sincere wish that you', 'I earnestly anticipate that you', 'My expectations are that you', 'I look forward to you', 'My desire is that you'],
            r'^I'm writing to': ['I am corresponding to', 'The purpose of my communication is to', 'I am reaching out to', 'This message serves to', 'I am taking the liberty of contacting you to']
        }
    }
    
    return phrases.get(domain, phrases['academic'])

def enhance_patterns_from_corpus():
    patterns = get_advanced_phrases()  # ê¸°ë³¸ íŒ¨í„´
    
    # í•™ìˆ  ë¬¸í—Œì—ì„œ ì¶”ì¶œí•œ ì¶”ê°€ íŒ¨í„´
    academic_patterns = {
        r'\bshow that\b': ['demonstrate that', 'indicate that', 'reveal that', 'establish that', 'evidence that'],
        r'\bthe result is\b': ['the outcome suggests', 'findings indicate', 'this yields', 'the data confirms', 'empirical evidence shows'],
        r'\bin recent years\b': ['in contemporary scholarship', 'in the current academic landscape', 'in modern research', 'in present-day studies', 'in the evolving literature'],
        r'\bthe purpose of this study\b': ['the objective of this investigation', 'this research aims to', 'the goal of this analysis', 'this inquiry seeks to', 'the focus of this examination'],
        r'\bprevious research\b': ['extant literature', 'prior investigations', 'established scholarship', 'existing studies', 'antecedent research'],
        r'\bneed more research\b': ['warrant further investigation', 'necessitate additional scholarly inquiry', 'require more comprehensive examination', 'demand deeper academic exploration', 'call for extended analysis'],
        r'\bthis paper discusses\b': ['this study examines', 'this investigation addresses', 'this research analyzes', 'this work explores', 'this scholarly effort evaluates'],
        r'\bit is clear that\b': ['evidence unequivocally suggests that', 'it is demonstrably evident that', 'findings conclusively indicate that', 'it is empirically verifiable that', 'data substantiates the conclusion that']
    }
    
    patterns.update(academic_patterns)
    return patterns

# ì˜ì‘ë¬¸ ì¬ì‘ì„± ê¸°ëŠ¥ ìˆ˜ì • (ë‹¤ì–‘í•œ ìˆ˜ì¤€ìœ¼ë¡œ ë³€í™˜)
def rewrite_text(text, level='similar'):
    """
    í•™ìƒì´ ì‘ì„±í•œ ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ ìˆ˜ì¤€ìœ¼ë¡œ ì¬ì‘ì„±í•©ë‹ˆë‹¤.
    
    Parameters:
    - text: ì›ë³¸ í…ìŠ¤íŠ¸
    - level: 'similar' (ë¹„ìŠ·í•œ ìˆ˜ì¤€), 'improved' (ì•½ê°„ ë” ë†’ì€ ìˆ˜ì¤€), 'advanced' (ê³ ê¸‰ ìˆ˜ì¤€)
    
    Returns:
    - ì¬ì‘ì„±ëœ í…ìŠ¤íŠ¸
    """
    if not text.strip():
        return ""
    
    try:
        # ê³ ê¸‰ AI ëª¨ë¸ ì‚¬ìš© ê°€ëŠ¥í•œ ê²½ìš°
        if level == 'advanced' and has_transformers:
            try:
                return advanced_rewrite_text(text, level)
            except Exception as e:
                st.warning(f"ê³ ê¸‰ ëª¨ë¸ ì‚¬ìš© ì¤‘ ì˜¤ë¥˜: {e}. ëŒ€ì²´ ë°©ë²•ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤.")
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ì„ ë° ì¬ì‘ì„±
        sentences = custom_sent_tokenize(text)
        rewritten_sentences = []
        
        for sentence in sentences:
            # ì›ë³¸ ë¬¸ì¥ êµ¬ì¡´ ë³´ì¡´
            if level == 'similar':
                rewritten = rewrite_similar_level(sentence)
            # ì•½ê°„ ë” ë†’ì€ ìˆ˜ì¤€ìœ¼ë¡œ ê°œì„ 
            elif level == 'improved':
                rewritten = rewrite_improved_level(sentence)
            # ê³ ê¸‰ ìˆ˜ì¤€ìœ¼ë¡œ ë³€í™˜
            elif level == 'advanced':
                rewritten = rewrite_advanced_level(sentence)
            else:
                rewritten = sentence
            
            rewritten_sentences.append(rewritten)
        
        # ë¬¸ì¥ êµ¬ì¡° ë³€í™˜ íŒ¨í„´ ì¶”ê°€
        structure_transformations = {
            r'^I believe': ['In my opinion', 'From my perspective', 'I am of the view that'],
            r'^There is': ['There exists', 'We can observe', 'It is evident that there is'],
            r'^It is important': ['It is crucial', 'It is essential', 'A key consideration is'],
        }
        
        # ì¬ì‘ì„± í•¨ìˆ˜ì— ì ìš©
        for i, sentence in enumerate(rewritten_sentences):
            for pattern, replacements in structure_transformations.items():
                if re.search(pattern, sentence):
                    replacement = random.choice(replacements)
                    rewritten_sentences[i] = re.sub(pattern, replacement, sentence)
        
        # ì£¼ì œ ì¼ê´€ì„± ê°•í™” ë° íë¦„ ê°œì„  (advanced ëª¨ë“œì—ì„œë§Œ)
        if level == 'advanced':
            rewritten_sentences = maintain_topic_coherence(rewritten_sentences)
        
        return ' '.join(rewritten_sentences)
    except Exception as e:
        st.error(f"í…ìŠ¤íŠ¸ ì¬ì‘ì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return text

# ê³ ê¸‰ ìˆ˜ì¤€ìœ¼ë¡œ ì¬ì‘ì„±
def rewrite_advanced_level(sentence):
    # ê³ ê¸‰ ì–´íœ˜ë¡œ ëŒ€ì²´
    advanced_synonyms = get_advanced_synonyms()
    advanced_phrases = get_advanced_phrases()
    
    # ë¬¸ì¥ êµ¬ì¡° ê°œì„  íŒ¨í„´
    structure_improvements = {
        r'^I am ': ['Being ', 'As someone who is '],
        r'^I have ': ['Having ', 'Possessing '],
        r'^This is ': ['This constitutes ', 'This represents '],
        r'^There are ': ['There exist ', 'One can observe '],
        r'^It is ': ['It remains ', 'It stands as '],
    }
    
    # ìš°ì„  ë‹¨ì–´ ìˆ˜ì¤€ ê°œì„ 
    words = custom_word_tokenize(sentence)
    result = []
    
    for word in words:
        word_lower = word.lower()
        if word_lower in advanced_synonyms and random.random() < 0.8:  # 80% í™•ë¥ ë¡œ ëŒ€ì²´
            synonyms = advanced_synonyms[word_lower]
            replacement = random.choice(synonyms)
            
            # ëŒ€ë¬¸ì ë³´ì¡´
            if word[0].isupper():
                replacement = replacement.capitalize()
            
            result.append(replacement)
        else:
            result.append(word)
    
    advanced_text = ' '.join(result)
    
    # ë¬¸êµ¬ íŒ¨í„´ ê°œì„ 
    for pattern, replacements in advanced_phrases.items():
        if re.search(pattern, advanced_text, re.IGNORECASE):
            replacement = random.choice(replacements)
            advanced_text = re.sub(pattern, replacement, advanced_text, flags=re.IGNORECASE)
    
    # ë¬¸ì¥ êµ¬ì¡° ê°œì„ 
    for pattern, replacements in structure_improvements.items():
        if re.search(pattern, advanced_text):
            if random.random() < 0.7:  # 70% í™•ë¥ ë¡œ êµ¬ì¡° ë³€ê²½
                replacement = random.choice(replacements)
                advanced_text = re.sub(pattern, replacement, advanced_text)
    
    return advanced_text

# ë¹„ìŠ·í•œ ìˆ˜ì¤€ìœ¼ë¡œ ì¬ì‘ì„±
def rewrite_similar_level(sentence):
    # ê°„ë‹¨í•œ ë™ì˜ì–´ ëŒ€ì²´ë§Œ ìˆ˜í–‰
    basic_synonyms = {
        'good': ['nice', 'fine', 'decent'],
        'bad': ['poor', 'negative', 'unpleasant'],
        'big': ['large', 'sizable', 'substantial'],
        'small': ['little', 'tiny', 'minor'],
        'happy': ['glad', 'pleased', 'content'],
        'sad': ['unhappy', 'upset', 'down']
    }
    
    words = custom_word_tokenize(sentence)
    result = []
    
    for word in words:
        word_lower = word.lower()
        if word_lower in basic_synonyms and random.random() < 0.3:  # 30% í™•ë¥ ë¡œ ëŒ€ì²´
            synonyms = basic_synonyms[word_lower]
            replacement = random.choice(synonyms)
            
            # ëŒ€ë¬¸ì ë³´ì¡´
            if word[0].isupper():
                replacement = replacement.capitalize()
            
            result.append(replacement)
        else:
            result.append(word)
    
    return ' '.join(result)

# ì•½ê°„ ë” ë†’ì€ ìˆ˜ì¤€ìœ¼ë¡œ ì¬ì‘ì„±
def rewrite_improved_level(sentence):
    # ì¤‘ê¸‰ ì–´íœ˜ë¡œ ëŒ€ì²´
    intermediate_synonyms = {
        'good': ['excellent', 'outstanding', 'superb'],
        'bad': ['inferior', 'substandard', 'inadequate'],
        'big': ['enormous', 'extensive', 'considerable'],
        'small': ['diminutive', 'slight', 'limited'],
        'happy': ['delighted', 'thrilled', 'overjoyed'],
        'sad': ['depressed', 'miserable', 'gloomy']
    }
    
    # ê¸°ë³¸ ë¬¸êµ¬ ê°œì„ 
    improved_phrases = {
        r'\bI think\b': ['I believe', 'In my opinion', 'I consider'],
        r'\bI like\b': ['I enjoy', 'I appreciate', 'I am fond of'],
        r'\bI want\b': ['I desire', 'I wish', 'I would like'],
        r'\blots of\b': ['numerous', 'many', 'plenty of'],
        r'\bvery\b': ['extremely', 'particularly', 'significantly']
    }
    
    # ìš°ì„  ë‹¨ì–´ ìˆ˜ì¤€ ê°œì„ 
    words = custom_word_tokenize(sentence)
    result = []
    
    for word in words:
        word_lower = word.lower()
        if word_lower in intermediate_synonyms and random.random() < 0.5:  # 50% í™•ë¥ ë¡œ ëŒ€ì²´
            synonyms = intermediate_synonyms[word_lower]
            replacement = random.choice(synonyms)
            
            # ëŒ€ë¬¸ì ë³´ì¡´
            if word[0].isupper():
                replacement = replacement.capitalize()
            
            result.append(replacement)
        else:
            result.append(word)
    
    improved_text = ' '.join(result)
    
    # ë¬¸êµ¬ íŒ¨í„´ ê°œì„ 
    for pattern, replacements in improved_phrases.items():
        if re.search(pattern, improved_text, re.IGNORECASE):
            replacement = random.choice(replacements)
            improved_text = re.sub(pattern, replacement, improved_text, flags=re.IGNORECASE)
    
    return improved_text

@st.cache_resource(ttl=86400)
def load_expanded_suggestions():
    try:
        # ì™¸ë¶€ ì†ŒìŠ¤ì—ì„œ ë°ì´í„° ë¡œë“œ ë˜ëŠ” ë¡œì»¬ íŒŒì¼ ì‚¬ìš©
        import json
        with open('data/expanded_suggestions.json', 'r') as f:
            return json.load(f)
    except:
        return get_custom_suggestions()  # í´ë°± ì˜µì…˜

@st.cache_resource
def get_expanded_vocabulary_levels():
    # A1~C2 ë ˆë²¨ë³„ ë‹¨ì–´ ë°ì´í„° ë¡œë“œ
    vocab_url = "https://raw.githubusercontent.com/openlanguagedata/cefr-english/main/cefr_wordlist.csv"
    try:
        import pandas as pd
        df = pd.read_csv(vocab_url)
        return {
            'basic': set(df[df['level'].isin(['A1', 'A2'])]['word']),
            'intermediate': set(df[df['level'].isin(['B1', 'B2'])]['word']),
            'advanced': set(df[df['level'].isin(['C1', 'C2'])]['word'])
        }
    except:
        return default_vocabulary_sets()

def get_enriched_synonyms(word, level):
    try:
        from nltk.corpus import wordnet
        synonyms = []
        for syn in wordnet.synsets(word):
            for lemma in syn.lemmas():
                synonyms.append(lemma.name())
        
        # ë‚œì´ë„ì— ë”°ë¼ ì ì ˆí•œ ë™ì˜ì–´ í•„í„°ë§
        return synonyms if synonyms else get_fallback_synonyms(word, level)
    except:
        return get_fallback_synonyms(word, level)

def get_fallback_synonyms(word, level):
    if level == 'advanced':
        synonyms_dict = get_advanced_synonyms()
    elif level == 'improved':
        synonyms_dict = {
            'good': ['excellent', 'outstanding', 'superb'],
            'bad': ['inferior', 'substandard', 'inadequate'],
            'big': ['enormous', 'extensive', 'considerable'],
            'small': ['diminutive', 'slight', 'limited'],
            'happy': ['delighted', 'thrilled', 'overjoyed'],
            'sad': ['depressed', 'miserable', 'gloomy']
        }
    else:  # similar/basic level
        synonyms_dict = {
            'good': ['nice', 'fine', 'decent'],
            'bad': ['poor', 'negative', 'unpleasant'],
            'big': ['large', 'sizable', 'substantial'],
            'small': ['little', 'tiny', 'minor'],
            'happy': ['glad', 'pleased', 'content'],
            'sad': ['unhappy', 'upset', 'down']
        }
    
    return synonyms_dict.get(word.lower(), [word])  # ê¸°ë³¸ê°’ìœ¼ë¡œ ì›ë˜ ë‹¨ì–´ ë°˜í™˜

def comprehensive_vocabulary_analysis(text):
    # ê¸°ë³¸ í‰ê°€
    basic_eval = evaluate_vocabulary_level(text)
    
    # ì¶”ê°€ ì¸¡ì •: Type-Token Ratio, Lexical Density, Academic Word Usage
    words = custom_word_tokenize(text.lower())
    if not words:
        return basic_eval
        
    # ì–´íœ˜ ë°€ë„(ë‚´ìš©ì–´ ë¹„ìœ¨)
    content_words = [w for w in words if w not in stopwords.words('english')]
    lexical_density = len(content_words) / len(words)
    
    # í•™ìˆ ì–´ ë¹„ìœ¨
    academic_words = get_academic_word_list()
    academic_ratio = len([w for w in words if w in academic_words]) / len(words)
    
    return {**basic_eval, 'lexical_density': lexical_density, 'academic_ratio': academic_ratio}
