import streamlit as st

# í˜ì´ì§€ ì„¤ì • (ê°€ì¥ ë¨¼ì € í˜¸ì¶œí•´ì•¼ í•¨)
st.set_page_config(
    page_title="ì˜ì‘ë¬¸ ìë™ ì²¨ì‚­ ì‹œìŠ¤í…œ",
    page_icon="ğŸ“",
    layout="wide"
)

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
from collections import Counter
import re
import io
from datetime import datetime
from textblob import TextBlob

# NLTK ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë°ì´í„° ì²˜ë¦¬
import nltk
from nltk.corpus import stopwords

# NLTK í•„ìš” ë°ì´í„° ë‹¤ìš´ë¡œë“œ (Streamlit Cloudì—ì„œë„ ì‘ë™í•˜ë„ë¡ ssl ê²€ì¦ ë¬´ì‹œ)
try:
    import ssl
    try:
        _create_unverified_https_context = ssl._create_unverified_context
    except AttributeError:
        pass
    else:
        ssl._create_default_https_context = _create_unverified_https_context
    
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
except Exception as e:
    st.warning(f"NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

# ë§ì¶¤ë²• ê²€ì‚¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì¡°ê±´ë¶€ë¡œ ì„í¬íŠ¸
try:
    import enchant
    has_enchant = True
except ImportError:
    has_enchant = False
    st.info("ë§ì¶¤ë²• ê²€ì‚¬ ë¼ì´ë¸ŒëŸ¬ë¦¬(enchant)ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. TextBlobì„ ì‚¬ìš©í•œ ê¸°ë³¸ ë§ì¶¤ë²• ê²€ì‚¬ë§Œ ì œê³µë©ë‹ˆë‹¤.")

# ìˆ˜ì •ëœ sent_tokenize í•¨ìˆ˜ (NLTK ì˜ì¡´ì„± ì œê±°)
def custom_sent_tokenize(text):
    if not text:
        return []
    
    # ì •ê·œì‹ ê¸°ë°˜ ë¬¸ì¥ ë¶„í• ê¸°
    # ì˜¨ì , ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ë’¤ì— ê³µë°±ì´ ì˜¤ëŠ” íŒ¨í„´ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
    sentences = re.split(r'(?<=[.!?])\s+', text)
    # ë¹ˆ ë¬¸ì¥ ì œê±°
    return [s.strip() for s in sentences if s.strip()]

# ìˆ˜ì •ëœ word_tokenize í•¨ìˆ˜ (NLTK ì˜ì¡´ì„± ì œê±°)
def custom_word_tokenize(text):
    if not text:
        return []
    
    # íš¨ê³¼ì ì¸ ì •ê·œì‹ íŒ¨í„´ìœ¼ë¡œ ë‹¨ì–´ í† í°í™”
    # ì¶•ì•½í˜•(I'm, don't ë“±), ì†Œìœ ê²©(John's), í•˜ì´í”ˆ ë‹¨ì–´(well-known) ë“±ì„ ì²˜ë¦¬
    tokens = []
    # ê¸°ë³¸ ë‹¨ì–´(ì•ŒíŒŒë²³ + ìˆ«ì + ì•„í¬ìŠ¤íŠ¸ë¡œí”¼ + í•˜ì´í”ˆ)
    words = re.findall(r'\b[\w\'-]+\b', text)
    # êµ¬ë‘ì 
    punctuation = re.findall(r'[.,!?;:"]', text)
    
    tokens.extend(words)
    tokens.extend(punctuation)
    
    return tokens

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'history' not in st.session_state:
    st.session_state.history = []

# ë§ì¶¤ë²• ì‚¬ì „ ì´ˆê¸°í™”
@st.cache_resource
def get_spell_checker():
    if has_enchant:
        try:
            return enchant.Dict("en_US")
        except:
            return None
    return None

# ë¬¸ë²• ê²€ì‚¬ í•¨ìˆ˜ (TextBlob ì‚¬ìš©)
def check_grammar(text):
    if not text.strip():
        return []
    
    errors = []
    
    try:
        blob = TextBlob(text)
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ì„
        sentences = custom_sent_tokenize(text)
        
        # ë§ì¶¤ë²• ê²€ì‚¬
        checker = get_spell_checker()
        words = custom_word_tokenize(text)
        
        # ë¬¸ì¥ì˜ ì‹œì‘ ìœ„ì¹˜ë¥¼ ì¶”ì í•˜ê¸° ìœ„í•œ ë³€ìˆ˜
        offset = 0
        
        for sentence in sentences:
            # ê°„ë‹¨í•œ ë¬¸ë²• ê²€ì‚¬ (TextBlobì˜ sentiment ì‚¬ìš©)
            try:
                sentence_blob = TextBlob(sentence)
                
                # ë¬¸ì¥ì˜ ë‹¨ì–´ ë¶„ì„
                for word in custom_word_tokenize(sentence):
                    if word.isalpha():  # ì•ŒíŒŒë²³ ë‹¨ì–´ë§Œ í™•ì¸
                        # ë§ì¶¤ë²• í™•ì¸ (enchantê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ)
                        if checker and not checker.check(word):
                            suggestions = checker.suggest(word)[:3]  # ìµœëŒ€ 3ê°œ ì œì•ˆ
                            word_offset = text.find(word, offset)
                            
                            if word_offset != -1:
                                errors.append({
                                    "offset": word_offset,
                                    "errorLength": len(word),
                                    "message": f"ë§ì¶¤ë²• ì˜¤ë¥˜: '{word}'",
                                    "replacements": suggestions
                                })
                        # TextBlobì„ ì‚¬ìš©í•œ ë§ì¶¤ë²• ê²€ì‚¬ ëŒ€ì•ˆ (enchantê°€ ì—†ëŠ” ê²½ìš°)
                        elif not checker and has_enchant == False:
                            try:
                                word_blob = TextBlob(word)
                                corrected = str(word_blob.correct())
                                if corrected != word and len(word) > 3:  # ì§§ì€ ë‹¨ì–´ëŠ” ë¬´ì‹œ
                                    word_offset = text.find(word, offset)
                                    if word_offset != -1:
                                        errors.append({
                                            "offset": word_offset,
                                            "errorLength": len(word),
                                            "message": f"ë§ì¶¤ë²• ì˜¤ë¥˜: '{word}'",
                                            "replacements": [corrected]
                                        })
                            except Exception as e:
                                # TextBlob ë§ì¶¤ë²• ê²€ì‚¬ ì˜¤ë¥˜ ë¬´ì‹œ
                                pass
            except Exception as e:
                # ë¬¸ì¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒì‹œ í•´ë‹¹ ë¬¸ì¥ ê±´ë„ˆë›°ê¸°
                pass
                
            # ë‹¤ìŒ ë¬¸ì¥ì˜ ê²€ìƒ‰ì„ ìœ„í•´ ì˜¤í”„ì…‹ ì—…ë°ì´íŠ¸
            offset += len(sentence)
    except Exception as e:
        st.error(f"í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    
    return errors

# í…ìŠ¤íŠ¸ í†µê³„ ë¶„ì„ í•¨ìˆ˜
def analyze_text(text):
    if not text.strip():
        return {
            'word_count': 0,
            'sentence_count': 0,
            'avg_word_length': 0,
            'avg_sentence_length': 0,
            'vocabulary_size': 0
        }
    
    sentences = custom_sent_tokenize(text)
    words = custom_word_tokenize(text)
    words = [word.lower() for word in words if re.match(r'\w+', word)]
    
    word_count = len(words)
    sentence_count = len(sentences)
    avg_word_length = sum(len(word) for word in words) / max(1, word_count)
    avg_sentence_length = word_count / max(1, sentence_count)
    vocabulary_size = len(set(words))
    
    return {
        'word_count': word_count,
        'sentence_count': sentence_count,
        'avg_word_length': round(avg_word_length, 2),
        'avg_sentence_length': round(avg_sentence_length, 2),
        'vocabulary_size': vocabulary_size
    }

# ì–´íœ˜ ë¶„ì„ í•¨ìˆ˜
def analyze_vocabulary(text):
    if not text.strip():
        return {
            'word_freq': {},
            'pos_dist': {}
        }
    
    words = custom_word_tokenize(text.lower())
    words = [word for word in words if re.match(r'\w+', word)]
    
    # ë¶ˆìš©ì–´ ì œê±°
    stop_words = set(stopwords.words('english'))
    filtered_words = [word for word in words if word not in stop_words]
    
    # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
    word_freq = Counter(filtered_words).most_common(20)
    
    return {
        'word_freq': dict(word_freq)
    }

# ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚°
def calculate_lexical_diversity(text):
    words = custom_word_tokenize(text.lower())
    words = [word for word in words if re.match(r'\w+', word)]
    
    if len(words) == 0:
        return 0
    
    return len(set(words)) / len(words)

# ë‹¨ì–´ ë¹ˆë„ ì‹œê°í™”
def plot_word_frequency(word_freq):
    if not word_freq:
        return None
    
    df = pd.DataFrame(list(word_freq.items()), columns=['ë‹¨ì–´', 'ë¹ˆë„'])
    df = df.sort_values('ë¹ˆë„', ascending=True)
    
    fig = px.bar(df.tail(10), x='ë¹ˆë„', y='ë‹¨ì–´', orientation='h', 
                title='ìƒìœ„ 10ê°œ ë‹¨ì–´ ë¹ˆë„')
    fig.update_layout(height=400)
    
    return fig

# ì–´íœ˜ ìˆ˜ì¤€ í‰ê°€ (ê°„ë‹¨í•œ ë²„ì „)
def evaluate_vocabulary_level(text):
    # ì˜ì–´ ë‹¨ì–´ ìˆ˜ì¤€ì„ ë‚˜íƒ€ë‚´ëŠ” ìƒ˜í”Œ ë°ì´í„° (ì‹¤ì œë¡œëŠ” ë” í° ë°ì´í„°ì…‹ í•„ìš”)
    basic_words = {'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'i', 'it', 
                  'for', 'not', 'on', 'with', 'he', 'as', 'you', 'do', 'at', 'this', 'but'}
    intermediate_words = {'achieve', 'consider', 'determine', 'establish', 'indicate', 'occur',
                        'participate', 'predict', 'provide', 'recognize', 'resolve', 'specific', 
                        'therefore', 'utilize', 'aspect', 'concept', 'context', 'diverse'}
    advanced_words = {'arbitrary', 'cognitive', 'encompass', 'facilitate', 'fundamental', 'implicit',
                     'intricate', 'legitimate', 'paradigm', 'phenomenon', 'pragmatic', 'scrutinize',
                     'sophisticated', 'subsequent', 'synthesis', 'theoretical', 'underlying'}
    
    words = custom_word_tokenize(text.lower())
    words = [word for word in words if re.match(r'\w+', word)]
    
    word_set = set(words)
    
    basic_count = len(word_set.intersection(basic_words))
    intermediate_count = len(word_set.intersection(intermediate_words))
    advanced_count = len(word_set.intersection(advanced_words))
    
    total = basic_count + intermediate_count + advanced_count
    if total == 0:
        return {'basic': 0, 'intermediate': 0, 'advanced': 0}
    
    return {
        'basic': basic_count / max(1, total),
        'intermediate': intermediate_count / max(1, total),
        'advanced': advanced_count / max(1, total)
    }

# ì–´íœ˜ ìˆ˜ì¤€ í‰ê°€ (ê°„ë‹¨í•œ ë²„ì „)
def evaluate_vocabulary_level_simple(text):
    # ì˜ì–´ ë‹¨ì–´ ìˆ˜ì¤€ì„ ë‚˜íƒ€ë‚´ëŠ” ìƒ˜í”Œ ë°ì´í„° (ì‹¤ì œë¡œëŠ” ë” í° ë°ì´í„°ì…‹ í•„ìš”)
    basic_words = {'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'i', 'it', 
                  'for', 'not', 'on', 'with', 'he', 'as', 'you', 'do', 'at', 'this', 'but'}
    intermediate_words = {'achieve', 'consider', 'determine', 'establish', 'indicate', 'occur',
                        'participate', 'predict', 'provide', 'recognize', 'resolve', 'specific', 
                        'therefore', 'utilize', 'aspect', 'concept', 'context', 'diverse'}
    advanced_words = {'arbitrary', 'cognitive', 'encompass', 'facilitate', 'fundamental', 'implicit',
                     'intricate', 'legitimate', 'paradigm', 'phenomenon', 'pragmatic', 'scrutinize',
                     'sophisticated', 'subsequent', 'synthesis', 'theoretical', 'underlying'}
    
    words = custom_word_tokenize(text.lower())
    words = [word for word in words if re.match(r'\w+', word)]
    
    word_set = set(words)
    
    basic_count = len(word_set.intersection(basic_words))
    intermediate_count = len(word_set.intersection(intermediate_words))
    advanced_count = len(word_set.intersection(advanced_words))
    
    total = basic_count + intermediate_count + advanced_count
    if total == 0:
        return {'basic': 0, 'intermediate': 0, 'advanced': 0}
    
    return {
        'basic': basic_count / max(1, total),
        'intermediate': intermediate_count / max(1, total),
        'advanced': advanced_count / max(1, total)
    }

# í•™ìƒ í˜ì´ì§€
def show_student_page():
    st.title("ì˜ì‘ë¬¸ ìë™ ì²¨ì‚­ ì‹œìŠ¤í…œ - í•™ìƒ")
    
    # ë¡œê·¸ì•„ì›ƒ ë²„íŠ¼
    if st.button("ë¡œê·¸ì•„ì›ƒ", key="student_logout"):
        st.session_state.user_type = None
        st.rerun()
    
    tab1, tab2 = st.tabs(["ì˜ì‘ë¬¸ ê²€ì‚¬", "ë‚´ ì‘ë¬¸ ê¸°ë¡"])
    
    with tab1:
        st.subheader("ì˜ì‘ë¬¸ ì…ë ¥")
        user_text = st.text_area("ì•„ë˜ì— ì˜ì–´ ì‘ë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", height=200)
        
        col1, col2 = st.columns(2)
        
        with col1:
            grammar_tab, vocab_tab = st.tabs(["ë¬¸ë²• ê²€ì‚¬", "ì–´íœ˜ ë¶„ì„"])
            
            with grammar_tab:
                if st.button("ë¬¸ë²• ê²€ì‚¬í•˜ê¸°"):
                    if not user_text:
                        st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    else:
                        # ë¬¸ë²• ì˜¤ë¥˜ ê²€ì‚¬
                        grammar_errors = check_grammar(user_text)
                        
                        if grammar_errors:
                            st.subheader("ë¬¸ë²• ì˜¤ë¥˜ ëª©ë¡")
                            
                            error_data = []
                            for error in grammar_errors:
                                error_data.append({
                                    "ì˜¤ë¥˜": user_text[error['offset']:error['offset'] + error['errorLength']],
                                    "ì˜¤ë¥˜ ë‚´ìš©": error['message'],
                                    "ìˆ˜ì • ì œì•ˆ": error['replacements']
                                })
                            
                            st.dataframe(pd.DataFrame(error_data))
                            
                            # ê¸°ë¡ì— ì €ì¥
                            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                            st.session_state.history.append({
                                'timestamp': timestamp,
                                'text': user_text,
                                'error_count': len(grammar_errors)
                            })
                        else:
                            st.success("ë¬¸ë²• ì˜¤ë¥˜ê°€ ì—†ìŠµë‹ˆë‹¤!")
                            
                            # ê¸°ë¡ì— ì €ì¥
                            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                            st.session_state.history.append({
                                'timestamp': timestamp,
                                'text': user_text,
                                'error_count': 0
                            })
            
            with vocab_tab:
                if st.button("ì–´íœ˜ ë¶„ì„í•˜ê¸°"):
                    if not user_text:
                        st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    else:
                        # ì–´íœ˜ ë¶„ì„
                        vocab_analysis = analyze_vocabulary(user_text)
                        
                        # ë‹¨ì–´ ë¹ˆë„ ì‹œê°í™”
                        fig = plot_word_frequency(vocab_analysis['word_freq'])
                        if fig:
                            st.plotly_chart(fig, use_container_width=True)
                        
                        # ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜
                        diversity_score = calculate_lexical_diversity(user_text)
                        st.metric("ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜", f"{diversity_score:.2f}", 
                                 delta="ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•œ ì–´íœ˜ ì‚¬ìš©")
                        
                        # ì–´íœ˜ ìˆ˜ì¤€ í‰ê°€
                        vocab_level = evaluate_vocabulary_level(user_text)
                        level_df = pd.DataFrame({
                            'ìˆ˜ì¤€': ['ê¸°ì´ˆ', 'ì¤‘ê¸‰', 'ê³ ê¸‰'],
                            'ë¹„ìœ¨': [vocab_level['basic'], vocab_level['intermediate'], vocab_level['advanced']]
                        })
                        
                        fig = px.pie(level_df, values='ë¹„ìœ¨', names='ìˆ˜ì¤€', 
                                    title='ì–´íœ˜ ìˆ˜ì¤€ ë¶„í¬',
                                    color_discrete_sequence=px.colors.sequential.Viridis)
                        st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            if st.button("í…ìŠ¤íŠ¸ ë¶„ì„í•˜ê¸°"):
                if not user_text:
                    st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                else:
                    # í…ìŠ¤íŠ¸ í†µê³„ ë¶„ì„
                    stats = analyze_text(user_text)
                    
                    st.subheader("í…ìŠ¤íŠ¸ í†µê³„")
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("ë‹¨ì–´ ìˆ˜", stats['word_count'])
                        st.metric("ë¬¸ì¥ ìˆ˜", stats['sentence_count'])
                    with col2:
                        st.metric("í‰ê·  ë‹¨ì–´ ê¸¸ì´", stats['avg_word_length'])
                        st.metric("í‰ê·  ë¬¸ì¥ ê¸¸ì´ (ë‹¨ì–´)", stats['avg_sentence_length'])
                    
                    st.metric("ì–´íœ˜ í¬ê¸° (ê³ ìœ  ë‹¨ì–´ ìˆ˜)", stats['vocabulary_size'])
                    
                    # ê²Œì´ì§€ ì°¨íŠ¸ë¡œ í‘œí˜„í•˜ê¸°
                    progress_col1, progress_col2 = st.columns(2)
                    with progress_col1:
                        # í‰ê·  ë¬¸ì¥ ê¸¸ì´ ê²Œì´ì§€ (ì ì • ì˜ì–´ ë¬¸ì¥ ê¸¸ì´: 15-20 ë‹¨ì–´)
                        sentence_gauge = min(1.0, stats['avg_sentence_length'] / 20)
                        st.progress(sentence_gauge)
                        st.caption(f"ë¬¸ì¥ ê¸¸ì´ ì ì •ì„±: {int(sentence_gauge * 100)}%")
                    
                    with progress_col2:
                        # ì–´íœ˜ ë‹¤ì–‘ì„± ê²Œì´ì§€
                        vocab_ratio = stats['vocabulary_size'] / max(1, stats['word_count'])
                        st.progress(min(1.0, vocab_ratio * 2))  # 0.5 ì´ìƒì´ë©´ 100%
                        st.caption(f"ì–´íœ˜ ë‹¤ì–‘ì„±: {int(min(1.0, vocab_ratio * 2) * 100)}%")
    
    with tab2:
        st.subheader("ë‚´ ì‘ë¬¸ ê¸°ë¡")
        if not st.session_state.history:
            st.info("ì•„ì§ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            history_df = pd.DataFrame(st.session_state.history)
            st.dataframe(history_df)
            
            # ì˜¤ë¥˜ ìˆ˜ ì¶”ì´ ì°¨íŠ¸
            if len(history_df) > 1:
                fig = px.line(history_df, x='timestamp', y='error_count', 
                            title='ë¬¸ë²• ì˜¤ë¥˜ ìˆ˜ ì¶”ì´',
                            labels={'timestamp': 'ë‚ ì§œ', 'error_count': 'ì˜¤ë¥˜ ìˆ˜'})
                st.plotly_chart(fig, use_container_width=True)

# êµì‚¬ í˜ì´ì§€
def show_teacher_page():
    st.title("ì˜ì‘ë¬¸ ìë™ ì²¨ì‚­ ì‹œìŠ¤í…œ - êµì‚¬")
    
    # ë¡œê·¸ì•„ì›ƒ ë²„íŠ¼
    if st.button("ë¡œê·¸ì•„ì›ƒ", key="teacher_logout"):
        st.session_state.user_type = None
        st.rerun()
    
    tab1, tab2 = st.tabs(["ì˜ì‘ë¬¸ ì²¨ì‚­", "í•™ìŠµ ëŒ€ì‹œë³´ë“œ"])
    
    with tab1:
        st.subheader("ì˜ì‘ë¬¸ ì…ë ¥ ë° ì²¨ì‚­")
        
        user_text = st.text_area("í•™ìƒì˜ ì˜ì–´ ì‘ë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", height=200)
        
        col1, col2 = st.columns(2)
        
        with col1:
            grammar_tab, vocab_tab = st.tabs(["ë¬¸ë²• ê²€ì‚¬", "ì–´íœ˜ ë¶„ì„"])
            
            with grammar_tab:
                if st.button("ë¬¸ë²• ê²€ì‚¬í•˜ê¸°", key="teacher_grammar"):
                    if not user_text:
                        st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    else:
                        # ë¬¸ë²• ì˜¤ë¥˜ ê²€ì‚¬
                        grammar_errors = check_grammar(user_text)
                        
                        if grammar_errors:
                            st.subheader("ë¬¸ë²• ì˜¤ë¥˜ ëª©ë¡")
                            
                            error_data = []
                            for error in grammar_errors:
                                error_data.append({
                                    "ì˜¤ë¥˜": user_text[error['offset']:error['offset'] + error['errorLength']],
                                    "ì˜¤ë¥˜ ë‚´ìš©": error['message'],
                                    "ìˆ˜ì • ì œì•ˆ": error['replacements']
                                })
                            
                            st.dataframe(pd.DataFrame(error_data))
                        else:
                            st.success("ë¬¸ë²• ì˜¤ë¥˜ê°€ ì—†ìŠµë‹ˆë‹¤!")
            
            with vocab_tab:
                if st.button("ì–´íœ˜ ë¶„ì„í•˜ê¸°", key="teacher_vocab"):
                    if not user_text:
                        st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    else:
                        # ì–´íœ˜ ë¶„ì„
                        vocab_analysis = analyze_vocabulary(user_text)
                        
                        # ë‹¨ì–´ ë¹ˆë„ ì‹œê°í™”
                        fig = plot_word_frequency(vocab_analysis['word_freq'])
                        if fig:
                            st.plotly_chart(fig, use_container_width=True)
                        
                        # ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜
                        diversity_score = calculate_lexical_diversity(user_text)
                        st.metric("ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜", f"{diversity_score:.2f}")
                        
                        # ì–´íœ˜ ìˆ˜ì¤€ í‰ê°€
                        vocab_level = evaluate_vocabulary_level(user_text)
                        level_df = pd.DataFrame({
                            'ìˆ˜ì¤€': ['ê¸°ì´ˆ', 'ì¤‘ê¸‰', 'ê³ ê¸‰'],
                            'ë¹„ìœ¨': [vocab_level['basic'], vocab_level['intermediate'], vocab_level['advanced']]
                        })
                        
                        fig = px.pie(level_df, values='ë¹„ìœ¨', names='ìˆ˜ì¤€', 
                                    title='ì–´íœ˜ ìˆ˜ì¤€ ë¶„í¬')
                        st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            if st.button("í…ìŠ¤íŠ¸ ë¶„ì„í•˜ê¸°", key="teacher_analysis"):
                if not user_text:
                    st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                else:
                    # í…ìŠ¤íŠ¸ í†µê³„ ë¶„ì„
                    stats = analyze_text(user_text)
                    
                    st.subheader("í…ìŠ¤íŠ¸ í†µê³„")
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("ë‹¨ì–´ ìˆ˜", stats['word_count'])
                        st.metric("ë¬¸ì¥ ìˆ˜", stats['sentence_count'])
                    with col2:
                        st.metric("í‰ê·  ë‹¨ì–´ ê¸¸ì´", stats['avg_word_length'])
                        st.metric("í‰ê·  ë¬¸ì¥ ê¸¸ì´ (ë‹¨ì–´)", stats['avg_sentence_length'])
                    
                    st.metric("ì–´íœ˜ í¬ê¸° (ê³ ìœ  ë‹¨ì–´ ìˆ˜)", stats['vocabulary_size'])
        
        # êµì‚¬ ì „ìš© ê¸°ëŠ¥
        st.subheader("ì²¨ì‚­ ë° í”¼ë“œë°±")
        
        # ì²¨ì‚­ ë…¸íŠ¸ í…œí”Œë¦¿
        feedback = st.text_area("ì²¨ì‚­ ë…¸íŠ¸", 
                 value="ë‹¤ìŒ ì‚¬í•­ì„ ì¤‘ì ì ìœ¼ë¡œ ê°œì„ í•´ë³´ì„¸ìš”:\n1. \n2. \n3. ", 
                 height=100,
                 key="feedback_template")
        
        # ì ìˆ˜ ì…ë ¥
        score_col1, score_col2, score_col3 = st.columns(3)
        with score_col1:
            grammar_score = st.slider("ë¬¸ë²• ì ìˆ˜", 0, 10, 5)
        with score_col2:
            vocab_score = st.slider("ì–´íœ˜ ì ìˆ˜", 0, 10, 5)
        with score_col3:
            content_score = st.slider("ë‚´ìš© ì ìˆ˜", 0, 10, 5)
        
        total_score = (grammar_score + vocab_score + content_score) / 3
        st.metric("ì¢…í•© ì ìˆ˜", f"{total_score:.1f} / 10")
        
        # ì €ì¥ ì˜µì…˜
        if st.button("ì²¨ì‚­ ê²°ê³¼ ì €ì¥ (Excel)"):
            if not user_text:
                st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                # ë¬¸ë²• ì˜¤ë¥˜ ê²€ì‚¬
                grammar_errors = check_grammar(user_text)
                
                # ì €ì¥í•  ë°ì´í„° ìƒì„±
                error_data = []
                for error in grammar_errors:
                    error_data.append({
                        "ì˜¤ë¥˜": user_text[error['offset']:error['offset'] + error['errorLength']],
                        "ì˜¤ë¥˜ ë‚´ìš©": error['message'],
                        "ìˆ˜ì • ì œì•ˆ": str(error['replacements']),
                        "ìœ„ì¹˜": f"{error['offset']}:{error['offset'] + error['errorLength']}"
                    })
                
                # ì–´íœ˜ ë¶„ì„
                vocab_analysis = analyze_vocabulary(user_text)
                
                # í†µê³„ ë¶„ì„
                stats = analyze_text(user_text)
                
                # ì—¬ëŸ¬ ì‹œíŠ¸ê°€ ìˆëŠ” Excel íŒŒì¼ ìƒì„±
                excel_buffer = io.BytesIO()
                with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                    # ì›ë³¸ í…ìŠ¤íŠ¸ ì‹œíŠ¸
                    pd.DataFrame({"ì›ë³¸ í…ìŠ¤íŠ¸": [user_text]}).to_excel(writer, sheet_name="ì›ë³¸ í…ìŠ¤íŠ¸", index=False)
                    
                    # ì˜¤ë¥˜ ë°ì´í„° ì‹œíŠ¸
                    pd.DataFrame(error_data).to_excel(writer, sheet_name="ë¬¸ë²• ì˜¤ë¥˜", index=False)
                    
                    # í†µê³„ ì‹œíŠ¸
                    stats_df = pd.DataFrame({
                        "í•­ëª©": ["ë‹¨ì–´ ìˆ˜", "ë¬¸ì¥ ìˆ˜", "í‰ê·  ë‹¨ì–´ ê¸¸ì´", "í‰ê·  ë¬¸ì¥ ê¸¸ì´", "ì–´íœ˜ í¬ê¸°"],
                        "ê°’": [stats['word_count'], stats['sentence_count'], stats['avg_word_length'], 
                             stats['avg_sentence_length'], stats['vocabulary_size']]
                    })
                    stats_df.to_excel(writer, sheet_name="í†µê³„", index=False)
                    
                    # í‰ê°€ ì ìˆ˜ ì‹œíŠ¸
                    score_df = pd.DataFrame({
                        "í‰ê°€ í•­ëª©": ["ë¬¸ë²•", "ì–´íœ˜", "ë‚´ìš©", "ì¢…í•© ì ìˆ˜"],
                        "ì ìˆ˜": [grammar_score, vocab_score, content_score, total_score]
                    })
                    score_df.to_excel(writer, sheet_name="í‰ê°€ ì ìˆ˜", index=False)
                    
                    # í”¼ë“œë°± ì‹œíŠ¸
                    pd.DataFrame({"ì²¨ì‚­ í”¼ë“œë°±": [feedback]}).to_excel(writer, sheet_name="í”¼ë“œë°±", index=False)
                
                # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                excel_buffer.seek(0)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                st.download_button(
                    label="Excel ë‹¤ìš´ë¡œë“œ",
                    data=excel_buffer,
                    file_name=f"ì²¨ì‚­ê²°ê³¼_{timestamp}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                )
    
    with tab2:
        st.subheader("í•™ìŠµ ëŒ€ì‹œë³´ë“œ")
        
        # ìƒ˜í”Œ í•™ìƒ ë°ì´í„° (ì‹¤ì œë¡œëŠ” DBì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨)
        sample_data = {
            "ë‚ ì§œ": ["2023-01-01", "2023-01-08", "2023-01-15", "2023-01-22", "2023-01-29"],
            "í•™ìƒ_A": [7, 6, 8, 7, 9],
            "í•™ìƒ_B": [5, 6, 6, 7, 6],
            "í•™ìƒ_C": [8, 8, 7, 9, 9],
            "í•™ìƒ_D": [4, 5, 6, 6, 7]
        }
        
        scores_df = pd.DataFrame(sample_data)
        
        # í•™ìƒë³„ ì ìˆ˜ ì¶”ì´
        fig = px.line(scores_df, x="ë‚ ì§œ", y=["í•™ìƒ_A", "í•™ìƒ_B", "í•™ìƒ_C", "í•™ìƒ_D"],
                     title="í•™ìƒë³„ ì˜ì‘ë¬¸ ì ìˆ˜ ì¶”ì´",
                     labels={"value": "ì ìˆ˜", "variable": "í•™ìƒ"})
        st.plotly_chart(fig, use_container_width=True)
        
        # ìµœê·¼ ì ìˆ˜ ë¶„í¬
        latest_scores = scores_df.iloc[-1, 1:].values
        students = scores_df.columns[1:]
        
        fig = px.bar(x=students, y=latest_scores, 
                    title="ìµœê·¼ ì˜ì‘ë¬¸ ì ìˆ˜ ë¶„í¬",
                    labels={"x": "í•™ìƒ", "y": "ì ìˆ˜"})
        st.plotly_chart(fig, use_container_width=True)
        
        # í‰ê·  ì˜¤ë¥˜ ìœ í˜• (ìƒ˜í”Œ ë°ì´í„°)
        error_types = {
            "ì˜¤ë¥˜ ìœ í˜•": ["ë¬¸ì¥ êµ¬ì¡°", "ì‹œì œ", "ê´€ì‚¬", "ì „ì¹˜ì‚¬", "ëŒ€ëª…ì‚¬", "ì² ì", "êµ¬ë‘ì "],
            "ë¹ˆë„": [45, 30, 25, 20, 15, 10, 5]
        }
        
        error_df = pd.DataFrame(error_types)
        
        fig = px.pie(error_df, values="ë¹ˆë„", names="ì˜¤ë¥˜ ìœ í˜•",
                    title="í‰ê·  ì˜¤ë¥˜ ìœ í˜• ë¶„í¬")
        st.plotly_chart(fig, use_container_width=True)

# ë©”ì¸ í•¨ìˆ˜
def main():
    # ì œëª© ë° ì†Œê°œ
    st.title("ì˜ì‘ë¬¸ ìë™ ì²¨ì‚­ ì‹œìŠ¤í…œ")
    st.markdown("""
    ì´ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ í•™ìƒë“¤ì˜ ì˜ì‘ë¬¸ì„ ìë™ìœ¼ë¡œ ì²¨ì‚­í•˜ê³  í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.
    í•™ìƒì€ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê³  ìë™ ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """)
    
    # ì§ì ‘ í•™ìƒ í˜ì´ì§€ë¡œ ì´ë™
    show_student_page()

if __name__ == "__main__":
    main()
