import streamlit as st

# í˜ì´ì§€ ì„¤ì • (ê°€ì¥ ë¨¼ì € í˜¸ì¶œí•´ì•¼ í•¨)
st.set_page_config(
    page_title="ì˜ì‘ë¬¸ ìë™ ì²¨ì‚­ ì‹œìŠ¤í…œ",
    page_icon="ğŸ“",
    layout="wide"
)

# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import plotly.express as px
from collections import Counter
import re
import io
from datetime import datetime
from textblob import TextBlob

# NLTK ë¼ì´ë¸ŒëŸ¬ë¦¬ ë° ë°ì´í„° ì²˜ë¦¬
import nltk
from nltk.corpus import stopwords

# NLTK í•„ìš” ë°ì´í„° ë‹¤ìš´ë¡œë“œ (Streamlit Cloudì—ì„œë„ ì‘ë™í•˜ë„ë¡ ssl ê²€ì¦ ë¬´ì‹œ)
try:
    import ssl
    try:
        _create_unverified_https_context = ssl._create_unverified_context
    except AttributeError:
        pass
    else:
        ssl._create_default_https_context = _create_unverified_https_context
    
    nltk.download('punkt', quiet=True)
    nltk.download('stopwords', quiet=True)
except Exception as e:
    st.warning(f"NLTK ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")

# ë§ì¶¤ë²• ê²€ì‚¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì¡°ê±´ë¶€ë¡œ ì„í¬íŠ¸
try:
    import enchant
    has_enchant = True
except ImportError:
    has_enchant = False
    st.info("ë§ì¶¤ë²• ê²€ì‚¬ ë¼ì´ë¸ŒëŸ¬ë¦¬(enchant)ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. TextBlobì„ ì‚¬ìš©í•œ ê¸°ë³¸ ë§ì¶¤ë²• ê²€ì‚¬ë§Œ ì œê³µë©ë‹ˆë‹¤.")

# ìˆ˜ì •ëœ sent_tokenize í•¨ìˆ˜ (NLTK ì˜ì¡´ì„± ì œê±°)
def custom_sent_tokenize(text):
    if not text:
        return []
    
    # ì •ê·œì‹ ê¸°ë°˜ ë¬¸ì¥ ë¶„í• ê¸°
    # ì˜¨ì , ëŠë‚Œí‘œ, ë¬¼ìŒí‘œ ë’¤ì— ê³µë°±ì´ ì˜¤ëŠ” íŒ¨í„´ì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„í• 
    sentences = re.split(r'(?<=[.!?])\s+', text)
    # ë¹ˆ ë¬¸ì¥ ì œê±°
    return [s.strip() for s in sentences if s.strip()]

# ìˆ˜ì •ëœ word_tokenize í•¨ìˆ˜ (NLTK ì˜ì¡´ì„± ì œê±°)
def custom_word_tokenize(text):
    if not text:
        return []
    
    # íš¨ê³¼ì ì¸ ì •ê·œì‹ íŒ¨í„´ìœ¼ë¡œ ë‹¨ì–´ í† í°í™”
    # ì¶•ì•½í˜•(I'm, don't ë“±), ì†Œìœ ê²©(John's), í•˜ì´í”ˆ ë‹¨ì–´(well-known) ë“±ì„ ì²˜ë¦¬
    tokens = []
    # ê¸°ë³¸ ë‹¨ì–´(ì•ŒíŒŒë²³ + ìˆ«ì + ì•„í¬ìŠ¤íŠ¸ë¡œí”¼ + í•˜ì´í”ˆ)
    words = re.findall(r'\b[\w\'-]+\b', text)
    # êµ¬ë‘ì 
    punctuation = re.findall(r'[.,!?;:"]', text)
    
    tokens.extend(words)
    tokens.extend(punctuation)
    
    return tokens

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'history' not in st.session_state:
    st.session_state.history = []

# ë§ì¶¤ë²• ì‚¬ì „ ì´ˆê¸°í™”
@st.cache_resource
def get_spell_checker():
    if has_enchant:
        try:
            return enchant.Dict("en_US")
        except:
            return None
    return None

# ìì£¼ í‹€ë¦¬ëŠ” ë‹¨ì–´ì— ëŒ€í•œ ì‚¬ìš©ì ì •ì˜ ì œì•ˆ ì‚¬ì „
@st.cache_resource
def get_custom_suggestions():
    return {
        'tonite': ['tonight'],
        'tomorow': ['tomorrow'],
        'yestarday': ['yesterday'],
        'definately': ['definitely'],
        'recieve': ['receive'],
        'occurence': ['occurrence'],
        'calender': ['calendar'],
        'wierd': ['weird'],
        'alot': ['a lot'],
        'untill': ['until'],
        'thier': ['their'],
        'truely': ['truly'],
        'begining': ['beginning'],
        'beleive': ['believe'],
        'seperate': ['separate'],
        'goverment': ['government'],
        'neccessary': ['necessary'],
        'occasionaly': ['occasionally'],
        'independant': ['independent'],
        'basicly': ['basically'],
        'wich': ['which'],
        'wont': ["won't"],
        'cant': ["can't"],
        'dont': ["don't"],
        'isnt': ["isn't"],
        'didnt': ["didn't"],
        'couldnt': ["couldn't"],
        'shouldnt': ["shouldn't"],
        'wouldnt': ["wouldn't"],
        'doesnt': ["doesn't"],
        'wasnt': ["wasn't"],
        'werent': ["weren't"],
        'havent': ["haven't"],
        'hasnt': ["hasn't"],
        'hadnt': ["hadn't"],
        'arent': ["aren't"]
    }

# ë¬¸ë²• ê²€ì‚¬ í•¨ìˆ˜ (TextBlob ì‚¬ìš©)
def check_grammar(text):
    if not text.strip():
        return []
    
    errors = []
    
    try:
        blob = TextBlob(text)
        
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ì„
        sentences = custom_sent_tokenize(text)
        
        # ë§ì¶¤ë²• ê²€ì‚¬
        checker = get_spell_checker()
        words = custom_word_tokenize(text)
        
        # ì‚¬ìš©ì ì •ì˜ ì œì•ˆ ì‚¬ì „ ë¡œë“œ
        custom_suggestions = get_custom_suggestions()
        
        # ë¬¸ì¥ì˜ ì‹œì‘ ìœ„ì¹˜ë¥¼ ì¶”ì í•˜ê¸° ìœ„í•œ ë³€ìˆ˜
        offset = 0
        
        for sentence in sentences:
            # ê°„ë‹¨í•œ ë¬¸ë²• ê²€ì‚¬ (TextBlobì˜ sentiment ì‚¬ìš©)
            try:
                sentence_blob = TextBlob(sentence)
                
                # ë¬¸ì¥ì˜ ë‹¨ì–´ ë¶„ì„
                for word in custom_word_tokenize(sentence):
                    if word.isalpha():  # ì•ŒíŒŒë²³ ë‹¨ì–´ë§Œ í™•ì¸
                        # ë§ì¶¤ë²• í™•ì¸ (enchantê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ)
                        if checker and not checker.check(word):
                            # ì‚¬ìš©ì ì •ì˜ ì œì•ˆì´ ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸
                            word_lower = word.lower()
                            if word_lower in custom_suggestions:
                                suggestions = custom_suggestions[word_lower]
                            else:
                                # ê¸°ì¡´ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ ì œì•ˆ ì‚¬ìš©
                                suggestions = checker.suggest(word)[:3]  # ìµœëŒ€ 3ê°œ ì œì•ˆ
                            
                            word_offset = text.find(word, offset)
                            
                            if word_offset != -1:
                                errors.append({
                                    "offset": word_offset,
                                    "errorLength": len(word),
                                    "message": f"ë§ì¶¤ë²• ì˜¤ë¥˜: '{word}'",
                                    "replacements": suggestions
                                })
                        # TextBlobì„ ì‚¬ìš©í•œ ë§ì¶¤ë²• ê²€ì‚¬ ëŒ€ì•ˆ (enchantê°€ ì—†ëŠ” ê²½ìš°)
                        elif not checker and has_enchant == False:
                            try:
                                # ì‚¬ìš©ì ì •ì˜ ì œì•ˆì´ ìˆëŠ”ì§€ ë¨¼ì € í™•ì¸
                                word_lower = word.lower()
                                if word_lower in custom_suggestions:
                                    corrected = custom_suggestions[word_lower][0]  # ì²« ë²ˆì§¸ ì œì•ˆ ì‚¬ìš©
                                    word_offset = text.find(word, offset)
                                    if word_offset != -1:
                                        errors.append({
                                            "offset": word_offset,
                                            "errorLength": len(word),
                                            "message": f"ë§ì¶¤ë²• ì˜¤ë¥˜: '{word}'",
                                            "replacements": custom_suggestions[word_lower]
                                        })
                                else:
                                    # TextBlobì„ ì‚¬ìš©í•œ ê¸°ì¡´ ë¡œì§
                                    word_blob = TextBlob(word)
                                    corrected = str(word_blob.correct())
                                    if corrected != word and len(word) > 3:  # ì§§ì€ ë‹¨ì–´ëŠ” ë¬´ì‹œ
                                        word_offset = text.find(word, offset)
                                        if word_offset != -1:
                                            errors.append({
                                                "offset": word_offset,
                                                "errorLength": len(word),
                                                "message": f"ë§ì¶¤ë²• ì˜¤ë¥˜: '{word}'",
                                                "replacements": [corrected]
                                            })
                            except Exception as e:
                                # TextBlob ë§ì¶¤ë²• ê²€ì‚¬ ì˜¤ë¥˜ ë¬´ì‹œ
                                pass
            except Exception as e:
                # ë¬¸ì¥ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒì‹œ í•´ë‹¹ ë¬¸ì¥ ê±´ë„ˆë›°ê¸°
                pass
                
            # ë‹¤ìŒ ë¬¸ì¥ì˜ ê²€ìƒ‰ì„ ìœ„í•´ ì˜¤í”„ì…‹ ì—…ë°ì´íŠ¸
            offset += len(sentence)
    except Exception as e:
        st.error(f"í…ìŠ¤íŠ¸ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
    
    return errors

# í…ìŠ¤íŠ¸ í†µê³„ ë¶„ì„ í•¨ìˆ˜
def analyze_text(text):
    if not text.strip():
        return {
            'word_count': 0,
            'sentence_count': 0,
            'avg_word_length': 0,
            'avg_sentence_length': 0,
            'vocabulary_size': 0
        }
    
    sentences = custom_sent_tokenize(text)
    words = custom_word_tokenize(text)
    words = [word.lower() for word in words if re.match(r'\w+', word)]
    
    word_count = len(words)
    sentence_count = len(sentences)
    avg_word_length = sum(len(word) for word in words) / max(1, word_count)
    avg_sentence_length = word_count / max(1, sentence_count)
    vocabulary_size = len(set(words))
    
    return {
        'word_count': word_count,
        'sentence_count': sentence_count,
        'avg_word_length': round(avg_word_length, 2),
        'avg_sentence_length': round(avg_sentence_length, 2),
        'vocabulary_size': vocabulary_size
    }

# ì–´íœ˜ ë¶„ì„ í•¨ìˆ˜
def analyze_vocabulary(text):
    if not text.strip():
        return {
            'word_freq': {},
            'pos_dist': {}
        }
    
    words = custom_word_tokenize(text.lower())
    words = [word for word in words if re.match(r'\w+', word)]
    
    # ë¶ˆìš©ì–´ ì œê±°
    stop_words = set(stopwords.words('english'))
    filtered_words = [word for word in words if word not in stop_words]
    
    # ë‹¨ì–´ ë¹ˆë„ ê³„ì‚°
    word_freq = Counter(filtered_words).most_common(20)
    
    return {
        'word_freq': dict(word_freq)
    }

# ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜ ê³„ì‚°
def calculate_lexical_diversity(text):
    words = custom_word_tokenize(text.lower())
    words = [word for word in words if re.match(r'\w+', word)]
    
    if len(words) == 0:
        return 0
    
    return len(set(words)) / len(words)

# ë‹¨ì–´ ë¹ˆë„ ì‹œê°í™”
def plot_word_frequency(word_freq):
    if not word_freq:
        return None
    
    df = pd.DataFrame(list(word_freq.items()), columns=['ë‹¨ì–´', 'ë¹ˆë„'])
    df = df.sort_values('ë¹ˆë„', ascending=True)
    
    fig = px.bar(df.tail(10), x='ë¹ˆë„', y='ë‹¨ì–´', orientation='h', 
                title='ìƒìœ„ 10ê°œ ë‹¨ì–´ ë¹ˆë„')
    fig.update_layout(height=400)
    
    return fig

# ì–´íœ˜ ìˆ˜ì¤€ í‰ê°€ (ê°„ë‹¨í•œ ë²„ì „)
def evaluate_vocabulary_level(text):
    # ì˜ì–´ ë‹¨ì–´ ìˆ˜ì¤€ì„ ë‚˜íƒ€ë‚´ëŠ” ìƒ˜í”Œ ë°ì´í„° (ì‹¤ì œë¡œëŠ” ë” í° ë°ì´í„°ì…‹ í•„ìš”)
    basic_words = {'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'i', 'it', 
                  'for', 'not', 'on', 'with', 'he', 'as', 'you', 'do', 'at', 'this', 'but'}
    intermediate_words = {'achieve', 'consider', 'determine', 'establish', 'indicate', 'occur',
                        'participate', 'predict', 'provide', 'recognize', 'resolve', 'specific', 
                        'therefore', 'utilize', 'aspect', 'concept', 'context', 'diverse'}
    advanced_words = {'arbitrary', 'cognitive', 'encompass', 'facilitate', 'fundamental', 'implicit',
                     'intricate', 'legitimate', 'paradigm', 'phenomenon', 'pragmatic', 'scrutinize',
                     'sophisticated', 'subsequent', 'synthesis', 'theoretical', 'underlying'}
    
    words = custom_word_tokenize(text.lower())
    words = [word for word in words if re.match(r'\w+', word)]
    
    word_set = set(words)
    
    basic_count = len(word_set.intersection(basic_words))
    intermediate_count = len(word_set.intersection(intermediate_words))
    advanced_count = len(word_set.intersection(advanced_words))
    
    total = basic_count + intermediate_count + advanced_count
    if total == 0:
        return {'basic': 0, 'intermediate': 0, 'advanced': 0}
    
    return {
        'basic': basic_count / max(1, total),
        'intermediate': intermediate_count / max(1, total),
        'advanced': advanced_count / max(1, total)
    }

# ì–´íœ˜ ìˆ˜ì¤€ í‰ê°€ (ê°„ë‹¨í•œ ë²„ì „)
def evaluate_vocabulary_level_simple(text):
    # ì˜ì–´ ë‹¨ì–´ ìˆ˜ì¤€ì„ ë‚˜íƒ€ë‚´ëŠ” ìƒ˜í”Œ ë°ì´í„° (ì‹¤ì œë¡œëŠ” ë” í° ë°ì´í„°ì…‹ í•„ìš”)
    basic_words = {'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'i', 'it', 
                  'for', 'not', 'on', 'with', 'he', 'as', 'you', 'do', 'at', 'this', 'but'}
    intermediate_words = {'achieve', 'consider', 'determine', 'establish', 'indicate', 'occur',
                        'participate', 'predict', 'provide', 'recognize', 'resolve', 'specific', 
                        'therefore', 'utilize', 'aspect', 'concept', 'context', 'diverse'}
    advanced_words = {'arbitrary', 'cognitive', 'encompass', 'facilitate', 'fundamental', 'implicit',
                     'intricate', 'legitimate', 'paradigm', 'phenomenon', 'pragmatic', 'scrutinize',
                     'sophisticated', 'subsequent', 'synthesis', 'theoretical', 'underlying'}
    
    words = custom_word_tokenize(text.lower())
    words = [word for word in words if re.match(r'\w+', word)]
    
    word_set = set(words)
    
    basic_count = len(word_set.intersection(basic_words))
    intermediate_count = len(word_set.intersection(intermediate_words))
    advanced_count = len(word_set.intersection(advanced_words))
    
    total = basic_count + intermediate_count + advanced_count
    if total == 0:
        return {'basic': 0, 'intermediate': 0, 'advanced': 0}
    
    return {
        'basic': basic_count / max(1, total),
        'intermediate': intermediate_count / max(1, total),
        'advanced': advanced_count / max(1, total)
    }

# ì˜ì‘ë¬¸ ì¬ì‘ì„± ê¸°ëŠ¥ (ë‹¤ì–‘í•œ ìˆ˜ì¤€ìœ¼ë¡œ ë³€í™˜)
def rewrite_text(text, level='similar'):
    """
    í•™ìƒì´ ì‘ì„±í•œ ì˜ì–´ í…ìŠ¤íŠ¸ë¥¼ ì§€ì •ëœ ìˆ˜ì¤€ìœ¼ë¡œ ì¬ì‘ì„±í•©ë‹ˆë‹¤.
    
    Parameters:
    - text: ì›ë³¸ í…ìŠ¤íŠ¸
    - level: 'similar' (ë¹„ìŠ·í•œ ìˆ˜ì¤€), 'improved' (ì•½ê°„ ë” ë†’ì€ ìˆ˜ì¤€), 'advanced' (ê³ ê¸‰ ìˆ˜ì¤€)
    
    Returns:
    - ì¬ì‘ì„±ëœ í…ìŠ¤íŠ¸
    """
    if not text.strip():
        return ""
    
    try:
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„ì„ ë° ì¬ì‘ì„±
        sentences = custom_sent_tokenize(text)
        rewritten_sentences = []
        
        for sentence in sentences:
            # ì›ë³¸ ë¬¸ì¥ êµ¬ì¡´ ë³´ì¡´
            if level == 'similar':
                rewritten = rewrite_similar_level(sentence)
            # ì•½ê°„ ë” ë†’ì€ ìˆ˜ì¤€ìœ¼ë¡œ ê°œì„ 
            elif level == 'improved':
                rewritten = rewrite_improved_level(sentence)
            # ê³ ê¸‰ ìˆ˜ì¤€ìœ¼ë¡œ ë³€í™˜
            elif level == 'advanced':
                rewritten = rewrite_advanced_level(sentence)
            else:
                rewritten = sentence
            
            rewritten_sentences.append(rewritten)
        
        return ' '.join(rewritten_sentences)
    except Exception as e:
        st.error(f"í…ìŠ¤íŠ¸ ì¬ì‘ì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return text

# ë¹„ìŠ·í•œ ìˆ˜ì¤€ìœ¼ë¡œ ì¬ì‘ì„± (ê¸°ë³¸ ë‹¨ì–´ ëŒ€ì²´)
def rewrite_similar_level(sentence):
    common_synonyms = {
        'good': ['nice', 'fine', 'decent'],
        'bad': ['poor', 'unpleasant', 'negative'],
        'big': ['large', 'sizable', 'substantial'],
        'small': ['little', 'tiny', 'modest'],
        'happy': ['glad', 'pleased', 'cheerful'],
        'sad': ['unhappy', 'upset', 'down'],
        'important': ['significant', 'key', 'crucial'],
        'difficult': ['hard', 'tough', 'challenging'],
        'easy': ['simple', 'straightforward', 'effortless'],
        'beautiful': ['pretty', 'lovely', 'attractive'],
        'interesting': ['engaging', 'intriguing', 'appealing'],
        'boring': ['dull', 'tedious', 'monotonous'],
        'quickly': ['rapidly', 'fast', 'swiftly'],
        'slowly': ['gradually', 'steadily', 'unhurriedly'],
    }
    
    words = custom_word_tokenize(sentence)
    result = []
    
    import random
    random.seed(sum(ord(c) for c in sentence))  # ê°™ì€ ë¬¸ì¥ì€ ê°™ì€ ê²°ê³¼ ìƒì„±
    
    for word in words:
        word_lower = word.lower()
        if word_lower in common_synonyms and random.random() < 0.4:  # 40% í™•ë¥ ë¡œ ëŒ€ì²´
            synonyms = common_synonyms[word_lower]
            replacement = random.choice(synonyms)
            
            # ëŒ€ë¬¸ì ë³´ì¡´
            if word[0].isupper():
                replacement = replacement.capitalize()
            
            result.append(replacement)
            else:
            result.append(word)
    
    return ' '.join(result)

# ì•½ê°„ ë” ë†’ì€ ìˆ˜ì¤€ìœ¼ë¡œ ì¬ì‘ì„±
def rewrite_improved_level(sentence):
    # ê¸°ë³¸ ë‹¨ì–´ë¥¼ ì•½ê°„ ë” ê³ ê¸‰ ë‹¨ì–´ë¡œ ëŒ€ì²´
    improved_synonyms = {
        'good': ['favorable', 'commendable', 'satisfactory'],
        'bad': ['unfavorable', 'substandard', 'inadequate'],
        'big': ['considerable', 'extensive', 'significant'],
        'small': ['diminutive', 'compact', 'slight'],
        'happy': ['delighted', 'content', 'joyful'],
        'sad': ['melancholy', 'gloomy', 'disheartened'],
        'important': ['essential', 'vital', 'fundamental'],
        'difficult': ['demanding', 'arduous', 'strenuous'],
        'easy': ['uncomplicated', 'manageable', 'painless'],
        'beautiful': ['exquisite', 'gorgeous', 'stunning'],
        'interesting': ['captivating', 'compelling', 'fascinating'],
        'boring': ['unstimulating', 'bland', 'uninteresting'],
        'quickly': ['promptly', 'expeditiously', 'hastily'],
        'slowly': ['leisurely', 'deliberately', 'methodically'],
        'very': ['extremely', 'notably', 'particularly'],
        'a lot': ['considerably', 'substantially', 'extensively'],
        'think': ['believe', 'consider', 'contemplate'],
        'like': ['appreciate', 'enjoy', 'admire'],
        'use': ['utilize', 'employ', 'apply'],
        'make': ['create', 'produce', 'develop'],
        'get': ['obtain', 'acquire', 'receive'],
        'tell': ['inform', 'explain', 'communicate'],
        'say': ['state', 'mention', 'express'],
    }
    
    # íŠ¹ì • ë¬¸êµ¬ íŒ¨í„´ ê°œì„ 
    improved_phrases = {
        r'\bi think\b': ['In my opinion', 'I believe', 'From my perspective'],
        r'\bi like\b': ['I appreciate', 'I enjoy', 'I am fond of'],
        r'\bi want\b': ['I would like', 'I desire', 'I wish'],
        r'\blots of\b': ['numerous', 'various', 'a variety of'],
        r'\bmany of\b': ['a significant number of', 'a considerable amount of', 'multiple'],
    }
    
    # ìš°ì„  ë‹¨ì–´ ìˆ˜ì¤€ ê°œì„ 
    words = custom_word_tokenize(sentence)
    result = []
    
    import random
    random.seed(sum(ord(c) for c in sentence))  # ê°™ì€ ë¬¸ì¥ì€ ê°™ì€ ê²°ê³¼ ìƒì„±
    
    for word in words:
        word_lower = word.lower()
        if word_lower in improved_synonyms and random.random() < 0.6:  # 60% í™•ë¥ ë¡œ ëŒ€ì²´
            synonyms = improved_synonyms[word_lower]
            replacement = random.choice(synonyms)
            
            # ëŒ€ë¬¸ì ë³´ì¡´
            if word[0].isupper():
                replacement = replacement.capitalize()
            
            result.append(replacement)
        else:
            result.append(word)
    
    improved_text = ' '.join(result)
    
    # ë¬¸êµ¬ íŒ¨í„´ ê°œì„ 
    for pattern, replacements in improved_phrases.items():
        if re.search(pattern, improved_text, re.IGNORECASE):
            replacement = random.choice(replacements)
            improved_text = re.sub(pattern, replacement, improved_text, flags=re.IGNORECASE)
    
    return improved_text

# ê³ ê¸‰ ìˆ˜ì¤€ìœ¼ë¡œ ì¬ì‘ì„±
def rewrite_advanced_level(sentence):
    # ê³ ê¸‰ ì–´íœ˜ë¡œ ëŒ€ì²´
    advanced_synonyms = {
        'good': ['exemplary', 'exceptional', 'impeccable'],
        'bad': ['detrimental', 'deplorable', 'egregious'],
        'big': ['immense', 'formidable', 'monumental'],
        'small': ['minuscule', 'negligible', 'infinitesimal'],
        'happy': ['euphoric', 'exuberant', 'ecstatic'],
        'sad': ['despondent', 'crestfallen', 'dejected'],
        'important': ['imperative', 'indispensable', 'paramount'],
        'difficult': ['formidable', 'insurmountable', 'Herculean'],
        'easy': ['effortless', 'rudimentary', 'facile'],
        'beautiful': ['resplendent', 'breathtaking', 'sublime'],
        'interesting': ['riveting', 'enthralling', 'spellbinding'],
        'boring': ['soporific', 'tedious', 'vapid'],
        'quickly': ['expeditiously', 'precipitously', 'instantaneously'],
        'slowly': ['incrementally', 'imperceptibly', 'languorously'],
        'very': ['exceedingly', 'remarkably', 'profoundly'],
        'a lot': ['copiously', 'abundantly', 'prolifically'],
        'think': ['postulate', 'theorize', 'presume'],
        'like': ['venerate', 'treasure', 'revere'],
        'use': ['leverage', 'harness', 'implement'],
        'make': ['fabricate', 'synthesize', 'construct'],
        'get': ['procure', 'ascertain', 'secure'],
        'tell': ['articulate', 'elucidate', 'explicate'],
        'say': ['proclaim', 'assert', 'pronounce'],
    }
    
    # ê³ ê¸‰ ë¬¸êµ¬ íŒ¨í„´
    advanced_phrases = {
        r'\bi think\b': ['I postulate that', 'I am of the conviction that', 'It is my considered opinion that'],
        r'\bi like\b': ['I am particularly enamored with', 'I hold in high regard', 'I find great merit in'],
        r'\bi want\b': ['I aspire to', 'I am inclined towards', 'My inclination is toward'],
        r'\blots of\b': ['a plethora of', 'an abundance of', 'a multitude of'],
        r'\bmany of\b': ['a preponderance of', 'a substantial proportion of', 'a significant contingent of'],
        r'\bbecause\b': ['owing to the fact that', 'as a consequence of', 'in view of the circumstance that'],
        r'\bso\b': ['consequently', 'thus', 'hence'],
        r'\bbut\b': ['nevertheless', 'notwithstanding', 'conversely'],
        r'\balso\b': ['furthermore', 'moreover', 'additionally'],
    }
    
    # ë¬¸ì¥ êµ¬ì¡° ê°œì„  íŒ¨í„´
    structure_improvements = {
        r'^I am ': ['Being ', 'As someone who is '],
        r'^I have ': ['Having ', 'Possessing '],
        r'^This is ': ['This constitutes ', 'This represents '],
        r'^There are ': ['There exist ', 'One can observe '],
        r'^It is ': ['It remains ', 'It stands as '],
    }
    
    # ìš°ì„  ë‹¨ì–´ ìˆ˜ì¤€ ê°œì„ 
    words = custom_word_tokenize(sentence)
    result = []
    
    import random
    random.seed(sum(ord(c) for c in sentence))  # ê°™ì€ ë¬¸ì¥ì€ ê°™ì€ ê²°ê³¼ ìƒì„±
    
    for word in words:
        word_lower = word.lower()
        if word_lower in advanced_synonyms and random.random() < 0.8:  # 80% í™•ë¥ ë¡œ ëŒ€ì²´
            synonyms = advanced_synonyms[word_lower]
            replacement = random.choice(synonyms)
            
            # ëŒ€ë¬¸ì ë³´ì¡´
            if word[0].isupper():
                replacement = replacement.capitalize()
            
            result.append(replacement)
        else:
            result.append(word)
    
    advanced_text = ' '.join(result)
    
    # ë¬¸êµ¬ íŒ¨í„´ ê°œì„ 
    for pattern, replacements in advanced_phrases.items():
        if re.search(pattern, advanced_text, re.IGNORECASE):
            replacement = random.choice(replacements)
            advanced_text = re.sub(pattern, replacement, advanced_text, flags=re.IGNORECASE)
    
    # ë¬¸ì¥ êµ¬ì¡° ê°œì„ 
    for pattern, replacements in structure_improvements.items():
        if re.search(pattern, advanced_text):
            if random.random() < 0.7:  # 70% í™•ë¥ ë¡œ êµ¬ì¡° ë³€ê²½
                replacement = random.choice(replacements)
                advanced_text = re.sub(pattern, replacement, advanced_text)
    
    return advanced_text

# ì–´íœ˜ ìˆ˜ì¤€ í‰ê°€ (ê°„ë‹¨í•œ ë²„ì „)
def evaluate_vocabulary_level(text):
    # ì˜ì–´ ë‹¨ì–´ ìˆ˜ì¤€ì„ ë‚˜íƒ€ë‚´ëŠ” ìƒ˜í”Œ ë°ì´í„° (ì‹¤ì œë¡œëŠ” ë” í° ë°ì´í„°ì…‹ í•„ìš”)
    basic_words = {'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'i', 'it', 
                  'for', 'not', 'on', 'with', 'he', 'as', 'you', 'do', 'at', 'this', 'but'}
    intermediate_words = {'achieve', 'consider', 'determine', 'establish', 'indicate', 'occur',
                        'participate', 'predict', 'provide', 'recognize', 'resolve', 'specific', 
                        'therefore', 'utilize', 'aspect', 'concept', 'context', 'diverse'}
    advanced_words = {'arbitrary', 'cognitive', 'encompass', 'facilitate', 'fundamental', 'implicit',
                     'intricate', 'legitimate', 'paradigm', 'phenomenon', 'pragmatic', 'scrutinize',
                     'sophisticated', 'subsequent', 'synthesis', 'theoretical', 'underlying'}
    
    words = custom_word_tokenize(text.lower())
    words = [word for word in words if re.match(r'\w+', word)]
    
    word_set = set(words)
    
    basic_count = len(word_set.intersection(basic_words))
    intermediate_count = len(word_set.intersection(intermediate_words))
    advanced_count = len(word_set.intersection(advanced_words))
    
    total = basic_count + intermediate_count + advanced_count
    if total == 0:
        return {'basic': 0, 'intermediate': 0, 'advanced': 0}
    
    return {
        'basic': basic_count / max(1, total),
        'intermediate': intermediate_count / max(1, total),
        'advanced': advanced_count / max(1, total)
    }

# ì–´íœ˜ ìˆ˜ì¤€ í‰ê°€ (ê°„ë‹¨í•œ ë²„ì „)
def evaluate_vocabulary_level_simple(text):
    # ì˜ì–´ ë‹¨ì–´ ìˆ˜ì¤€ì„ ë‚˜íƒ€ë‚´ëŠ” ìƒ˜í”Œ ë°ì´í„° (ì‹¤ì œë¡œëŠ” ë” í° ë°ì´í„°ì…‹ í•„ìš”)
    basic_words = {'the', 'be', 'to', 'of', 'and', 'a', 'in', 'that', 'have', 'i', 'it', 
                  'for', 'not', 'on', 'with', 'he', 'as', 'you', 'do', 'at', 'this', 'but'}
    intermediate_words = {'achieve', 'consider', 'determine', 'establish', 'indicate', 'occur',
                        'participate', 'predict', 'provide', 'recognize', 'resolve', 'specific', 
                        'therefore', 'utilize', 'aspect', 'concept', 'context', 'diverse'}
    advanced_words = {'arbitrary', 'cognitive', 'encompass', 'facilitate', 'fundamental', 'implicit',
                     'intricate', 'legitimate', 'paradigm', 'phenomenon', 'pragmatic', 'scrutinize',
                     'sophisticated', 'subsequent', 'synthesis', 'theoretical', 'underlying'}
    
    words = custom_word_tokenize(text.lower())
    words = [word for word in words if re.match(r'\w+', word)]
    
    word_set = set(words)
    
    basic_count = len(word_set.intersection(basic_words))
    intermediate_count = len(word_set.intersection(intermediate_words))
    advanced_count = len(word_set.intersection(advanced_words))
    
    total = basic_count + intermediate_count + advanced_count
    if total == 0:
        return {'basic': 0, 'intermediate': 0, 'advanced': 0}
    
    return {
        'basic': basic_count / max(1, total),
        'intermediate': intermediate_count / max(1, total),
        'advanced': advanced_count / max(1, total)
    }

# í•™ìƒ í˜ì´ì§€
def show_student_page():
    st.title("ì˜ì‘ë¬¸ ìë™ ì²¨ì‚­ ì‹œìŠ¤í…œ - í•™ìƒ")
    
    # ë¡œê·¸ì•„ì›ƒ ë²„íŠ¼
    if st.button("ë¡œê·¸ì•„ì›ƒ", key="student_logout"):
        st.session_state.user_type = None
        st.rerun()
    
    # íƒ­ ìƒì„±
    tabs = st.tabs(["ì˜ì‘ë¬¸ ê²€ì‚¬", "ì˜ì‘ë¬¸ ì¬ì‘ì„±", "ë‚´ ì‘ë¬¸ ê¸°ë¡"])
    
    # ì˜ì‘ë¬¸ ê²€ì‚¬ íƒ­
    with tabs[0]:
        st.subheader("ì˜ì‘ë¬¸ ì…ë ¥")
        user_text = st.text_area("ì•„ë˜ì— ì˜ì–´ ì‘ë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", height=200, key="text_tab1")
        
        col1, col2 = st.columns([3, 1])
        
        # ëª¨ë“  ë¶„ì„ì„ í•œ ë²ˆì— ì‹¤í–‰í•˜ëŠ” ë²„íŠ¼
        with col1:
            if st.button("ì „ì²´ ë¶„ì„í•˜ê¸°", use_container_width=True, key="analyze_button"):
                    if not user_text:
                        st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    else:
                    # í…ìŠ¤íŠ¸ í†µê³„ ë¶„ì„
                    stats = analyze_text(user_text)
                    
                        # ë¬¸ë²• ì˜¤ë¥˜ ê²€ì‚¬
                        grammar_errors = check_grammar(user_text)
                        
                    # ì–´íœ˜ ë¶„ì„
                    vocab_analysis = analyze_vocabulary(user_text)
                    
                    # ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜
                    diversity_score = calculate_lexical_diversity(user_text)
                    
                    # ì–´íœ˜ ìˆ˜ì¤€ í‰ê°€
                    vocab_level = evaluate_vocabulary_level(user_text)
                    
                    # ì„¸ì…˜ ìƒíƒœì— ê²°ê³¼ ì €ì¥
                    if 'analysis_results' not in st.session_state:
                        st.session_state.analysis_results = {}
                    
                    st.session_state.analysis_results = {
                        'stats': stats,
                        'grammar_errors': grammar_errors,
                        'vocab_analysis': vocab_analysis,
                        'diversity_score': diversity_score,
                        'vocab_level': vocab_level,
                        'original_text': user_text  # ì›ë³¸ í…ìŠ¤íŠ¸ë„ ì €ì¥
                    }
                            
                            # ê¸°ë¡ì— ì €ì¥
                            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                            st.session_state.history.append({
                                'timestamp': timestamp,
                                'text': user_text,
                        'error_count': len(grammar_errors) if grammar_errors else 0
                    })
                    
                    st.success("ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì•„ë˜ íƒ­ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                    
                    # ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŒì„ í‘œì‹œí•˜ëŠ” í”Œë˜ê·¸
                    st.session_state.analysis_completed = True
                    st.rerun()  # ì¬ì‹¤í–‰í•˜ì—¬ ë²„íŠ¼ í‘œì‹œ ì—…ë°ì´íŠ¸
        
        # ì¬ì‘ì„± ì¶”ì²œ ë²„íŠ¼ ì¶”ê°€
        with col2:
            # ë¶„ì„ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë²„íŠ¼ í‘œì‹œ
            if 'analysis_results' in st.session_state and 'original_text' in st.session_state.analysis_results:
                if st.button("âœ¨ ì˜ì‘ë¬¸ ì¬ì‘ì„± ì¶”ì²œ âœ¨", 
                          key="rewrite_recommendation",
                          use_container_width=True,
                          help="ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì˜ì‘ë¬¸ì„ ë” ì¢‹ì€ í‘œí˜„ìœ¼ë¡œ ì¬ì‘ì„±í•´ë³´ì„¸ìš”!",
                          type="primary"):
                    # í…ìŠ¤íŠ¸ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                    st.session_state.copy_to_rewrite = st.session_state.analysis_results['original_text']
                    
                    # ì‚¬ìš©ìì—ê²Œ ì•ˆë‚´ ë©”ì‹œì§€ í‘œì‹œ
                    st.success("í…ìŠ¤íŠ¸ê°€ ë³µì‚¬ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒë‹¨ì˜ 'ì˜ì‘ë¬¸ ì¬ì‘ì„±' íƒ­ì„ í´ë¦­í•˜ì„¸ìš”!")
                    st.balloons()  # ì‹œê°ì  íš¨ê³¼ ì¶”ê°€
        
        # ê²°ê³¼ í‘œì‹œë¥¼ ìœ„í•œ íƒ­
        result_tab1, result_tab2, result_tab3 = st.tabs(["ë¬¸ë²• ê²€ì‚¬", "ì–´íœ˜ ë¶„ì„", "í…ìŠ¤íŠ¸ í†µê³„"])
        
        with result_tab1:
            if 'analysis_results' in st.session_state and 'grammar_errors' in st.session_state.analysis_results:
                grammar_errors = st.session_state.analysis_results['grammar_errors']
                
                if grammar_errors:
                    st.subheader("ë¬¸ë²• ì˜¤ë¥˜ ëª©ë¡")
                    
                    error_data = []
                    for error in grammar_errors:
                        error_data.append({
                            "ì˜¤ë¥˜": user_text[error['offset']:error['offset'] + error['errorLength']],
                            "ì˜¤ë¥˜ ë‚´ìš©": error['message'],
                            "ìˆ˜ì • ì œì•ˆ": error['replacements']
                        })
                    
                    st.dataframe(pd.DataFrame(error_data))
                    else:
                    st.success("ë¬¸ë²• ì˜¤ë¥˜ê°€ ì—†ìŠµë‹ˆë‹¤!")
        
        with result_tab2:
            if 'analysis_results' in st.session_state and 'vocab_analysis' in st.session_state.analysis_results:
                vocab_analysis = st.session_state.analysis_results['vocab_analysis']
                diversity_score = st.session_state.analysis_results['diversity_score']
                vocab_level = st.session_state.analysis_results['vocab_level']
                        
                        # ë‹¨ì–´ ë¹ˆë„ ì‹œê°í™”
                        fig = plot_word_frequency(vocab_analysis['word_freq'])
                        if fig:
                            st.plotly_chart(fig, use_container_width=True)
                        
                        # ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜
                        st.metric("ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜", f"{diversity_score:.2f}", 
                                 delta="ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘í•œ ì–´íœ˜ ì‚¬ìš©")
                        
                        # ì–´íœ˜ ìˆ˜ì¤€ í‰ê°€
                        level_df = pd.DataFrame({
                            'ìˆ˜ì¤€': ['ê¸°ì´ˆ', 'ì¤‘ê¸‰', 'ê³ ê¸‰'],
                            'ë¹„ìœ¨': [vocab_level['basic'], vocab_level['intermediate'], vocab_level['advanced']]
                        })
                        
                        fig = px.pie(level_df, values='ë¹„ìœ¨', names='ìˆ˜ì¤€', 
                                    title='ì–´íœ˜ ìˆ˜ì¤€ ë¶„í¬',
                                    color_discrete_sequence=px.colors.sequential.Viridis)
                        st.plotly_chart(fig, use_container_width=True)
        
        with result_tab3:
            if 'analysis_results' in st.session_state and 'stats' in st.session_state.analysis_results:
                stats = st.session_state.analysis_results['stats']
                    
                    st.subheader("í…ìŠ¤íŠ¸ í†µê³„")
                    
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("ë‹¨ì–´ ìˆ˜", stats['word_count'])
                        st.metric("ë¬¸ì¥ ìˆ˜", stats['sentence_count'])
                    with col2:
                        st.metric("í‰ê·  ë‹¨ì–´ ê¸¸ì´", stats['avg_word_length'])
                        st.metric("í‰ê·  ë¬¸ì¥ ê¸¸ì´ (ë‹¨ì–´)", stats['avg_sentence_length'])
                    
                    st.metric("ì–´íœ˜ í¬ê¸° (ê³ ìœ  ë‹¨ì–´ ìˆ˜)", stats['vocabulary_size'])
                    
                    # ê²Œì´ì§€ ì°¨íŠ¸ë¡œ í‘œí˜„í•˜ê¸°
                    progress_col1, progress_col2 = st.columns(2)
                    with progress_col1:
                        # í‰ê·  ë¬¸ì¥ ê¸¸ì´ ê²Œì´ì§€ (ì ì • ì˜ì–´ ë¬¸ì¥ ê¸¸ì´: 15-20 ë‹¨ì–´)
                        sentence_gauge = min(1.0, stats['avg_sentence_length'] / 20)
                        st.progress(sentence_gauge)
                        st.caption(f"ë¬¸ì¥ ê¸¸ì´ ì ì •ì„±: {int(sentence_gauge * 100)}%")
                    
                    with progress_col2:
                        # ì–´íœ˜ ë‹¤ì–‘ì„± ê²Œì´ì§€
                        vocab_ratio = stats['vocabulary_size'] / max(1, stats['word_count'])
                        st.progress(min(1.0, vocab_ratio * 2))  # 0.5 ì´ìƒì´ë©´ 100%
                        st.caption(f"ì–´íœ˜ ë‹¤ì–‘ì„±: {int(min(1.0, vocab_ratio * 2) * 100)}%")
    
    # ì˜ì‘ë¬¸ ì¬ì‘ì„± íƒ­
    with tabs[1]:
        st.subheader("ì˜ì‘ë¬¸ ì¬ì‘ì„±")
        
        # ì™¼ìª½ ì—´: ì…ë ¥ ë° ì˜µì…˜
        left_col, right_col = st.columns(2)
        
        with left_col:
            # ë¶„ì„ íƒ­ì—ì„œ ë„˜ì–´ì˜¨ ê²½ìš° í•´ë‹¹ í…ìŠ¤íŠ¸ë¥¼ ìë™ìœ¼ë¡œ ë¡œë“œ
            default_text = ""
            if 'copy_to_rewrite' in st.session_state:
                default_text = st.session_state.copy_to_rewrite
                st.success("ë¶„ì„ ê²°ê³¼ í…ìŠ¤íŠ¸ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
                # í•œ ë²ˆ ì‚¬ìš© í›„ ì„ì‹œ ë³€ìˆ˜ë¡œ ì˜®ê²¨ ì €ì¥
                st.session_state.copy_to_rewrite_temp = default_text
                del st.session_state.copy_to_rewrite
            elif 'copy_to_rewrite_temp' in st.session_state:
                default_text = st.session_state.copy_to_rewrite_temp
            
            rewrite_text_input = st.text_area("ì•„ë˜ì— ì˜ì–´ ì‘ë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", 
                                            value=default_text,
                                            height=200, 
                                            key="text_tab2")
            
            level_option = st.radio(
                "ì‘ë¬¸ ìˆ˜ì¤€ ì„ íƒ",
                options=["ë¹„ìŠ·í•œ ìˆ˜ì¤€", "ì•½ê°„ ë†’ì€ ìˆ˜ì¤€", "ê³ ê¸‰ ìˆ˜ì¤€"],
                horizontal=True
            )
            
            level_map = {
                "ë¹„ìŠ·í•œ ìˆ˜ì¤€": "similar",
                "ì•½ê°„ ë†’ì€ ìˆ˜ì¤€": "improved",
                "ê³ ê¸‰ ìˆ˜ì¤€": "advanced"
            }
            
            if st.button("ì¬ì‘ì„±í•˜ê¸°"):
                if not rewrite_text_input:
                    st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                else:
                    level = level_map.get(level_option, "similar")
                    
                    # ì¬ì‘ì„± ì²˜ë¦¬
                    with st.spinner("í…ìŠ¤íŠ¸ë¥¼ ì¬ì‘ì„± ì¤‘ì…ë‹ˆë‹¤..."):
                        rewritten_text = rewrite_text(rewrite_text_input, level)
                        
                        # ì¬ì‘ì„±ëœ í…ìŠ¤íŠ¸ë¥¼ ì„¸ì…˜ ìƒíƒœì— ì €ì¥
                        if 'rewritten_text' not in st.session_state:
                            st.session_state.rewritten_text = {}
                        
                        st.session_state.rewritten_text[level] = rewritten_text
                        
                        # ê¸°ë¡ì— ì¶”ê°€
                        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        st.session_state.history.append({
                            'timestamp': timestamp,
                            'text': rewrite_text_input,
                            'action': f"ì¬ì‘ì„± ({level_option})"
                        })
        
        with right_col:
            st.subheader("ì¬ì‘ì„± ê²°ê³¼")
            
            if 'rewritten_text' in st.session_state and st.session_state.rewritten_text:
                level = level_map.get(level_option, "similar")
                
                if level in st.session_state.rewritten_text:
                    rewritten = st.session_state.rewritten_text[level]
                    st.text_area("ì¬ì‘ì„±ëœ í…ìŠ¤íŠ¸", value=rewritten, height=250, key="rewritten_result")
                    
                    # ì¬ì‘ì„± í…ìŠ¤íŠ¸ ë³µì‚¬ ê¸°ëŠ¥ ëŒ€ì‹  ë‹¤ìš´ë¡œë“œ ê¸°ëŠ¥ ì œê³µ
                    if rewritten:
                        output = io.BytesIO()
                        output.write(rewritten.encode('utf-8'))
                        output.seek(0)
                        
                        st.download_button(
                            label="ì¬ì‘ì„± í…ìŠ¤íŠ¸ ë‹¤ìš´ë¡œë“œ",
                            data=output,
                            file_name=f"rewritten_text_{level}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt",
                            mime="text/plain"
                        )
                    
                    # ì›ë³¸ê³¼ ì¬ì‘ì„± í…ìŠ¤íŠ¸ ë¹„êµ
                    if rewrite_text_input and rewritten:
                        st.subheader("ì›ë³¸ vs ì¬ì‘ì„± ë¹„êµ")
                        
                        comparison_data = []
                        original_sentences = custom_sent_tokenize(rewrite_text_input)
                        rewritten_sentences = custom_sent_tokenize(rewritten)
                        
                        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¹„êµ (ë” ì§§ì€ ë¦¬ìŠ¤íŠ¸ ê¸°ì¤€)
                        for i in range(min(len(original_sentences), len(rewritten_sentences))):
                            comparison_data.append({
                                "ì›ë³¸": original_sentences[i],
                                "ì¬ì‘ì„±": rewritten_sentences[i]
                            })
                        
                        if comparison_data:
                            st.dataframe(pd.DataFrame(comparison_data), use_container_width=True)
            else:
                st.info("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê³  ì¬ì‘ì„± ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
    
    # ë‚´ ì‘ë¬¸ ê¸°ë¡ íƒ­
    with tabs[2]:
        st.subheader("ë‚´ ì‘ë¬¸ ê¸°ë¡")
        if not st.session_state.history:
            st.info("ì•„ì§ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            history_df = pd.DataFrame(st.session_state.history)
            st.dataframe(history_df)
            
            # ì˜¤ë¥˜ ìˆ˜ ì¶”ì´ ì°¨íŠ¸
            if len(history_df) > 1 and 'error_count' in history_df.columns:
                fig = px.line(history_df, x='timestamp', y='error_count', 
                            title='ë¬¸ë²• ì˜¤ë¥˜ ìˆ˜ ì¶”ì´',
                            labels={'timestamp': 'ë‚ ì§œ', 'error_count': 'ì˜¤ë¥˜ ìˆ˜'})
                st.plotly_chart(fig, use_container_width=True)

# êµì‚¬ í˜ì´ì§€
def show_teacher_page():
    st.title("ì˜ì‘ë¬¸ ìë™ ì²¨ì‚­ ì‹œìŠ¤í…œ - êµì‚¬")
    
    # ë¡œê·¸ì•„ì›ƒ ë²„íŠ¼
    if st.button("ë¡œê·¸ì•„ì›ƒ", key="teacher_logout"):
        st.session_state.user_type = None
        st.rerun()
    
    # íƒ­ ìƒì„±
    tabs = st.tabs(["ì˜ì‘ë¬¸ ì²¨ì‚­", "í•™ìŠµ ëŒ€ì‹œë³´ë“œ"])
    
    # ì˜ì‘ë¬¸ ì²¨ì‚­ íƒ­
    with tabs[0]:
        st.subheader("ì˜ì‘ë¬¸ ì…ë ¥ ë° ì²¨ì‚­")
        
        user_text = st.text_area("í•™ìƒì˜ ì˜ì–´ ì‘ë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", height=200, key="teacher_text")
        
        col1, col2 = st.columns([3, 1])
        
        # ëª¨ë“  ë¶„ì„ì„ í•œ ë²ˆì— ì‹¤í–‰í•˜ëŠ” ë²„íŠ¼
        with col1:
            if st.button("ì „ì²´ ë¶„ì„í•˜ê¸°", key="teacher_analyze_all", use_container_width=True):
                    if not user_text:
                        st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
                    else:
                        # ë¬¸ë²• ì˜¤ë¥˜ ê²€ì‚¬
                        grammar_errors = check_grammar(user_text)
                    
                    # ì–´íœ˜ ë¶„ì„
                    vocab_analysis = analyze_vocabulary(user_text)
                    
                    # ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜
                    diversity_score = calculate_lexical_diversity(user_text)
                    
                    # ì–´íœ˜ ìˆ˜ì¤€ í‰ê°€
                    vocab_level = evaluate_vocabulary_level(user_text)
                    
                    # í…ìŠ¤íŠ¸ í†µê³„ ë¶„ì„
                    stats = analyze_text(user_text)
                    
                    # ì„¸ì…˜ ìƒíƒœì— ê²°ê³¼ ì €ì¥
                    if 'teacher_analysis_results' not in st.session_state:
                        st.session_state.teacher_analysis_results = {}
                    
                    st.session_state.teacher_analysis_results = {
                        'stats': stats,
                        'grammar_errors': grammar_errors,
                        'vocab_analysis': vocab_analysis,
                        'diversity_score': diversity_score,
                        'vocab_level': vocab_level,
                        'original_text': user_text  # ì›ë³¸ í…ìŠ¤íŠ¸ë„ ì €ì¥
                    }
                    
                    st.success("ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤! ì•„ë˜ íƒ­ì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
        
        # ì¬ì‘ì„± ì¶”ì²œ ë²„íŠ¼ ì¶”ê°€
        with col2:
            # ë¶„ì„ ê²°ê³¼ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë²„íŠ¼ í‘œì‹œ
            if 'teacher_analysis_results' in st.session_state and 'original_text' in st.session_state.teacher_analysis_results:
                if st.button("âœ¨ ì˜ì‘ë¬¸ ì¬ì‘ì„± ì¶”ì²œ âœ¨", 
                          key="teacher_rewrite_recommendation",
                          use_container_width=True,
                          help="ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ í•™ìƒì˜ ì˜ì‘ë¬¸ì„ ë” ì¢‹ì€ í‘œí˜„ìœ¼ë¡œ ì¬ì‘ì„±í•´ë³´ì„¸ìš”!",
                          type="primary"):
                    # êµì‚¬ ì²¨ì‚­ ì˜ì—­ì— ëª¨ë²” ë‹µì•ˆ ì‘ì„±ìš©ìœ¼ë¡œ ì¶”ê°€
                    # ì¬ì‘ì„± í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¦‰ì‹œ ê³ ê¸‰ ìˆ˜ì¤€ìœ¼ë¡œ ì¬ì‘ì„±
                    original_text = st.session_state.teacher_analysis_results['original_text']
                    rewritten_text = rewrite_text(original_text, "advanced")
                    
                    # ì²¨ì‚­ ë…¸íŠ¸ì— ì¬ì‘ì„±ëœ í…ìŠ¤íŠ¸ ì¶”ê°€
                    if 'feedback_template' not in st.session_state:
                        st.session_state.feedback_template = "ë‹¤ìŒì€ í•™ìƒ ì‘ë¬¸ì„ ê³ ê¸‰ ìˆ˜ì¤€ìœ¼ë¡œ ì¬ì‘ì„±í•œ ì˜ˆì‹œì…ë‹ˆë‹¤:\n\n" + rewritten_text
                    else:
                        st.session_state.feedback_template += "\n\nì¶”ì²œ ëª¨ë²” ì˜ˆì‹œ:\n" + rewritten_text
                    
                    st.success("ê³ ê¸‰ ìˆ˜ì¤€ìœ¼ë¡œ ì¬ì‘ì„±ëœ í…ìŠ¤íŠ¸ê°€ ì²¨ì‚­ ë…¸íŠ¸ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    st.balloons()  # ì‹œê°ì  íš¨ê³¼ ì¶”ê°€
        
        # ê²°ê³¼ í‘œì‹œë¥¼ ìœ„í•œ íƒ­
        result_tab1, result_tab2, result_tab3 = st.tabs(["ë¬¸ë²• ê²€ì‚¬", "ì–´íœ˜ ë¶„ì„", "í…ìŠ¤íŠ¸ í†µê³„"])
        
        with result_tab1:
            if 'teacher_analysis_results' in st.session_state and 'grammar_errors' in st.session_state.teacher_analysis_results:
                grammar_errors = st.session_state.teacher_analysis_results['grammar_errors']
                        
                        if grammar_errors:
                            st.subheader("ë¬¸ë²• ì˜¤ë¥˜ ëª©ë¡")
                            
                            error_data = []
                            for error in grammar_errors:
                                error_data.append({
                            "ì˜¤ë¥˜": user_text[error['offset']:error['offset'] + error['errorLength']],
                            "ì˜¤ë¥˜ ë‚´ìš©": error['message'],
                            "ìˆ˜ì • ì œì•ˆ": error['replacements']
                                })
                            
                            st.dataframe(pd.DataFrame(error_data))
                        else:
                            st.success("ë¬¸ë²• ì˜¤ë¥˜ê°€ ì—†ìŠµë‹ˆë‹¤!")
            
        with result_tab2:
            if 'teacher_analysis_results' in st.session_state and 'vocab_analysis' in st.session_state.teacher_analysis_results:
                vocab_analysis = st.session_state.teacher_analysis_results['vocab_analysis']
                diversity_score = st.session_state.teacher_analysis_results['diversity_score']
                vocab_level = st.session_state.teacher_analysis_results['vocab_level']
                        
                        # ë‹¨ì–´ ë¹ˆë„ ì‹œê°í™”
                        fig = plot_word_frequency(vocab_analysis['word_freq'])
                        if fig:
                            st.plotly_chart(fig, use_container_width=True)
                        
                        # ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜
                        st.metric("ì–´íœ˜ ë‹¤ì–‘ì„± ì ìˆ˜", f"{diversity_score:.2f}")
                        
                        # ì–´íœ˜ ìˆ˜ì¤€ í‰ê°€
                        level_df = pd.DataFrame({
                            'ìˆ˜ì¤€': ['ê¸°ì´ˆ', 'ì¤‘ê¸‰', 'ê³ ê¸‰'],
                            'ë¹„ìœ¨': [vocab_level['basic'], vocab_level['intermediate'], vocab_level['advanced']]
                        })
                        
                        fig = px.pie(level_df, values='ë¹„ìœ¨', names='ìˆ˜ì¤€', 
                                    title='ì–´íœ˜ ìˆ˜ì¤€ ë¶„í¬')
                        st.plotly_chart(fig, use_container_width=True)
        
        with result_tab3:
            if 'teacher_analysis_results' in st.session_state and 'stats' in st.session_state.teacher_analysis_results:
                stats = st.session_state.teacher_analysis_results['stats']
                    
                    st.subheader("í…ìŠ¤íŠ¸ í†µê³„")
                    col1, col2 = st.columns(2)
                    with col1:
                        st.metric("ë‹¨ì–´ ìˆ˜", stats['word_count'])
                        st.metric("ë¬¸ì¥ ìˆ˜", stats['sentence_count'])
                    with col2:
                        st.metric("í‰ê·  ë‹¨ì–´ ê¸¸ì´", stats['avg_word_length'])
                        st.metric("í‰ê·  ë¬¸ì¥ ê¸¸ì´ (ë‹¨ì–´)", stats['avg_sentence_length'])
                    
                    st.metric("ì–´íœ˜ í¬ê¸° (ê³ ìœ  ë‹¨ì–´ ìˆ˜)", stats['vocabulary_size'])
        
        # êµì‚¬ ì „ìš© ê¸°ëŠ¥
        st.subheader("ì²¨ì‚­ ë° í”¼ë“œë°±")
        
        # ì²¨ì‚­ ë…¸íŠ¸ í…œí”Œë¦¿
        feedback_default = "ë‹¤ìŒ ì‚¬í•­ì„ ì¤‘ì ì ìœ¼ë¡œ ê°œì„ í•´ë³´ì„¸ìš”:\n1. \n2. \n3. "
        if 'feedback_template' in st.session_state:
            feedback_default = st.session_state.feedback_template
        
        feedback = st.text_area("ì²¨ì‚­ ë…¸íŠ¸", 
                             value=feedback_default, 
                             height=200,  # ë” ë†’ê²Œ ì¡°ì •
                 key="feedback_template")
        
        # ì„¸ì…˜ ìƒíƒœ ì—…ë°ì´íŠ¸
        st.session_state.feedback_template = feedback
        
        # ì ìˆ˜ ì…ë ¥
        score_col1, score_col2, score_col3 = st.columns(3)
        with score_col1:
            grammar_score = st.slider("ë¬¸ë²• ì ìˆ˜", 0, 10, 5)
        with score_col2:
            vocab_score = st.slider("ì–´íœ˜ ì ìˆ˜", 0, 10, 5)
        with score_col3:
            content_score = st.slider("ë‚´ìš© ì ìˆ˜", 0, 10, 5)
        
        total_score = (grammar_score + vocab_score + content_score) / 3
        st.metric("ì¢…í•© ì ìˆ˜", f"{total_score:.1f} / 10")
        
        # ì €ì¥ ì˜µì…˜
        if st.button("ì²¨ì‚­ ê²°ê³¼ ì €ì¥ (Excel)"):
            if not user_text:
                st.warning("í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            else:
                # ë¬¸ë²• ì˜¤ë¥˜ ê²€ì‚¬
                grammar_errors = check_grammar(user_text)
                
                # ì €ì¥í•  ë°ì´í„° ìƒì„±
                error_data = []
                for error in grammar_errors:
                    error_data.append({
                        "ì˜¤ë¥˜": user_text[error['offset']:error['offset'] + error['errorLength']],
                        "ì˜¤ë¥˜ ë‚´ìš©": error['message'],
                        "ìˆ˜ì • ì œì•ˆ": str(error['replacements']),
                        "ìœ„ì¹˜": f"{error['offset']}:{error['offset'] + error['errorLength']}"
                    })
                
                # ì–´íœ˜ ë¶„ì„
                vocab_analysis = analyze_vocabulary(user_text)
                
                # í†µê³„ ë¶„ì„
                stats = analyze_text(user_text)
                
                # ì—¬ëŸ¬ ì‹œíŠ¸ê°€ ìˆëŠ” Excel íŒŒì¼ ìƒì„±
                excel_buffer = io.BytesIO()
                with pd.ExcelWriter(excel_buffer, engine='openpyxl') as writer:
                    # ì›ë³¸ í…ìŠ¤íŠ¸ ì‹œíŠ¸
                    pd.DataFrame({"ì›ë³¸ í…ìŠ¤íŠ¸": [user_text]}).to_excel(writer, sheet_name="ì›ë³¸ í…ìŠ¤íŠ¸", index=False)
                    
                    # ì˜¤ë¥˜ ë°ì´í„° ì‹œíŠ¸
                    pd.DataFrame(error_data).to_excel(writer, sheet_name="ë¬¸ë²• ì˜¤ë¥˜", index=False)
                    
                    # í†µê³„ ì‹œíŠ¸
                    stats_df = pd.DataFrame({
                        "í•­ëª©": ["ë‹¨ì–´ ìˆ˜", "ë¬¸ì¥ ìˆ˜", "í‰ê·  ë‹¨ì–´ ê¸¸ì´", "í‰ê·  ë¬¸ì¥ ê¸¸ì´", "ì–´íœ˜ í¬ê¸°"],
                        "ê°’": [stats['word_count'], stats['sentence_count'], stats['avg_word_length'], 
                             stats['avg_sentence_length'], stats['vocabulary_size']]
                    })
                    stats_df.to_excel(writer, sheet_name="í†µê³„", index=False)
                    
                    # í‰ê°€ ì ìˆ˜ ì‹œíŠ¸
                    score_df = pd.DataFrame({
                        "í‰ê°€ í•­ëª©": ["ë¬¸ë²•", "ì–´íœ˜", "ë‚´ìš©", "ì¢…í•© ì ìˆ˜"],
                        "ì ìˆ˜": [grammar_score, vocab_score, content_score, total_score]
                    })
                    score_df.to_excel(writer, sheet_name="í‰ê°€ ì ìˆ˜", index=False)
                    
                    # í”¼ë“œë°± ì‹œíŠ¸
                    pd.DataFrame({"ì²¨ì‚­ í”¼ë“œë°±": [feedback]}).to_excel(writer, sheet_name="í”¼ë“œë°±", index=False)
                
                # ë‹¤ìš´ë¡œë“œ ë²„íŠ¼
                excel_buffer.seek(0)
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                st.download_button(
                    label="Excel ë‹¤ìš´ë¡œë“œ",
                    data=excel_buffer,
                    file_name=f"ì²¨ì‚­ê²°ê³¼_{timestamp}.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                )
    
    # í•™ìŠµ ëŒ€ì‹œë³´ë“œ íƒ­
    with tabs[1]:
        st.subheader("í•™ìŠµ ëŒ€ì‹œë³´ë“œ")
        
        # ìƒ˜í”Œ í•™ìƒ ë°ì´í„° (ì‹¤ì œë¡œëŠ” DBì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨)
        sample_data = {
            "ë‚ ì§œ": ["2023-01-01", "2023-01-08", "2023-01-15", "2023-01-22", "2023-01-29"],
            "í•™ìƒ_A": [7, 6, 8, 7, 9],
            "í•™ìƒ_B": [5, 6, 6, 7, 6],
            "í•™ìƒ_C": [8, 8, 7, 9, 9],
            "í•™ìƒ_D": [4, 5, 6, 6, 7]
        }
        
        scores_df = pd.DataFrame(sample_data)
        
        # í•™ìƒë³„ ì ìˆ˜ ì¶”ì´
        fig = px.line(scores_df, x="ë‚ ì§œ", y=["í•™ìƒ_A", "í•™ìƒ_B", "í•™ìƒ_C", "í•™ìƒ_D"],
                     title="í•™ìƒë³„ ì˜ì‘ë¬¸ ì ìˆ˜ ì¶”ì´",
                     labels={"value": "ì ìˆ˜", "variable": "í•™ìƒ"})
        st.plotly_chart(fig, use_container_width=True)
        
        # ìµœê·¼ ì ìˆ˜ ë¶„í¬
        latest_scores = scores_df.iloc[-1, 1:].values
        students = scores_df.columns[1:]
        
        fig = px.bar(x=students, y=latest_scores, 
                    title="ìµœê·¼ ì˜ì‘ë¬¸ ì ìˆ˜ ë¶„í¬",
                    labels={"x": "í•™ìƒ", "y": "ì ìˆ˜"})
        st.plotly_chart(fig, use_container_width=True)
        
        # í‰ê·  ì˜¤ë¥˜ ìœ í˜• (ìƒ˜í”Œ ë°ì´í„°)
        error_types = {
            "ì˜¤ë¥˜ ìœ í˜•": ["ë¬¸ì¥ êµ¬ì¡°", "ì‹œì œ", "ê´€ì‚¬", "ì „ì¹˜ì‚¬", "ëŒ€ëª…ì‚¬", "ì² ì", "êµ¬ë‘ì "],
            "ë¹ˆë„": [45, 30, 25, 20, 15, 10, 5]
        }
        
        error_df = pd.DataFrame(error_types)
        
        fig = px.pie(error_df, values="ë¹ˆë„", names="ì˜¤ë¥˜ ìœ í˜•",
                    title="í‰ê·  ì˜¤ë¥˜ ìœ í˜• ë¶„í¬")
        st.plotly_chart(fig, use_container_width=True)

# ë©”ì¸ í•¨ìˆ˜
def main():
    # ì œëª© ë° ì†Œê°œ
    st.title("ì˜ì‘ë¬¸ ìë™ ì²¨ì‚­ ì‹œìŠ¤í…œ")
    st.markdown("""
    ì´ ì• í”Œë¦¬ì¼€ì´ì…˜ì€ í•™ìƒë“¤ì˜ ì˜ì‘ë¬¸ì„ ìë™ìœ¼ë¡œ ì²¨ì‚­í•˜ê³  í”¼ë“œë°±ì„ ì œê³µí•©ë‹ˆë‹¤.
    í•™ìƒì€ í…ìŠ¤íŠ¸ë¥¼ ì…ë ¥í•˜ê³  ìë™ ë¶„ì„ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
    """)
    
    # ì§ì ‘ í•™ìƒ í˜ì´ì§€ë¡œ ì´ë™
        show_student_page()

if __name__ == "__main__":
    main()
